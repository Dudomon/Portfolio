"""
ðŸš€ TwoHeadV4Intelligent48h - Policy especializada para trades de atÃ© 48h
Baseada na TwoHeadV3HybridEnhanced com 6 melhorias inteligentes

MELHORIAS INTELIGENTES PARA 48H:
âœ… Temporal Horizon Awareness (ConsciÃªncia do horizonte temporal)
âœ… Multi-Timeframe Fusion Intelligence (FusÃ£o multi-timeframe)
âœ… Advanced Pattern Memory System (MemÃ³ria avanÃ§ada multi-horizonte)
âœ… Dynamic Risk Adaptation (AdaptaÃ§Ã£o dinÃ¢mica de risco)
âœ… Market Regime Intelligence (InteligÃªncia de regime de mercado)
âœ… Predictive Lookahead System (Sistema de previsÃ£o futura)
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Any, Dict, List, Optional, Type, Union
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
# Fallback para PyTorchObs
try:
    from stable_baselines3.common.type_aliases import PyTorchObs
except ImportError:
    import torch
    PyTorchObs = torch.Tensor
# Imports corretos para RecurrentPPO
try:
    from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy
    from sb3_contrib.common.recurrent.type_aliases import RNNStates
except ImportError:
    from stable_baselines3.common.policies import RecurrentActorCriticPolicy
    from stable_baselines3.common.recurrent_policies import RNNStates

from stable_baselines3.common.utils import obs_as_tensor
from trading_framework.extractors.transformer_extractor import TradingTransformerFeatureExtractor


class TwoHeadV4Intelligent48h(RecurrentActorCriticPolicy):
    """
    ðŸš€ TWOHEAD V4 INTELLIGENT 48H - Policy especializada para trades de atÃ© 48h
    
    ARQUITETURA BASE (herdada da V3HybridEnhanced):
    - 2 LSTM Layers (hierarquia temporal)
    - 1 GRU Stabilizer (noise reduction)
    - 8 Attention Heads (dependÃªncias temporais)
    - Pattern Recognition (micro/macro)
    - Two Heads especializadas (Entry/Management)
    
    MELHORIAS INTELIGENTES PARA 48H:
    - Temporal Horizon Awareness: Embedding de horizonte temporal
    - Multi-Timeframe Fusion: FusÃ£o de features de mÃºltiplos timeframes
    - Advanced Pattern Memory: MemÃ³ria multi-horizonte (1h, 4h, 48h)
    - Dynamic Risk Adaptation: AdaptaÃ§Ã£o dinÃ¢mica baseada em risco
    - Market Regime Intelligence: DetecÃ§Ã£o e uso de regime de mercado
    - Predictive Lookahead: PrevisÃ£o de retorno futuro como feature auxiliar
    """
    
    def __init__(
        self,
        observation_space,
        action_space,
        lr_schedule,
        net_arch: Optional[List[Union[int, Dict[str, List[int]]]]] = None,
        activation_fn: Type[nn.Module] = nn.ReLU,
        ortho_init: bool = True,
        use_sde: bool = False,
        log_std_init: float = 0.0,
        use_expln: bool = False,
        squash_output: bool = False,
        features_extractor_class: Type[BaseFeaturesExtractor] = TradingTransformerFeatureExtractor,
        features_extractor_kwargs: Optional[Dict[str, Any]] = None,
        normalize_images: bool = True,
        optimizer_class: Type[torch.optim.Optimizer] = torch.optim.Adam,
        optimizer_kwargs: Optional[Dict[str, Any]] = None,
        # ðŸš€ PARÃ‚METROS BASE (herdados da V3HybridEnhanced)
        lstm_hidden_size: int = 128,
        n_lstm_layers: int = 2,
        attention_heads: int = 8,
        gru_enabled: bool = True,
        pattern_recognition: bool = True,
        # ðŸš€ MELHORIAS CONSERVADORAS (herdadas)
        adaptive_lr: bool = True,
        gradient_clipping: bool = True,
        feature_weighting: bool = True,
        dynamic_attention: bool = True,
        memory_bank_size: int = 100,
        # ðŸš€ MELHORIAS INTELIGENTES PARA 48H (novas)
        enable_temporal_horizon: bool = True,
        enable_multi_timeframe: bool = True,
        enable_advanced_memory: bool = True,
        enable_dynamic_risk: bool = True,
        enable_regime_intelligence: bool = True,
        enable_lookahead: bool = True,
        **kwargs
    ):
        """V4Intelligent48h com todas as melhorias inteligentes para trades de 48h"""
        
        # ðŸš€ MANTER CONFIGURAÃ‡Ã•ES COMPROVADAS
        if features_extractor_kwargs is None:
            features_extractor_kwargs = {
                'features_dim': 128,  # COMPROVADO
                'seq_len': 10         # COMPROVADO
            }
        
        if net_arch is None:
            net_arch = [dict(pi=[320, 256, 128], vf=[320, 256, 128])]  # COMPROVADO
        
        # ðŸš€ PARÃ‚METROS BASE (herdados)
        self.lstm_hidden_size = lstm_hidden_size
        self.n_lstm_layers = n_lstm_layers
        self.attention_heads = attention_heads
        self.gru_enabled = gru_enabled
        self.pattern_recognition = pattern_recognition
        
        # ðŸš€ MELHORIAS CONSERVADORAS (herdadas)
        self.adaptive_lr = adaptive_lr
        self.gradient_clipping = gradient_clipping
        self.feature_weighting = feature_weighting
        self.dynamic_attention = dynamic_attention
        self.memory_bank_size = memory_bank_size
        
        # ðŸš€ MELHORIAS INTELIGENTES PARA 48H (novas)
        self.enable_temporal_horizon = enable_temporal_horizon
        self.enable_multi_timeframe = enable_multi_timeframe
        self.enable_advanced_memory = enable_advanced_memory
        self.enable_dynamic_risk = enable_dynamic_risk
        self.enable_regime_intelligence = enable_regime_intelligence
        self.enable_lookahead = enable_lookahead
        
        # Detectar action space
        self.action_dim = action_space.shape[0]
        
        # Inicializar classe pai
        super().__init__(
            observation_space=observation_space,
            action_space=action_space,
            lr_schedule=lr_schedule,
            net_arch=net_arch,
            activation_fn=activation_fn,
            ortho_init=ortho_init,
            use_sde=use_sde,
            log_std_init=log_std_init,
            use_expln=use_expln,
            squash_output=squash_output,
            features_extractor_class=features_extractor_class,
            features_extractor_kwargs=features_extractor_kwargs,
            normalize_images=normalize_images,
            optimizer_class=optimizer_class,
            optimizer_kwargs=optimizer_kwargs,
            lstm_hidden_size=self.lstm_hidden_size,
            n_lstm_layers=self.n_lstm_layers,
            **kwargs
        )
        
        # Inicializar componentes
        self._init_hybrid_components()
        self._init_enhancements()
        self._init_intelligent_48h_components()
        self._initialize_balanced_weights()
        
        # Logs
        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"ðŸš€ TwoHeadV4Intelligent48h: {total_params:,} parÃ¢metros")
        print(f"ðŸš€ Base: 2-LSTM + 1-GRU + 8-Head Attention")
        print(f"ðŸš€ Melhorias Conservadoras: Adaptive LR, Gradient Clipping, Feature Weighting")
        print(f"ðŸš€ Melhorias Inteligentes 48h: Temporal Horizon, Multi-Timeframe, Advanced Memory, Dynamic Risk, Regime Intelligence, Lookahead")
    
    def _init_hybrid_components(self):
        """Inicializa componentes hÃ­bridos COMPROVADOS da V3HybridEnhanced"""
        features_dim = self.features_extractor.features_dim
        
        # ðŸš€ GRU ESTABILIZADOR (COMPROVADO)
        if self.gru_enabled:
            self.gru_stabilizer = nn.GRU(
                input_size=self.lstm_hidden_size,
                hidden_size=self.lstm_hidden_size,
                num_layers=1,
                batch_first=True,
                dropout=0.0
            )
            self.gru_norm = nn.LayerNorm(self.lstm_hidden_size)
        
        # ðŸš€ ATTENTION (COMPROVADO - 8 heads)
        self.temporal_attention = nn.MultiheadAttention(
            embed_dim=self.lstm_hidden_size,
            num_heads=self.attention_heads,
            dropout=0.10,  # COMPROVADO
            batch_first=True
        )
        
        # ðŸš€ PATTERN RECOGNITION (COMPROVADO)
        if self.pattern_recognition:
            self.micro_pattern_detector = nn.Sequential(
                nn.Linear(self.lstm_hidden_size, 64),
                nn.ReLU(),
                nn.Dropout(0.10),  # COMPROVADO
                nn.Linear(64, 32),
                nn.Sigmoid()
            )
            
            self.macro_pattern_detector = nn.Sequential(
                nn.Linear(self.lstm_hidden_size, 64),
                nn.ReLU(),
                nn.Dropout(0.10),  # COMPROVADO
                nn.Linear(64, 32),
                nn.Sigmoid()
            )
            pattern_features = 64
        else:
            pattern_features = 0
        
        # ðŸš€ FEATURE FUSION (COMPROVADO)
        fusion_input_size = self.lstm_hidden_size
        if self.gru_enabled:
            fusion_input_size += self.lstm_hidden_size
        fusion_input_size += pattern_features
        
        self.feature_fusion = nn.Sequential(
            nn.Linear(fusion_input_size, 320),  # COMPROVADO
            nn.LayerNorm(320),
            nn.ReLU(),
            nn.Dropout(0.15),  # COMPROVADO
            nn.Linear(320, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.10),  # COMPROVADO
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU()
        )
        
        # ðŸš€ TWO HEADS (COMPROVADO)
        self.entry_head = nn.Sequential(
            nn.Linear(128, 96),
            nn.LayerNorm(96),
            nn.ReLU(),
            nn.Dropout(0.12),  # COMPROVADO
            nn.Linear(96, 64),
            nn.LayerNorm(64),
            nn.ReLU()
        )
        
        self.management_head = nn.Sequential(
            nn.Linear(128, 96),
            nn.LayerNorm(96),
            nn.ReLU(),
            nn.Dropout(0.12),  # COMPROVADO
            nn.Linear(96, 64),
            nn.LayerNorm(64),
            nn.ReLU()
        )
        
        # Residual connection
        self.residual_connection = nn.Linear(128, 128)
    
    def _init_enhancements(self):
        """Inicializa MELHORIAS CONSERVADORAS da V3HybridEnhanced"""
        
        # ðŸš€ MELHORIA 1: Feature Importance Weighting
        if self.feature_weighting:
            self.feature_importance = nn.Parameter(
                torch.ones(self.lstm_hidden_size) * 0.5,  # Inicia neutro
                requires_grad=True
            )
        
        # ðŸš€ MELHORIA 2: Dynamic Attention Temperature
        if self.dynamic_attention:
            self.attention_temperature = nn.Parameter(
                torch.tensor(1.0),  # Inicia neutro
                requires_grad=True
            )
        
        # ðŸš€ MELHORIA 3: Memory Bank para padrÃµes recorrentes
        if self.memory_bank_size > 0:
            self.register_buffer(
                'pattern_memory', 
                torch.zeros(self.memory_bank_size, 64)  # 64 = micro + macro patterns
            )
            self.register_buffer('memory_ptr', torch.zeros(1, dtype=torch.long))
        
        # ðŸš€ MELHORIA 4: Gradient statistics tracking
        if self.gradient_clipping:
            self.register_buffer('grad_norm_ema', torch.tensor(1.0))
            self.grad_ema_decay = 0.99
    
    def _init_intelligent_48h_components(self):
        """Inicializa MELHORIAS INTELIGENTES PARA 48H"""
        
        # ðŸš€ MELHORIA 1: Temporal Horizon Awareness
        if self.enable_temporal_horizon:
            self.horizon_embedding = nn.Linear(1, 8)  # 1 valor (steps ou horas) -> 8d embedding
        
        # ðŸš€ MELHORIA 2: Multi-Timeframe Fusion
        if self.enable_multi_timeframe:
            self.timeframe_fusion = nn.Sequential(
                nn.Linear(self.features_extractor.features_dim * 3, 128),  # 3 timeframes (5m, 15m, 4h)
                nn.ReLU(),
                nn.LayerNorm(128)
            )
        
        # ðŸš€ MELHORIA 3: Advanced Pattern Memory System (multi-horizonte)
        if self.enable_advanced_memory:
            # MemÃ³ria separada para cada horizonte temporal
            self.memory_1h = nn.Parameter(torch.zeros(self.memory_bank_size, 64), requires_grad=False)
            self.memory_4h = nn.Parameter(torch.zeros(self.memory_bank_size, 64), requires_grad=False)
            self.memory_48h = nn.Parameter(torch.zeros(self.memory_bank_size, 64), requires_grad=False)
            self.memory_ptr_1h = nn.Parameter(torch.zeros(1, dtype=torch.long), requires_grad=False)
            self.memory_ptr_4h = nn.Parameter(torch.zeros(1, dtype=torch.long), requires_grad=False)
            self.memory_ptr_48h = nn.Parameter(torch.zeros(1, dtype=torch.long), requires_grad=False)
            
            # Attention cruzada entre padrÃµes atuais e memÃ³rias
            self.cross_attention_1h = nn.MultiheadAttention(embed_dim=64, num_heads=4, batch_first=True)
            self.cross_attention_4h = nn.MultiheadAttention(embed_dim=64, num_heads=4, batch_first=True)
            self.cross_attention_48h = nn.MultiheadAttention(embed_dim=64, num_heads=4, batch_first=True)
        
        # ðŸš€ MELHORIA 4: Dynamic Risk Adaptation
        if self.enable_dynamic_risk:
            self.risk_embedding = nn.Linear(4, 8)  # [drawdown, vol, concentraÃ§Ã£o, streak] -> 8d
            self.risk_gate = nn.Sequential(
                nn.Linear(8, 16),
                nn.ReLU(),
                nn.Linear(16, 1),
                nn.Sigmoid()
            )
        
        # ðŸš€ MELHORIA 5: Market Regime Intelligence
        if self.enable_regime_intelligence:
            self.regime_detector = nn.Sequential(
                nn.Linear(self.features_extractor.features_dim, 16),
                nn.ReLU(),
                nn.Linear(16, 4),  # 4 regimes: tendÃªncia, lateral, vol alto, vol baixo
                nn.Softmax(dim=-1)
            )
            self.regime_embedding = nn.Linear(4, 8)
        
        # ðŸš€ MELHORIA 6: Predictive Lookahead System
        if self.enable_lookahead:
            self.lookahead_head = nn.Sequential(
                nn.Linear(self.lstm_hidden_size, 32),
                nn.ReLU(),
                nn.Linear(32, 1)  # PrevisÃ£o de retorno futuro
            )
    
    def _initialize_balanced_weights(self):
        """InicializaÃ§Ã£o COMPROVADA da V3HybridEnhanced"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.5)  # COMPROVADO
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.LayerNorm):
                nn.init.constant_(module.bias, 0)
                nn.init.constant_(module.weight, 1.0)
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.xavier_uniform_(param.data, gain=0.2)
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(param.data, gain=0.2)
                    elif 'bias' in name:
                        param.data.fill_(0)
                        n = param.size(0)
                        param.data[n//4:n//2].fill_(1)  # Forget gate bias = 1
            elif isinstance(module, nn.GRU):
                for name, param in module.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.xavier_uniform_(param.data, gain=0.2)
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(param.data, gain=0.2)
                    elif 'bias' in name:
                        param.data.fill_(0)
    
    def _apply_intelligent_48h_processing(self, input_features: torch.Tensor, extra_info: Optional[Dict[str, torch.Tensor]] = None) -> torch.Tensor:
        """Aplica processamento inteligente para trades de 48h"""
        
        # 1. ProjeÃ§Ã£o inicial (se necessÃ¡rio)
        if input_features.shape[-1] != self.lstm_hidden_size:
            if not hasattr(self, 'input_projector'):
                self.input_projector = nn.Linear(
                    input_features.shape[-1], 
                    self.lstm_hidden_size
                ).to(input_features.device)
            projected_features = self.input_projector(input_features)
        else:
            projected_features = input_features
        
        # ðŸš€ MELHORIA: Feature Importance Weighting
        if self.feature_weighting:
            importance_weights = torch.sigmoid(self.feature_importance)
            projected_features = projected_features * importance_weights.unsqueeze(0)
        
        # 2. GRU estabilizador (COMPROVADO)
        if self.gru_enabled:
            gru_input = projected_features.unsqueeze(1)
            gru_out, _ = self.gru_stabilizer(gru_input)
            gru_out = self.gru_norm(gru_out.squeeze(1))
        else:
            gru_out = projected_features
        
        # 3. Attention com temperature dinÃ¢mica
        attn_input = projected_features.unsqueeze(1)
        
        # ðŸš€ MELHORIA: Dynamic Attention Temperature
        if self.dynamic_attention:
            temperature = torch.clamp(self.attention_temperature, 0.1, 2.0)
            attn_out, attn_weights = self.temporal_attention(
                attn_input, attn_input, attn_input
            )
            attn_out = attn_out / temperature
        else:
            attn_out, attn_weights = self.temporal_attention(
                attn_input, attn_input, attn_input
            )
        
        attn_out = attn_out.squeeze(1)
        
        # 4. Pattern Recognition (COMPROVADO)
        feature_list = [attn_out]
        
        if self.gru_enabled:
            feature_list.append(gru_out)
        
        if self.pattern_recognition:
            micro_patterns = self.micro_pattern_detector(attn_out)
            macro_patterns = self.macro_pattern_detector(attn_out)
            
            # ðŸš€ MELHORIA: Advanced Pattern Memory (multi-horizonte)
            if self.enable_advanced_memory and self.training:
                current_patterns = torch.cat([micro_patterns, macro_patterns], dim=-1)
                
                # Atualizar memÃ³rias multi-horizonte
                self._update_multi_horizon_memory(current_patterns)
                
                # Usar attention cruzada com memÃ³rias
                enhanced_patterns = self._apply_multi_horizon_attention(current_patterns)
                
                # Separar de volta
                pattern_dim = micro_patterns.shape[-1]
                micro_patterns = enhanced_patterns[:, :pattern_dim]
                macro_patterns = enhanced_patterns[:, pattern_dim:]
            
            feature_list.extend([micro_patterns, macro_patterns])
        
        # ðŸš€ MELHORIAS INTELIGENTES: Adicionar embeddings especiais
        if extra_info is not None:
            # Temporal Horizon
            if self.enable_temporal_horizon and 'horizon' in extra_info:
                horizon_emb = self.horizon_embedding(extra_info['horizon'])
                feature_list.append(horizon_emb)
            
            # Multi-Timeframe
            if self.enable_multi_timeframe and 'multi_timeframe' in extra_info:
                mtf = extra_info['multi_timeframe']
                mtf_fused = self.timeframe_fusion(mtf)
                feature_list.append(mtf_fused)
            
            # Dynamic Risk
            if self.enable_dynamic_risk and 'risk_state' in extra_info:
                risk_emb = self.risk_embedding(extra_info['risk_state'])
                feature_list.append(risk_emb)
            
            # Market Regime
            if self.enable_regime_intelligence and 'regime' in extra_info:
                regime_emb = self.regime_embedding(extra_info['regime'])
                feature_list.append(regime_emb)
        
        # 5. Feature Fusion (COMPROVADO)
        fused_features = torch.cat(feature_list, dim=-1)
        fused_features = self.feature_fusion(fused_features)
        
        # 6. Two Heads (COMPROVADO)
        entry_features = self.entry_head(fused_features)
        management_features = self.management_head(fused_features)
        
        # 7. Residual Connection (COMPROVADO)
        residual = self.residual_connection(fused_features)
        
        # 8. Combinar
        combined_features = torch.cat([entry_features, management_features], dim=-1)
        combined_features = combined_features + residual
        
        # 9. Projetar para dimensÃ£o correta
        final_features = self._project_to_parent_dimension(combined_features)
        
        return final_features
    
    def _update_multi_horizon_memory(self, patterns: torch.Tensor):
        """Atualiza memÃ³rias multi-horizonte"""
        batch_size = patterns.shape[0]
        
        for i in range(batch_size):
            # Atualizar memÃ³ria 1h
            ptr_1h = int(self.memory_ptr_1h.item())
            self.memory_1h[ptr_1h] = patterns[i].detach()
            self.memory_ptr_1h[0] = (ptr_1h + 1) % self.memory_bank_size
            
            # Atualizar memÃ³ria 4h (a cada 4 updates)
            if ptr_1h % 4 == 0:
                ptr_4h = int(self.memory_ptr_4h.item())
                self.memory_4h[ptr_4h] = patterns[i].detach()
                self.memory_ptr_4h[0] = (ptr_4h + 1) % self.memory_bank_size
            
            # Atualizar memÃ³ria 48h (a cada 48 updates)
            if ptr_1h % 48 == 0:
                ptr_48h = int(self.memory_ptr_48h.item())
                self.memory_48h[ptr_48h] = patterns[i].detach()
                self.memory_ptr_48h[0] = (ptr_48h + 1) % self.memory_bank_size
    
    def _apply_multi_horizon_attention(self, current_patterns: torch.Tensor) -> torch.Tensor:
        """Aplica attention cruzada com memÃ³rias multi-horizonte"""
        enhanced_patterns = current_patterns
        
        # Attention com memÃ³ria 1h
        if hasattr(self, 'cross_attention_1h'):
            attn_1h, _ = self.cross_attention_1h(
                current_patterns.unsqueeze(1),
                self.memory_1h.unsqueeze(0).expand(current_patterns.shape[0], -1, -1),
                self.memory_1h.unsqueeze(0).expand(current_patterns.shape[0], -1, -1)
            )
            enhanced_patterns = enhanced_patterns + 0.1 * attn_1h.squeeze(1)
        
        # Attention com memÃ³ria 4h
        if hasattr(self, 'cross_attention_4h'):
            attn_4h, _ = self.cross_attention_4h(
                current_patterns.unsqueeze(1),
                self.memory_4h.unsqueeze(0).expand(current_patterns.shape[0], -1, -1),
                self.memory_4h.unsqueeze(0).expand(current_patterns.shape[0], -1, -1)
            )
            enhanced_patterns = enhanced_patterns + 0.1 * attn_4h.squeeze(1)
        
        # Attention com memÃ³ria 48h
        if hasattr(self, 'cross_attention_48h'):
            attn_48h, _ = self.cross_attention_48h(
                current_patterns.unsqueeze(1),
                self.memory_48h.unsqueeze(0).expand(current_patterns.shape[0], -1, -1),
                self.memory_48h.unsqueeze(0).expand(current_patterns.shape[0], -1, -1)
            )
            enhanced_patterns = enhanced_patterns + 0.1 * attn_48h.squeeze(1)
        
        return enhanced_patterns
    
    def _project_to_parent_dimension(self, combined_features: torch.Tensor) -> torch.Tensor:
        """Projeta para dimensÃ£o que a classe pai espera"""
        if combined_features.shape[-1] == self.lstm_hidden_size:
            return combined_features
        else:
            if not hasattr(self, 'dimension_projector'):
                self.dimension_projector = nn.Linear(
                    combined_features.shape[-1], 
                    self.lstm_hidden_size
                ).to(combined_features.device)
            return self.dimension_projector(combined_features)
    
    def _get_latent_from_obs(
        self,
        obs: PyTorchObs,
        lstm_states: RNNStates,
        episode_starts: torch.Tensor
    ) -> torch.Tensor:
        """MÃ©todo interno compatÃ­vel com RecurrentPPO"""
        features = self.extract_features(obs)
        
        # Usar processamento inteligente para 48h
        return self._apply_intelligent_48h_processing(features)
    
    def predict_deterministic(self, obs: PyTorchObs, deterministic: bool = True):
        """MÃ©todo determinÃ­stico personalizado"""
        was_training = self.training
        self.eval()
        
        try:
            self.reset_all_internal_states()
            self.force_deterministic_state()
            
            if isinstance(obs, np.ndarray):
                obs = torch.from_numpy(obs).float()
            
            if hasattr(self, 'device'):
                obs = obs.to(self.device)
            
            batch_size = obs.shape[0] if len(obs.shape) > 1 else 1
            device = obs.device
            
            with torch.no_grad():
                from stable_baselines3.common.utils import obs_as_tensor
                obs_tensor = obs_as_tensor(obs, device)
                episode_starts = torch.ones(batch_size, dtype=torch.bool, device=device)
                
                actions, values, log_probs, lstm_states = self.forward(
                    obs_tensor, 
                    lstm_states=None,
                    episode_starts=episode_starts,
                    deterministic=deterministic
                )
                
                return actions.cpu().numpy(), None
                
        finally:
            if was_training:
                self.train()
            else:
                self.eval()
    
    def set_deterministic_mode(self, deterministic: bool = True):
        """ForÃ§a modo determinÃ­stico"""
        if deterministic:
            self.eval()
            self._disable_all_dropouts()
            
            for param in self.parameters():
                param.requires_grad = False
                
            if hasattr(self, 'pattern_memory') and hasattr(self, 'memory_ptr'):
                self.pattern_memory.zero_()
                self.memory_ptr.zero_()
            
            if hasattr(self, 'temporal_attention'):
                self.temporal_attention.dropout = 0.0
                
            print("ðŸ”§ Modo DETERMINÃSTICO ativado")
        else:
            self.train()
            
            for param in self.parameters():
                param.requires_grad = True
                
            print("ðŸ”§ Modo TREINAMENTO ativado")
    
    def _disable_all_dropouts(self):
        """Desabilita todos os dropouts da rede"""
        dropout_count = 0
        
        for module in self.modules():
            if isinstance(module, nn.Dropout):
                module.p = 0.0
                module.eval()
                dropout_count += 1
        
        print(f"ðŸ”§ {dropout_count} camadas de Dropout desabilitadas")
    
    def force_deterministic_state(self):
        """ForÃ§a estado completamente determinÃ­stico"""
        torch.manual_seed(42)
        torch.cuda.manual_seed(42)
        torch.cuda.manual_seed_all(42)
        
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
        self.eval()
        self._disable_all_dropouts()
        
        if hasattr(self, 'pattern_memory') and hasattr(self, 'memory_ptr'):
            self.pattern_memory.zero_()
            self.memory_ptr.zero_()
        
        if hasattr(self, 'temporal_attention'):
            self.temporal_attention.dropout = 0.0
        
        for param in self.parameters():
            param.requires_grad = False
        
        print("ðŸ”§ ESTADO DETERMINÃSTICO TOTAL ativado")
    
    def reset_memory_bank(self):
        """Reseta Memory Bank para estado inicial"""
        if hasattr(self, 'pattern_memory') and hasattr(self, 'memory_ptr'):
            self.pattern_memory.zero_()
            self.memory_ptr.zero_()
        
        if hasattr(self, 'memory_1h') and hasattr(self, 'memory_ptr_1h'):
            self.memory_1h.zero_()
            self.memory_ptr_1h.zero_()
        
        if hasattr(self, 'memory_4h') and hasattr(self, 'memory_ptr_4h'):
            self.memory_4h.zero_()
            self.memory_ptr_4h.zero_()
        
        if hasattr(self, 'memory_48h') and hasattr(self, 'memory_ptr_48h'):
            self.memory_48h.zero_()
            self.memory_ptr_48h.zero_()
        
        print("ðŸ”§ Memory Bank resetado")
    
    def reset_all_internal_states(self):
        """Reseta TODOS os estados internos"""
        self.reset_memory_bank()
        
        for module in self.modules():
            if isinstance(module, (nn.LSTM, nn.GRU)):
                module.reset_parameters()
        
        for name, buffer in self.named_buffers():
            if 'running' in name or 'num_batches' in name:
                buffer.zero_()
        
        self.zero_grad()
        
        print("ðŸ”§ TODOS os estados internos resetados")


def get_intelligent_48h_kwargs() -> Dict[str, Any]:
    """ConfiguraÃ§Ãµes para V4Intelligent48h"""
    return {
        # ðŸš€ BASE COMPROVADA (herdada)
        'lstm_hidden_size': 128,
        'n_lstm_layers': 2,
        'attention_heads': 8,
        'gru_enabled': True,
        'pattern_recognition': True,
        
        # ðŸš€ MELHORIAS CONSERVADORAS (herdadas)
        'adaptive_lr': True,
        'gradient_clipping': True,
        'feature_weighting': True,
        'dynamic_attention': True,
        'memory_bank_size': 100,
        
        # ðŸš€ MELHORIAS INTELIGENTES PARA 48H (novas)
        'enable_temporal_horizon': True,
        'enable_multi_timeframe': True,
        'enable_advanced_memory': True,
        'enable_dynamic_risk': True,
        'enable_regime_intelligence': True,
        'enable_lookahead': True,
        
        # ðŸš€ FEATURES EXTRACTOR (COMPROVADO)
        'features_extractor_class': TradingTransformerFeatureExtractor,
        'features_extractor_kwargs': {
            'features_dim': 128,
            'seq_len': 10,
        },
        
        # ðŸš€ NETWORK ARCHITECTURE (COMPROVADO)
        'net_arch': [dict(pi=[320, 256, 128], vf=[320, 256, 128])],
        'ortho_init': True,
        'activation_fn': nn.ReLU,
    } 