#!/usr/bin/env python3
"""
ðŸš¨ CORREÃ‡ÃƒO EMERGENCIAL PARA GRADIENT VANISHING
Sistema para detectar e corrigir gradientes que estÃ£o desaparecendo
"""

import torch
import torch.nn as nn
import numpy as np

def emergency_gradient_fix(model, min_grad_norm=1e-6, lstm_lr_multiplier=3.0):
    """
    CorreÃ§Ã£o emergencial para gradientes vanishing
    """
    print("ðŸš¨ APLICANDO CORREÃ‡ÃƒO EMERGENCIAL DE GRADIENTES")
    
    fixes_applied = 0
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            zero_ratio = (param.grad.abs() < 1e-8).float().mean().item()
            
            # Detectar gradientes vanishing crÃ­ticos
            if zero_ratio > 0.7 and 'lstm' in name.lower():
                print(f"ðŸš¨ GRADIENT VANISHING CRÃTICO: {name}")
                print(f"   Zero ratio: {zero_ratio:.1%}")
                print(f"   Grad norm: {grad_norm:.2e}")
                
                # CORREÃ‡ÃƒO 1: Reescalar gradientes LSTM
                if 'weight_hh' in name or 'bias_hh' in name:
                    param.grad.data *= lstm_lr_multiplier
                    print(f"   âœ… Gradiente reescalado por {lstm_lr_multiplier}x")
                    fixes_applied += 1
                
                # CORREÃ‡ÃƒO 2: Adicionar ruÃ­do para quebrar simetria
                if zero_ratio > 0.8:
                    noise = torch.randn_like(param.grad) * 1e-6
                    param.grad.data += noise
                    print(f"   âœ… RuÃ­do adicionado para quebrar simetria")
                    fixes_applied += 1
            
            # Detectar outros gradientes problemÃ¡ticos
            elif zero_ratio > 0.5:
                print(f"âš ï¸  Gradiente com muitos zeros: {name} ({zero_ratio:.1%})")
    
    print(f"ðŸ”§ Total de correÃ§Ãµes aplicadas: {fixes_applied}")
    return fixes_applied

def create_emergency_gradient_callback():
    """Cria callback para correÃ§Ã£o emergencial"""
    
    class EmergencyGradientCallback:
        def __init__(self):
            self.step_count = 0
            self.last_fix_step = 0
            
        def __call__(self, model):
            self.step_count += 1
            
            # Aplicar correÃ§Ã£o a cada 1000 steps ou se detectar problema severo
            if self.step_count % 1000 == 0 or self.step_count - self.last_fix_step > 5000:
                fixes = emergency_gradient_fix(model)
                if fixes > 0:
                    self.last_fix_step = self.step_count
                    print(f"ðŸš¨ CorreÃ§Ã£o emergencial aplicada no step {self.step_count}")
    
    return EmergencyGradientCallback()

if __name__ == "__main__":
    print("ðŸš¨ Sistema de CorreÃ§Ã£o Emergencial de Gradientes")
    print("   - Detecta gradient vanishing")
    print("   - Reescala gradientes LSTM")
    print("   - Adiciona ruÃ­do para quebrar simetria")
    print("   - Monitora zero ratios crÃ­ticos")