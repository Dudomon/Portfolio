# üéØ SUGEST√ïES DE MELHORIA PARA PERFORMANCE DE TRADING - V11Sigmoid

**Data**: 2025-10-13
**Contexto**: An√°lise do sistema V11Sigmoid, cherry.py e reward system V3 Brutal
**Objetivo**: Identificar oportunidades mal aproveitadas que podem melhorar significativamente a performance de trading

---

## üìä SITUA√á√ÉO ATUAL

### ‚úÖ Pontos Fortes Identificados:
1. **Arquitetura H√≠brida LSTM+GRU**: Boa combina√ß√£o de mem√≥ria longo prazo (LSTM) + reatividade (GRU)
2. **Market Context Encoder**: Detec√ß√£o de 4 regimes de mercado (Bull/Bear/Sideways/Volatile)
3. **Reward System V3 Brutal**: Sistema focado em PnL real (70%) + Shaping (30%)
4. **Corre√ß√£o de Vi√©s LONG**: Observation space balanceado ap√≥s fix do D√©cimo

### ‚ö†Ô∏è Problemas Cr√≠ticos Identificados:

#### 1. **üö® HYBRID FUSION SUB-UTILIZADA**
**Localiza√ß√£o**: `two_head_v11_sigmoid.py:380-386`

```python
self.hybrid_fusion = nn.Sequential(
    nn.Linear(self.v8_lstm_hidden * 2, self.v8_lstm_hidden),  # 512->256
    nn.LeakyReLU(negative_slope=0.01),
    nn.LayerNorm(self.v8_lstm_hidden),
    nn.Dropout(0.05)
)
```

**Problema**: A fus√£o LSTM+GRU est√° desperdi√ßando informa√ß√£o valiosa ao comprimir 512D ‚Üí 256D logo ap√≥s concatenar.

**Impacto**:
- Perde diferencia√ß√£o entre padr√µes de longo prazo (LSTM) e curto prazo (GRU)
- Dropout de 5% √© insuficiente para prevenir overfitting na fus√£o
- N√£o h√° mecanismo de aten√ß√£o para pesar dinamicamente LSTM vs GRU

**Proposta de Solu√ß√£o**:
```python
# OP√á√ÉO 1: Fus√£o com Aten√ß√£o (RECOMENDADO)
self.hybrid_attention = nn.Sequential(
    nn.Linear(self.v8_lstm_hidden * 2, 2),  # 512->2 (weights para LSTM e GRU)
    nn.Softmax(dim=-1)
)
self.hybrid_fusion = nn.Sequential(
    nn.Linear(self.v8_lstm_hidden * 2, self.v8_lstm_hidden * 2),  # 512->512 (mant√©m info)
    nn.LeakyReLU(negative_slope=0.01),
    nn.LayerNorm(self.v8_lstm_hidden * 2),
    nn.Dropout(0.15),  # Aumentar regulariza√ß√£o
    nn.Linear(self.v8_lstm_hidden * 2, self.v8_lstm_hidden),  # 512->256 (final)
    nn.LayerNorm(self.v8_lstm_hidden)
)

# OP√á√ÉO 2: Fus√£o Residual (MAIS SIMPLES)
self.hybrid_fusion = nn.Sequential(
    nn.Linear(self.v8_lstm_hidden * 2, self.v8_lstm_hidden),
    nn.LeakyReLU(negative_slope=0.01),
    nn.LayerNorm(self.v8_lstm_hidden),
    nn.Dropout(0.15)
)
# + adicionar conex√£o residual no forward:
# fused = self.hybrid_fusion(hybrid_input) + lstm_out.mean(dim=1, keepdim=True)
```

---

#### 2. **üéØ MARKET CONTEXT ENCODER LIMITADO**
**Localiza√ß√£o**: `two_head_v11_sigmoid.py:41-107`

**Problema**: O Market Context detecta apenas 4 regimes gen√©ricos, mas N√ÉO usa features cr√≠ticas de mercado:
- N√£o considera volatilidade atual (ATR, volatility_regime)
- N√£o considera momentum (returns recentes, trend_strength)
- N√£o considera suporte/resist√™ncia (support_resistance feature)
- Embedding de regime (32D) √© sub-utilizado

**Impacto**:
- Entry/Management heads recebem contexto **gen√©rico** ao inv√©s de **espec√≠fico**
- Modelo pode entrar LONG em regime "Bull" mesmo se volatilidade est√° extrema ou pr√≥ximo de resist√™ncia forte

**Proposta de Solu√ß√£o**:
```python
class EnhancedMarketContextEncoder(nn.Module):
    """üåç Enhanced Market Context - USA FEATURES DO AMBIENTE"""

    def __init__(self, input_dim: int = 256, context_dim: int = 64, market_features_dim: int = 7):
        super().__init__()

        # Detector de regime (4 regimes b√°sicos)
        self.regime_detector = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.LeakyReLU(negative_slope=0.01),
            nn.LayerNorm(128),
            nn.Dropout(0.05),
            nn.Linear(128, 64),
            nn.LeakyReLU(negative_slope=0.01),
            nn.Linear(64, 4)
        )

        # Embedding do regime
        self.regime_embedding = nn.Embedding(4, 32)

        # üéØ NOVO: Market Features Processor
        # Processa: [volatility_regime, support_resistance, breakout_strength,
        #            trend_consistency, atr_14_1m, rsi_14_1m, trend_strength_1m]
        self.market_features_processor = nn.Sequential(
            nn.Linear(market_features_dim, 32),
            nn.LeakyReLU(negative_slope=0.01),
            nn.LayerNorm(32),
            nn.Dropout(0.1),
            nn.Linear(32, 32),
            nn.LeakyReLU(negative_slope=0.01)
        )

        # Context processor EXPANDIDO (input_dim + 32 regime + 32 market features)
        self.context_processor = nn.Sequential(
            nn.Linear(input_dim + 32 + 32, context_dim),
            nn.LeakyReLU(negative_slope=0.01),
            nn.LayerNorm(context_dim)
        )

    def forward(self, lstm_features: torch.Tensor, market_features: torch.Tensor = None):
        """
        Args:
            lstm_features: Output do LSTM [batch, seq, 256]
            market_features: Features de mercado [batch, 7] (volatility, S/R, etc)
        """
        # Detectar regime
        regime_logits = self.regime_detector(lstm_features)
        regime_id_tensor = torch.argmax(regime_logits[0], dim=-1)

        # Embedding do regime
        regime_emb = self.regime_embedding(regime_id_tensor)
        if len(lstm_features.shape) == 3:
            batch_size, seq_len = lstm_features.shape[:2]
            regime_emb = regime_emb.unsqueeze(0).unsqueeze(1).expand(batch_size, seq_len, -1)

        # üéØ PROCESSAR MARKET FEATURES
        if market_features is not None:
            market_emb = self.market_features_processor(market_features)
            if len(lstm_features.shape) == 3:
                market_emb = market_emb.unsqueeze(1).expand(batch_size, seq_len, -1)

            # Combinar: LSTM + Regime + Market Features
            combined = torch.cat([lstm_features, regime_emb, market_emb], dim=-1)
        else:
            # Fallback: apenas LSTM + Regime
            combined = torch.cat([lstm_features, regime_emb], dim=-1)

        context_features = self.context_processor(combined)

        info = {'regime_id': regime_id_tensor}
        return context_features, regime_id_tensor, info
```

**Como integrar no cherry.py**:
```python
# Em cherry.py, linha ~3700, passar market features para o modelo:
def _get_market_features_for_context(self):
    """Extrai features de mercado para Market Context Encoder"""
    current_step = self.current_step

    features = np.array([
        self.df['volatility_regime'].iloc[current_step],
        self.df['support_resistance'].iloc[current_step],
        self.df['breakout_strength'].iloc[current_step],
        self.df['trend_consistency'].iloc[current_step],
        self.df['atr_14_1m'].iloc[current_step] / 30.0,  # Normalizar ATR
        self.df['rsi_14_1m'].iloc[current_step] / 100.0,  # Normalizar RSI
        self.df['trend_strength_1m'].iloc[current_step]
    ], dtype=np.float32)

    return features
```

---

#### 3. **üí∞ CRITIC OVERFITTING - DROPOUT INSUFICIENTE**
**Localiza√ß√£o**: `two_head_v11_sigmoid.py:411-425`

```python
self.v8_critic = nn.Sequential(
    nn.Linear(self.v8_lstm_hidden + self.v8_context_dim, 256),
    nn.LeakyReLU(negative_slope=0.01),
    nn.LayerNorm(256),
    nn.Dropout(0.2),  # 20% dropout
    nn.Linear(256, 128),
    nn.LeakyReLU(negative_slope=0.01),
    nn.LayerNorm(128),
    nn.Dropout(0.2),  # 20% dropout
    nn.Linear(128, 64),
    nn.LeakyReLU(negative_slope=0.01),
    nn.Dropout(0.1),
    nn.Linear(64, 1)
)
```

**Problema**:
- Dropout 20% √© **moderado** mas insuficiente para prevenir overfitting em 4.2M steps
- Critic est√° super-otimizado (`critic_learning_rate: 4.0e-05`) mas sem regulariza√ß√£o forte
- Sem weight decay na defini√ß√£o da rede
- Ultra_reliable_peaks mostram Sharpe 7.96 no step 4.2M mas portfolio apenas $650 ‚Üí poss√≠vel overfitting

**Impacto**:
- Critic pode estar **superestimando** valores esperados
- Leva policy a tomar a√ß√µes "safe" demais (HOLD predominante)
- Explica√ß√£o do vi√©s HOLD observado em testes recentes

**Proposta de Solu√ß√£o**:
```python
# OP√á√ÉO 1: Aumentar Dropout Progressivamente
self.v8_critic = nn.Sequential(
    nn.Linear(self.v8_lstm_hidden + self.v8_context_dim, 256),
    nn.LeakyReLU(negative_slope=0.01),
    nn.LayerNorm(256),
    nn.Dropout(0.3),  # ‚¨ÜÔ∏è 20% -> 30%
    nn.Linear(256, 128),
    nn.LeakyReLU(negative_slope=0.01),
    nn.LayerNorm(128),
    nn.Dropout(0.25),  # ‚¨ÜÔ∏è 20% -> 25%
    nn.Linear(128, 64),
    nn.LeakyReLU(negative_slope=0.01),
    nn.LayerNorm(64),  # Adicionar LayerNorm
    nn.Dropout(0.2),   # ‚¨ÜÔ∏è 10% -> 20%
    nn.Linear(64, 1)
)

# OP√á√ÉO 2: Adicionar L2 Regularization no optimizer
# Em cherry.py BEST_PARAMS, adicionar:
"critic_kwargs": {
    "weight_decay": 1e-4  # L2 regularization
}
```

---

#### 4. **üéØ REWARD SYSTEM: SL/TP GAMING AINDA POSS√çVEL**
**Localiza√ß√£o**: `reward_daytrade_v3_brutal.py:1227-1300`

**Problema**: Anti-gaming system detecta SL m√≠nimo (‚â§10.2pt) e TP m√°ximo (‚â•17.8pt), mas:
- Penalty de -0.15 √ó multiplier (max -0.75) √© **insuficiente** comparado ao reward de TP hit (+0.20)
- Modelo pode "gamar" mantendo SL m√≠nimo + TP m√°ximo por curto per√≠odo e fechando r√°pido
- TP realista bonus (+0.10) n√£o compensa suficientemente TPs no range 12-18pt vs TP no cap

**Impacto**:
- Modelo pode estar aprendendo a "apertar" SL demais (10-11pt)
- TPs podem estar indo para extremos (17-18pt) ao inv√©s de m√©dio (14-15pt)

**Proposta de Solu√ß√£o**:
```python
def _calculate_sltp_gaming_penalty(self, env) -> float:
    """
    üö® PENALIDADE BRUTAL AUMENTADA: Gaming de SL/TP
    """
    try:
        penalty = 0.0
        positions = getattr(env, 'positions', [])

        for position in positions:
            # ... c√≥digo existente ...

            # üö® GAMING DETECTION #1: SL no m√≠nimo - PENALTY AUMENTADA
            if sl_distance <= 10.2:
                # ‚¨ÜÔ∏è AUMENTAR de -0.05 para -0.12 (2.4x mais forte)
                penalty -= 0.12 * max(1, duration / 10)

            # üö® GAMING DETECTION #2: TP no m√°ximo - PENALTY AUMENTADA
            if tp_distance >= 17.8:
                # ‚¨ÜÔ∏è AUMENTAR de -0.05 para -0.12 (2.4x mais forte)
                penalty -= 0.12 * max(1, duration / 10)

            # üö® GAMING DETECTION #3: COMBO SL MIN + TP MAX
            if sl_distance <= 10.2 and tp_distance >= 17.8:
                # ‚¨ÜÔ∏è AUMENTAR de -0.15 para -0.35 (2.3x mais forte)
                multiplier = min(duration / 5, 5.0)
                penalty -= 0.35 * multiplier  # At√© -1.75 por posi√ß√£o!

            # üéØ NOVO: BONUS POR SL/TP NO SWEET SPOT
            # Recompensar ativamente SL 12-14pt e TP 14-16pt
            if 12 <= sl_distance <= 14 and 14 <= tp_distance <= 16:
                # SWEET SPOT: SL e TP ideais
                bonus = 0.08 * min(duration / 5, 2.0)  # Max +0.16
                penalty += bonus  # Adiciona bonus (reduz penalty total)

        return max(penalty, -3.5)  # ‚¨ÜÔ∏è Cap aumentado de -2.5 para -3.5
    except Exception as e:
        return 0.0
```

---

#### 5. **üìä TREND FOLLOWING REWARD - SIM√âTRICO MAS FRACO**
**Localiza√ß√£o**: `reward_daytrade_v3_brutal.py:1153-1225`

**Problema**:
- Reward trend following √© sim√©trico (+0.15 LONG em uptrend, -0.15 SHORT em uptrend)
- MAS: Magnitude de 0.15 √© **baixa** comparada a outros rewards
- TP hit reward (+0.20) > Trend following (+0.15) ‚Üí modelo pode ignorar tend√™ncia
- Apenas usa `trend_consistency` + `returns_1m`, ignora `trend_strength_1m`

**Impacto**:
- Modelo pode abrir posi√ß√µes contra-tend√™ncia se "achar" que vai ganhar TP r√°pido
- N√£o h√° incentivo **forte** suficiente para operar a favor da tend√™ncia

**Proposta de Solu√ß√£o**:
```python
def _calculate_trend_following_reward(self, env) -> float:
    """
    üéØ TREND FOLLOWING REWARD AMPLIFICADO
    Usar trend_strength_1m + trend_consistency para reward mais forte
    """
    try:
        reward = 0.0
        df = getattr(env, 'df', None)
        current_step = getattr(env, 'current_step', 0)

        if df is None or 'trend_consistency' not in df.columns:
            return 0.0

        if current_step >= len(df):
            return 0.0

        # Pegar trend_consistency E trend_strength
        trend_consistency = df['trend_consistency'].iloc[current_step]
        trend_strength = df.get('trend_strength_1m', pd.Series([0.0])).iloc[current_step]

        # Detectar dire√ß√£o do trend
        if 'returns_1m' in df.columns and current_step >= 10:
            recent_returns = df['returns_1m'].iloc[max(0, current_step-10):current_step].values
            avg_return = recent_returns.mean() if len(recent_returns) > 0 else 0

            positions = getattr(env, 'positions', [])

            for pos in positions:
                if not isinstance(pos, dict):
                    continue

                pos_type = pos.get('type', '')

                # üéØ AMPLIFICAR REWARD baseado em trend_strength
                # trend_strength (0-1): quanto mais forte, maior o multiplicador
                strength_multiplier = 1.0 + (trend_strength * 1.5)  # 1.0x a 2.5x

                # CASO 1: TREND UP FORTE
                if avg_return > 0.001 and trend_consistency > 0.6:
                    if pos_type == 'long':
                        # LONG em uptrend = BOM! (amplificado por strength)
                        base_reward = 0.25  # ‚¨ÜÔ∏è Aumentado de 0.15 para 0.25
                        reward += base_reward * trend_consistency * strength_multiplier
                    elif pos_type == 'short':
                        # SHORT em uptrend = BURRICE! (penalty amplificada)
                        base_penalty = -0.25  # ‚¨ÜÔ∏è Aumentado de -0.15 para -0.25
                        reward += base_penalty * trend_consistency * strength_multiplier

                # CASO 2: TREND DOWN FORTE
                elif avg_return < -0.001 and trend_consistency > 0.6:
                    if pos_type == 'short':
                        # SHORT em downtrend = BOM!
                        base_reward = 0.25
                        reward += base_reward * trend_consistency * strength_multiplier
                    elif pos_type == 'long':
                        # LONG em downtrend = BURRICE!
                        base_penalty = -0.25
                        reward += base_penalty * trend_consistency * strength_multiplier

        return max(min(reward, 0.6), -0.6)  # ‚¨ÜÔ∏è Cap aumentado de ¬±0.3 para ¬±0.6

    except Exception as e:
        return 0.0
```

---

#### 6. **üîß HIPERPAR√ÇMETROS: ENT_COEF BAIXO**
**Localiza√ß√£o**: `cherry.py:3505, 3519`

```python
"ent_coef": 0.08,  # Entropy coefficient
```

**Problema**:
- `ent_coef` de 0.08 √© **BAIXO** para um modelo que est√° mostrando comportamento conservador (93% HOLD no D√©cimo 350k)
- Baixa entropia = policy determin√≠stica r√°pido demais = menos explora√ß√£o
- Modelo pode estar convergindo prematuramente para a√ß√µes "safe"

**Impacto**:
- Modelo explora pouco, converge r√°pido para HOLD
- Nunca aprende SHORTs porque n√£o explora suficientemente cen√°rios de downtrend

**Proposta de Solu√ß√£o**:
```python
# OP√á√ÉO 1: Entropy Annealing (RECOMENDADO)
# Come√ßar alto (0.15) e decair progressivamente
PHASE_CONFIGS = {
    "Phase_1_Fundamentals_Extended": {
        "ent_coef": 0.15,  # Alta explora√ß√£o no in√≠cio
        # ...
    },
    "Phase_2_Risk_Management": {
        "ent_coef": 0.12,  # Moderada explora√ß√£o
        # ...
    },
    "Phase_3_Noise_Handling_Fixed": {
        "ent_coef": 0.10,  # Reduzindo explora√ß√£o
        # ...
    },
    "Phase_4_Integration": {
        "ent_coef": 0.08,  # Baixa explora√ß√£o
        # ...
    },
    "Phase_5_Stress_Testing": {
        "ent_coef": 0.06,  # M√≠nima explora√ß√£o
        # ...
    }
}

# OP√á√ÉO 2: Entropy Fixo Maior
BEST_PARAMS = {
    # ...
    "ent_coef": 0.12,  # ‚¨ÜÔ∏è Aumentar de 0.08 para 0.12
    # ...
}
```

---

## üéØ PRIORIZA√á√ÉO DAS MELHORIAS

### üî• **PRIORIDADE ALTA** (Implementar AGORA):
1. **Market Context Encoder com Features Reais** (#2) ‚Üí **+30-40% impacto esperado**
   - Raz√£o: Heads est√£o tomando decis√µes sem ver volatilidade, S/R, breakout
   - Implementa√ß√£o: M√©dio esfor√ßo (2-3h)

2. **Trend Following Reward Amplificado** (#5) ‚Üí **+25-35% impacto esperado**
   - Raz√£o: Reward atual √© fraco demais vs outros incentivos
   - Implementa√ß√£o: Baixo esfor√ßo (30min)

3. **Entropy Coefficient Annealing** (#6) ‚Üí **+20-30% impacto esperado**
   - Raz√£o: Aumenta explora√ß√£o, crucial para aprender SHORTs
   - Implementa√ß√£o: Baixo esfor√ßo (15min)

### ‚ö†Ô∏è **PRIORIDADE M√âDIA** (Implementar na sequ√™ncia):
4. **SL/TP Gaming Penalty Refor√ßada** (#4) ‚Üí **+15-20% impacto esperado**
   - Raz√£o: Previne gaming, mas modelo j√° tem outros incentivos
   - Implementa√ß√£o: Baixo esfor√ßo (30min)

5. **Critic Dropout Aumentado** (#3) ‚Üí **+10-15% impacto esperado**
   - Raz√£o: Previne overfitting, mas apenas ap√≥s 3M+ steps
   - Implementa√ß√£o: Baixo esfor√ßo (10min)

### üîß **PRIORIDADE BAIXA** (Opcional, longo prazo):
6. **Hybrid Fusion com Aten√ß√£o** (#1) ‚Üí **+5-10% impacto esperado**
   - Raz√£o: Melhoria arquitetural sutil, requer re-treino completo
   - Implementa√ß√£o: Alto esfor√ßo (4-6h + re-treino)

---

## üìã PLANO DE IMPLEMENTA√á√ÉO SUGERIDO

### **FASE 1: Quick Wins (1 dia)**
1. Implementar Trend Following Reward Amplificado
2. Implementar Entropy Coefficient Annealing
3. Implementar SL/TP Gaming Penalty Refor√ßada
4. Testar em treino de 500k steps

**Resultado Esperado**: +40-60% melhoria em atividade de trading, modelo come√ßa a aprender SHORTs

---

### **FASE 2: Context Enhancement (2-3 dias)**
1. Implementar EnhancedMarketContextEncoder
2. Modificar cherry.py para passar market features
3. Atualizar forward passes da V11Sigmoid
4. Testar em treino de 1M steps

**Resultado Esperado**: +50-70% melhoria em qualidade das entradas, SL/TP mais inteligentes

---

### **FASE 3: Regularization (1 dia)**
1. Aumentar Critic Dropout
2. Adicionar L2 Weight Decay
3. Treino completo 5M steps

**Resultado Esperado**: Melhor generaliza√ß√£o, menos overfitting em checkpoints tardios

---

## üìà M√âTRICAS PARA AVALIAR SUCESSO

### **Antes das Melhorias** (baseline atual):
- **D√©cimo 350k**: 6.8% LONG, 0% SHORT, 93.2% HOLD
- **Nineth 3.95M**: 28% LONG, 0% SHORT, 72% HOLD (em 500 steps)
- **Sharpe no pico**: ~7.96 (step 4.2M)
- **Portfolio no pico**: ~$650

### **Ap√≥s Melhorias** (meta):
- **Atividade**: ‚â•15% LONG, ‚â•5% SHORT, ‚â§80% HOLD
- **Balance L/S**: Ratio entre 1.5-3.0 (sem vi√©s estrutural)
- **Sharpe sustentado**: ‚â•6.0 por 500k+ steps
- **Portfolio crescimento**: $700-$900 em picos confi√°veis
- **Trend Following**: ‚â•70% posi√ß√µes a favor da tend√™ncia detectada

---

## üí° OBSERVA√á√ïES FINAIS

1. **N√ÉO implementar tudo de uma vez** - testar incrementalmente para isolar impactos
2. **Priorizar #2, #5, #6** - s√£o os quick wins com maior ROI esperado
3. **Monitorar vi√©s LONG/SHORT** - ap√≥s mudan√ßas, rodar `test_nineth_balance.py` a cada 250k steps
4. **Considerar curriculum learning** - come√ßar rewards de trend following baixos e aumentar ap√≥s 1M steps
5. **Documentar checkpoints** - salvar modelos a cada mudan√ßa para poder reverter se necess√°rio

---

## üî¨ AN√ÅLISE T√âCNICA COMPLEMENTAR

### **Por que Market Context √© cr√≠tico?**
Entry/Management heads recebem apenas 64D de contexto gen√©rico. Features cr√≠ticas como:
- `volatility_regime` (0-1): indica se volatilidade est√° extrema
- `support_resistance` (0-1): indica proximidade de S/R forte
- `breakout_strength` (0-1): indica for√ßa de rompimento

Est√£o sendo **ignoradas** pela policy. Isso for√ßa o modelo a "adivinhar" essas condi√ß√µes apenas pelo hist√≥rico de pre√ßos, desperdi√ßando features j√° calculadas.

### **Por que Trend Following precisa ser mais forte?**
An√°lise dos rewards:
- TP hit: +0.20 (evento raro)
- Trend following correto: +0.15 (evento frequente)
- SL near-miss: +0.10 (evento ocasional)

Propor√ß√£o inadequada: modelo pode preferir "gamble" em TP contra-tend√™ncia (+0.20) ao inv√©s de seguir tend√™ncia (+0.15).

### **Por que Entropy importa para SHORTs?**
Com `ent_coef=0.08`:
- Policy converge r√°pido para a√ß√µes determin√≠sticas
- SHORTs s√£o a√ß√µes raras (< 0.1% do tempo)
- Baixa entropia = nunca explora a√ß√µes raras suficientemente
- Resultado: modelo nunca aprende SHORTs naturalmente

Aumentando para 0.12-0.15 inicialmente:
- Policy mant√©m explora√ß√£o por mais tempo
- Modelo experimenta SHORTs em downtrends
- Feedback positivo (reward) refor√ßa SHORT quando apropriado
- Converg√™ncia natural para policy balanceada

---

**FIM DO RELAT√ìRIO**
