#!/usr/bin/env python3
"""
üìä COMPARA√á√ÉO 8M vs 10M STEPS - DAYTRADER V7
Testa checkpoint 8M e compara performance com 10M
"""

import sys
import os
import traceback
from datetime import datetime
sys.path.append("D:/Projeto")

import numpy as np
import pandas as pd
import torch

# Configura√ß√£o espec√≠fica - USAR OS MESMOS CHECKPOINTS QUE A AVALIA√á√ÉO ANTERIOR
CHECKPOINT_8M = "D:/Projeto/Otimizacao/treino_principal/models/DAYTRADER/DAYTRADER_phase4stresstesting_8000000_steps_20250811_195650.zip"
CHECKPOINT_10M = "D:/Projeto/Otimizacao/treino_principal/models/DAYTRADER/DAYTRADER_phase5integration_10000000_steps_20250812_003213.zip"  # MESMO USADO ANTES

INITIAL_PORTFOLIO = 500.0
BASE_LOT_SIZE = 0.02
MAX_LOT_SIZE = 0.03
TEST_STEPS = 3000
NUM_EPISODES = 3
EPISODE_SPACING = 5000

def test_checkpoint(checkpoint_path, checkpoint_name):
    """üéØ Teste de um checkpoint espec√≠fico"""
    
    print(f"\n{'=' * 80}")
    print(f"üéØ TESTANDO {checkpoint_name}")
    print(f"üìÇ {os.path.basename(checkpoint_path)}")
    print(f"{'=' * 80}")
    
    try:
        # Imports
        from sb3_contrib import RecurrentPPO
        from daytrader import TradingEnv
        
        print("‚úÖ Imports carregados")
        
        # Dataset real para teste
        print("üìä Carregando dataset para teste...")
        dataset_path = "D:/Projeto/data/GC_YAHOO_ENHANCED_V3_BALANCED_20250804_192226.csv"
        
        if not os.path.exists(dataset_path):
            print(f"‚ùå Dataset n√£o encontrado: {dataset_path}")
            return None
            
        df = pd.read_csv(dataset_path)
        
        # Processar dataset
        if 'time' in df.columns:
            df['timestamp'] = pd.to_datetime(df['time'])
            df.set_index('timestamp', inplace=True)
            df.drop('time', axis=1, inplace=True)
        
        # Renomear colunas
        df = df.rename(columns={
            'open': 'open_5m',
            'high': 'high_5m',
            'low': 'low_5m', 
            'close': 'close_5m',
            'tick_volume': 'volume_5m'
        })
        
        total_len = len(df)
        print(f"‚úÖ Dataset carregado: {total_len:,} barras")
        
        # Par√¢metros de trading
        trading_params = {
            'base_lot_size': BASE_LOT_SIZE,
            'max_lot_size': MAX_LOT_SIZE,
            'initial_balance': INITIAL_PORTFOLIO,
            'target_trades_per_day': 18,
            'stop_loss_range': (2.0, 8.0),
            'take_profit_range': (3.0, 15.0)
        }
        
        # Carregar modelo
        print(f"ü§ñ Carregando {checkpoint_name}...")
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        try:
            model = RecurrentPPO.load(checkpoint_path, device=device)
            print("‚úÖ Carregamento bem-sucedido")
        except Exception as e:
            print(f"‚ùå Erro ao carregar modelo: {e}")
            return None
            
        model.policy.set_training_mode(False)
        print(f"‚úÖ Modelo em modo infer√™ncia")
        
        # Executar testes
        print(f"üöÄ Iniciando {NUM_EPISODES} epis√≥dios...")
        
        all_episodes = []
        total_returns = []
        
        for episode_num in range(NUM_EPISODES):
            print(f"\nüéÆ EPIS√ìDIO {episode_num + 1}/{NUM_EPISODES}")
            
            # Selecionar peda√ßo do dataset
            start_idx = episode_num * EPISODE_SPACING
            if start_idx + TEST_STEPS + 100 > total_len:
                start_idx = total_len - TEST_STEPS - 100
            
            end_idx = start_idx + TEST_STEPS + 100
            episode_df = df.iloc[start_idx:end_idx].copy()
            
            print(f"üìä Per√≠odo: {episode_df.index.min()} at√© {episode_df.index.max()}")
            
            # Criar ambiente
            env = TradingEnv(
                episode_df,
                window_size=20,
                is_training=False,
                initial_balance=INITIAL_PORTFOLIO,
                trading_params=trading_params
            )
            
            # Executar epis√≥dio
            obs = env.reset()
            lstm_states = None
            done = False
            step = 0
            
            trades_count = 0
            actions_log = []
            
            while not done and step < TEST_STEPS:
                # Predi√ß√£o em modo infer√™ncia
                action, lstm_states = model.predict(obs, state=lstm_states, deterministic=False)
                obs, reward, done, info = env.step(action)
                
                # Log da a√ß√£o
                actions_log.append({
                    'step': step,
                    'entry_decision': int(action[0]),
                    'entry_quality': float(action[1]) if len(action) > 1 else 0.0,
                    'portfolio_value': env.portfolio_value
                })
                
                # Contar trades
                if hasattr(info, 'get') and info.get('trade_closed', False):
                    trades_count += 1
                    if trades_count <= 3:  # Log primeiros 3 trades
                        print(f"  üíº Trade #{trades_count}: {info.get('trade_type', 'unknown')} PnL=${info.get('trade_pnl', 0):.2f}")
                
                if (step + 1) % 1500 == 0:
                    print(f"  Step {step+1}/{TEST_STEPS} - Portfolio: ${env.portfolio_value:.2f} - Trades: {trades_count}")
                
                step += 1
            
            # An√°lise do epis√≥dio
            final_portfolio = env.portfolio_value
            episode_return = ((final_portfolio - INITIAL_PORTFOLIO) / INITIAL_PORTFOLIO) * 100
            
            episode_result = {
                'episode': episode_num + 1,
                'period': f"{episode_df.index.min()} at√© {episode_df.index.max()}",
                'initial_portfolio': INITIAL_PORTFOLIO,
                'final_portfolio': final_portfolio,
                'return_pct': episode_return,
                'trades_count': trades_count,
                'actions_log': actions_log
            }
            
            all_episodes.append(episode_result)
            total_returns.append(episode_return)
            
            print(f"‚úÖ Resultado: ${INITIAL_PORTFOLIO:.2f} ‚Üí ${final_portfolio:.2f} ({episode_return:+.2f}%) - {trades_count} trades")
            
            # Limpeza
            del env
            del episode_df
        
        # An√°lise final
        avg_return = np.mean(total_returns)
        std_return = np.std(total_returns)
        min_return = min(total_returns)
        max_return = max(total_returns)
        positive_episodes = len([r for r in total_returns if r > 0])
        total_trades = sum(ep['trades_count'] for ep in all_episodes)
        
        # An√°lise de a√ß√µes do √∫ltimo epis√≥dio
        last_actions = all_episodes[-1]['actions_log'] if all_episodes else []
        if last_actions:
            entry_decisions = [a['entry_decision'] for a in last_actions]
            entry_qualities = [a['entry_quality'] for a in last_actions]
            
            hold_pct = (sum(1 for d in entry_decisions if d == 0) / len(entry_decisions)) * 100
            long_pct = (sum(1 for d in entry_decisions if d == 1) / len(entry_decisions)) * 100
            short_pct = (sum(1 for d in entry_decisions if d == 2) / len(entry_decisions)) * 100
            avg_quality = np.mean(entry_qualities)
        else:
            hold_pct = long_pct = short_pct = avg_quality = 0
        
        print(f"\nüìä RESUMO {checkpoint_name}:")
        print(f"üíµ Retorno M√©dio: {avg_return:+.2f}% (œÉ={std_return:.2f}%)")
        print(f"üìà Range: {min_return:+.2f}% at√© {max_return:+.2f}%")
        print(f"üéØ Epis√≥dios Lucrativos: {positive_episodes}/{NUM_EPISODES} ({(positive_episodes/NUM_EPISODES)*100:.1f}%)")
        print(f"üìä Total Trades: {total_trades}")
        print(f"üéÆ A√ß√µes: HOLD={hold_pct:.1f}%, LONG={long_pct:.1f}%, SHORT={short_pct:.1f}%")
        print(f"‚≠ê Entry Quality: {avg_quality:.3f}")
        
        # Limpeza de mem√≥ria
        del model
        torch.cuda.empty_cache()
        
        return {
            'checkpoint_name': checkpoint_name,
            'avg_return': avg_return,
            'std_return': std_return,
            'min_return': min_return,
            'max_return': max_return,
            'positive_episodes': positive_episodes,
            'total_episodes': NUM_EPISODES,
            'total_trades': total_trades,
            'hold_pct': hold_pct,
            'long_pct': long_pct,
            'short_pct': short_pct,
            'avg_quality': avg_quality,
            'all_episodes': all_episodes
        }
        
    except Exception as e:
        print(f\"‚ùå ERRO: {e}\")
        print(f\"Detalhes: {traceback.format_exc()}\")
        return None

def main():
    print("üî• COMPARA√á√ÉO DAYTRADER: 8M vs 10M STEPS")
    print("=" * 80)
    print(f"üí∞ Portfolio Inicial: ${INITIAL_PORTFOLIO}")
    print(f"üìä Epis√≥dios: {NUM_EPISODES} √ó {TEST_STEPS} steps")
    print(f"üéØ Modo: Infer√™ncia n√£o-determin√≠stica")
    print("=" * 80)
    
    # Testar checkpoint 8M
    result_8m = test_checkpoint(CHECKPOINT_8M, "8M STEPS (Phase 4 - Stress Testing)")
    
    # Testar checkpoint 10M  
    result_10m = test_checkpoint(CHECKPOINT_10M, "10M STEPS (Phase 5 - Integration)")
    
    # Compara√ß√£o final
    if result_8m and result_10m:
        print(\"\\n\" + \"=\" * 100)
        print(\"üèÜ COMPARA√á√ÉO FINAL: 8M vs 10M\")
        print(\"=\" * 100)
        
        print(f\"üìà RETORNO M√âDIO:\")
        print(f\"   8M:  {result_8m['avg_return']:+7.2f}% (œÉ={result_8m['std_return']:.2f}%)\")
        print(f\"   10M: {result_10m['avg_return']:+7.2f}% (œÉ={result_10m['std_return']:.2f}%)\")
        
        delta_return = result_8m['avg_return'] - result_10m['avg_return']
        print(f\"   üìä Diferen√ßa: {delta_return:+.2f}% ({'‚úÖ 8M melhor' if delta_return > 0 else '‚ùå 10M melhor'})\")
        
        print(f\"\\nüéØ EPIS√ìDIOS LUCRATIVOS:\")
        print(f\"   8M:  {result_8m['positive_episodes']}/{result_8m['total_episodes']} ({(result_8m['positive_episodes']/result_8m['total_episodes'])*100:.1f}%)\")
        print(f\"   10M: {result_10m['positive_episodes']}/{result_10m['total_episodes']} ({(result_10m['positive_episodes']/result_10m['total_episodes'])*100:.1f}%)\")
        
        print(f\"\\nüìä ATIVIDADE DE TRADING:\")
        print(f\"   8M:  {result_8m['total_trades']} trades totais\")
        print(f\"   10M: {result_10m['total_trades']} trades totais\")
        
        print(f\"\\nüéÆ COMPORTAMENTO (Entry Decisions):\")
        print(f\"   8M:  HOLD={result_8m['hold_pct']:.1f}%, LONG={result_8m['long_pct']:.1f}%, SHORT={result_8m['short_pct']:.1f}%\")
        print(f\"   10M: HOLD={result_10m['hold_pct']:.1f}%, LONG={result_10m['long_pct']:.1f}%, SHORT={result_10m['short_pct']:.1f}%\")
        
        # Determina√ß√£o do vencedor
        score_8m = 0
        score_10m = 0
        
        # Crit√©rio 1: Retorno m√©dio
        if result_8m['avg_return'] > result_10m['avg_return']:
            score_8m += 1
        else:
            score_10m += 1
            
        # Crit√©rio 2: Consist√™ncia (epis√≥dios lucrativos)
        if result_8m['positive_episodes'] > result_10m['positive_episodes']:
            score_8m += 1
        else:
            score_10m += 1
            
        # Crit√©rio 3: Atividade (trading activity)
        if result_8m['total_trades'] > result_10m['total_trades']:
            score_8m += 1
        else:
            score_10m += 1
        
        print(f\"\\nüèÜ VEREDITO FINAL:\")
        print(f\"   Score 8M:  {score_8m}/3\")
        print(f\"   Score 10M: {score_10m}/3\")
        
        if score_8m > score_10m:
            print(\"   ü•á VENCEDOR: 8M STEPS (menos overtraining)\")
            print(\"   üí° RECOMENDA√á√ÉO: Usar checkpoint 8M para produ√ß√£o\")
        elif score_10m > score_8m:
            print(\"   ü•á VENCEDOR: 10M STEPS (mais treinamento)\")
            print(\"   üí° RECOMENDA√á√ÉO: Usar checkpoint 10M para produ√ß√£o\")
        else:
            print(\"   ü§ù EMPATE - ambos equivalentes\")
            print(\"   üí° RECOMENDA√á√ÉO: Usar 8M (menos risco de overtraining)\")
    
    print(f\"\\n‚úÖ COMPARA√á√ÉO CONCLU√çDA - {datetime.now().strftime('%H:%M:%S')}\")

if __name__ == \"__main__\":
    main()