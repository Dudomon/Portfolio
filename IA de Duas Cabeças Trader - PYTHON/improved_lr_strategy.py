#!/usr/bin/env python3
"""
üöÄ ESTRAT√âGIA DE LR MELHORADA PARA LSTM
Baseada na an√°lise do seu sistema atual + necessidades das LSTMs
"""

def analyze_current_lr_system():
    """üîç An√°lise do sistema atual"""
    print("üîç AN√ÅLISE DO SEU SISTEMA DE LR ATUAL")
    print("=" * 60)
    
    current_system = {
        'lr_fixo': {
            'valor': 2.678385767462569e-05,
            'pros': ['Est√°vel', 'Testado', 'Funciona'],
            'contras': ['N√£o adapta', 'Pode ser lento para LSTM']
        },
        'adaptive_callback': {
            'frequencia': 2000,
            'range': '1e-6 a 1e-3',
            'pros': ['Monitora gradientes', 'Adapta automaticamente'],
            'contras': ['S√≥ diminui LR', 'L√≥gica contraproducente', 'Conflita com LR fixo']
        }
    }
    
    print("üìä SISTEMA ATUAL:")
    for system, details in current_system.items():
        print(f"\n{system.upper()}:")
        if 'valor' in details:
            print(f"   Valor: {details['valor']}")
        if 'frequencia' in details:
            print(f"   Frequ√™ncia: {details['frequencia']} steps")
        if 'range' in details:
            print(f"   Range: {details['range']}")
        
        print(f"   ‚úÖ Pr√≥s: {', '.join(details['pros'])}")
        print(f"   ‚ùå Contras: {', '.join(details['contras'])}")
    
    return current_system

def recommend_lstm_lr_strategy():
    """üí° Estrat√©gia recomendada para LSTM"""
    print(f"\nüí° ESTRAT√âGIA RECOMENDADA PARA LSTM")
    print("=" * 60)
    
    strategies = [
        {
            'name': 'OP√á√ÉO 1: LR FIXO + WARMUP (RECOMENDADO)',
            'description': 'Manter LR fixo mas adicionar warmup para LSTM',
            'implementation': '''
def lr_schedule_with_warmup(progress):
    warmup_steps = 0.1  # 10% dos steps
    base_lr = 2.678385767462569e-05  # Seu LR otimizado
    
    if progress < warmup_steps:
        # Warmup: come√ßar com LR baixo
        return base_lr * 0.1 * (progress / warmup_steps)
    else:
        # LR fixo ap√≥s warmup
        return base_lr
            ''',
            'pros': ['Mant√©m estabilidade', 'Ajuda LSTM inicializar', 'Simples'],
            'contras': ['N√£o adapta durante treinamento'],
            'impact': 'Alto para LSTM',
            'risk': 'Baixo'
        },
        {
            'name': 'OP√á√ÉO 2: ADAPTIVE LR MELHORADO',
            'description': 'Corrigir l√≥gica do adaptive LR para LSTM',
            'implementation': '''
def determine_new_lr_for_lstm(gradient_health, lstm_health):
    if lstm_health < 0.1:  # LSTM muito problem√°tica
        return current_lr * 0.5  # Diminuir
    elif lstm_health < 0.2:  # LSTM moderadamente problem√°tica  
        return current_lr * 0.8  # Diminuir pouco
    elif gradient_health > 0.8:  # Gradientes muito ativos
        return current_lr * 1.1  # AUMENTAR (diferente do atual!)
    else:
        return current_lr  # Manter
            ''',
            'pros': ['Adapta especificamente para LSTM', 'Pode aumentar LR'],
            'contras': ['Mais complexo', 'Pode instabilizar'],
            'impact': 'Alto',
            'risk': 'M√©dio'
        },
        {
            'name': 'OP√á√ÉO 3: LR DIFERENCIADO POR LAYER',
            'description': 'LR espec√≠fico para LSTM vs outros layers',
            'implementation': '''
# Separar par√¢metros
lstm_params = []
other_params = []

for name, param in model.policy.named_parameters():
    if 'lstm' in name.lower():
        lstm_params.append(param)
    else:
        other_params.append(param)

# Optimizer com LRs diferentes
optimizer = torch.optim.Adam([
    {'params': lstm_params, 'lr': 1e-4},     # LR menor para LSTM
    {'params': other_params, 'lr': 2.68e-5}  # LR normal
])
            ''',
            'pros': ['Controle fino', 'LSTM pode ter LR pr√≥prio'],
            'contras': ['Requer modifica√ß√£o do PPO', 'Complexo'],
            'impact': 'Muito Alto',
            'risk': 'Alto'
        },
        {
            'name': 'OP√á√ÉO 4: DESABILITAR ADAPTIVE + WARMUP',
            'description': 'Remover adaptive LR conflitante e usar s√≥ warmup',
            'implementation': '''
# 1. Comentar/remover adaptive_lr_callback do daytrader.py
# adaptive_lr_callback = create_adaptive_lr_callback(...)

# 2. Usar LR schedule com warmup
def lr_schedule_lstm_friendly(progress):
    base_lr = 2.678385767462569e-05
    warmup = 0.05  # 5% warmup
    
    if progress < warmup:
        return base_lr * 0.2 * (progress / warmup)  # Come√ßar com 20%
    else:
        return base_lr  # LR fixo testado
            ''',
            'pros': ['Remove conflito', 'Simples', 'Mant√©m LR testado'],
            'contras': ['N√£o adapta durante treinamento'],
            'impact': 'M√©dio',
            'risk': 'Muito Baixo'
        }
    ]
    
    print("üéØ OP√á√ïES DISPON√çVEIS:")
    for i, strategy in enumerate(strategies, 1):
        print(f"\n{i}. {strategy['name']}")
        print(f"   üìù {strategy['description']}")
        print(f"   üéØ Impacto: {strategy['impact']}")
        print(f"   ‚ö†Ô∏è Risco: {strategy['risk']}")
        print(f"   ‚úÖ Pr√≥s: {', '.join(strategy['pros'])}")
        print(f"   ‚ùå Contras: {', '.join(strategy['contras'])}")
    
    return strategies

def generate_immediate_fix():
    """üöÄ Corre√ß√£o imediata recomendada"""
    print(f"\nüöÄ CORRE√á√ÉO IMEDIATA RECOMENDADA")
    print("=" * 60)
    
    print("üéØ IMPLEMENTAR OP√á√ÉO 4: DESABILITAR ADAPTIVE + WARMUP")
    print("\nüìù PASSOS:")
    print("1. Comentar adaptive_lr_callback no daytrader.py")
    print("2. Implementar lr_schedule com warmup")
    print("3. Aplicar gradient clipping 0.5")
    print("4. Aplicar LSTM initialization")
    
    code_changes = {
        'step1': '''
# No daytrader.py, comentar estas linhas:
# adaptive_lr_callback = create_adaptive_lr_callback(
#     initial_lr=BEST_PARAMS["learning_rate"],
#     min_lr=1e-6,
#     max_lr=1e-3,
#     adaptation_freq=2000,
#     verbose=1
# )

# E remover da CallbackList:
combined_callback = CallbackList([
    robust_callback, 
    metrics_callback, 
    progress_callback, 
    gradient_callback,
    zero_debug_callback,
    # adaptive_lr_callback,  # COMENTAR ESTA LINHA
    lstm_rescue_callback
])
        ''',
        
        'step2': '''
# Substituir lr_schedule por:
def lr_schedule_lstm_warmup(progress):
    """LR schedule otimizado para LSTM com warmup"""
    base_lr = 2.678385767462569e-05  # Seu LR testado
    warmup_steps = 0.05  # 5% dos steps para warmup
    
    if progress < warmup_steps:
        # Warmup suave: come√ßar com 20% do LR
        warmup_factor = 0.2 + 0.8 * (progress / warmup_steps)
        return base_lr * warmup_factor
    else:
        # LR fixo ap√≥s warmup (testado e est√°vel)
        return base_lr

# Usar na cria√ß√£o da policy:
policy = TwoHeadV6Intelligent48h(
    observation_space=env.observation_space,
    action_space=env.action_space,
    lr_schedule=lr_schedule_lstm_warmup,  # NOVA FUN√á√ÉO
    lstm_hidden_size=128
)
        '''
    }
    
    print("\nüíª C√ìDIGO PARA APLICAR:")
    for step, code in code_changes.items():
        print(f"\n{step.upper()}:")
        print(code)
    
    return code_changes

if __name__ == "__main__":
    # Analisar sistema atual
    current = analyze_current_lr_system()
    
    # Recomendar estrat√©gias
    strategies = recommend_lstm_lr_strategy()
    
    # Gerar corre√ß√£o imediata
    fix = generate_immediate_fix()
    
    print(f"\n" + "=" * 60)
    print("üéØ RESUMO EXECUTIVO")
    print("=" * 60)
    
    print("üîç PROBLEMA IDENTIFICADO:")
    print("   ‚ùå Adaptive LR conflita com LR fixo")
    print("   ‚ùå Adaptive LR s√≥ diminui (nunca aumenta)")
    print("   ‚ùå LSTM precisa de warmup suave")
    
    print(f"\nüí° SOLU√á√ÉO RECOMENDADA:")
    print("   ‚úÖ DESABILITAR Adaptive LR (conflitante)")
    print("   ‚úÖ IMPLEMENTAR LR Warmup (5% dos steps)")
    print("   ‚úÖ MANTER LR fixo testado (2.68e-5)")
    print("   ‚úÖ APLICAR gradient clipping 0.5")
    
    print(f"\nüöÄ RESULTADO ESPERADO:")
    print("   ‚úÖ LSTM inicializa suavemente")
    print("   ‚úÖ Sem conflitos de LR")
    print("   ‚úÖ Gradientes LSTM: 15.43% ‚Üí <5% zeros")
    print("   ‚úÖ Sistema mais est√°vel")