"""
üöÄ AVALIADOR DE MODELOS - DETEC√á√ÉO AUTOM√ÅTICA DE POL√çTICAS
=========================================================

üî• AVALIADOR AVAN√áADO COM DETEC√á√ÉO AUTOM√ÅTICA V5/V6/V7:
- ACTION SPACE: 11 dimens√µes (compat√≠vel com todas as pol√≠ticas V5/V6/V7)
- DETEC√á√ÉO AUTOM√ÅTICA: TwoHeadV5Intelligent48h, TwoHeadV6Intelligent48h ou TwoHeadV7Simple
- STRATEGIC FUSION LAYER: Suporte completo para modelos com/sem fusion layer
- FEATURE EXTRACTOR: TradingTransformerFeatureExtractor (compat√≠vel)
- VECNORMALIZE: enhanced_normalizer.pkl da pasta "Modelo PPO Trader"
- COMPATIBILIDADE: 100% com modelos V5 e V6 treinados com/sem Strategic Fusion Layer
- SEM REWARD SYSTEM: Avalia√ß√£o s√≥ precisa executar modelo e calcular m√©tricas
- DATASET: Usa mesmo dataset do PPOv1 (Yahoo massivo ou GOLD_final_nostatic)
- EPIS√ìDIOS: 1500 steps cada (padr√£o otimizado)
- SEM SPAM: Logs de trading removidos
- DETEC√á√ÉO INTELIGENTE: Identifica automaticamente V5/V6 com/sem Strategic Fusion
- üéØ V5 SUPPORT: Entry Head Ultra-Especializada + Strategic Fusion Layer
- üéØ V6 SUPPORT: Entry Head Simples + Strategic Fusion Layer
- üéØ V7 SUPPORT: Entry Head com Gates Especializados + Arquitetura Simplificada
- üéØ MODO DETERMIN√çSTICO: Resultados consistentes e reproduz√≠veis (seed=42)
"""

import os
import sys
import time
import numpy as np
import pandas as pd
import gym
from gym import spaces
from sklearn.impute import KNNImputer
from sb3_contrib import RecurrentPPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# üî• CONFIGURAR OUTPUT UNBUFFERED PARA LOGS EM TEMPO REAL
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)

def print_realtime(*args, **kwargs):
    """Print com flush autom√°tico para exibi√ß√£o em tempo real"""
    print(*args, **kwargs)
    sys.stdout.flush()

# üî• IMPORTAR COMPONENTES CORRETOS DO FRAMEWORK
try:
    from trading_framework.policies.two_head_v5_intelligent_48h import TwoHeadV5Intelligent48h
    from trading_framework.policies.two_head_v6_intelligent_48h import TwoHeadV6Intelligent48h
    from trading_framework.policies.two_head_v7_simple import TwoHeadV7Simple
    from trading_framework.policies.two_head_v7_intuition import TwoHeadV7Intuition, get_v7_intuition_kwargs
    from trading_framework.extractors.transformer_extractor import TradingTransformerFeatureExtractor
    print("[EVAL] TwoHeadV5Intelligent48h, TwoHeadV6Intelligent48h, TwoHeadV7Simple, TwoHeadV7Intuition e TradingTransformerFeatureExtractor importados do framework")
except ImportError as e:
    print(f"[EVAL] Erro ao importar do framework: {e}")
    TwoHeadV5Intelligent48h = None
    TwoHeadV6Intelligent48h = None
    TwoHeadV7Simple = None
    TwoHeadV7Intuition = None
    TradingTransformerFeatureExtractor = None

# üî• IMPORTAR CONFIGURA√á√ïES DO PPOv1 PARA COMPATIBILIDADE TOTAL
try:
    # Tentar importar as configura√ß√µes de trading do PPOv1
    from ppov1 import TRIAL_2_TRADING_PARAMS
    PPOV1_TRADING_PARAMS = TRIAL_2_TRADING_PARAMS
    print("[EVAL] ‚úÖ Configura√ß√µes de trading do PPOv1 importadas")
except ImportError:
    # Fallback: usar as mesmas configura√ß√µes definidas no PPOv1
    # ALINHADO COM DAYTRADER.PY - RANGES DAYTRADE CORRETOS
    PPOV1_TRADING_PARAMS = {
        "sl_range_min": 2.0,                     # üöÄ DAYTRADER: 2 pontos (daytrade)
        "sl_range_max": 8.0,                     # üöÄ DAYTRADER: 8 pontos (daytrade)
        "tp_range_min": 3.0,                     # üöÄ DAYTRADER: 3 pontos (daytrade)
        "tp_range_max": 15.0,                    # üöÄ DAYTRADER: 15 pontos (daytrade)
        "target_trades_per_day": 18,             # OTIMIZADO: 16‚Üí18 (+12.5% atividade)
        "portfolio_weight": 0.7878338511058235,  # OTIMIZADO: Peso portfolio ajustado
        "drawdown_weight": 0.5100531293444458,   # OTIMIZADO: Peso drawdown refinado
        "max_drawdown_tolerance": 0.3378997883128378,  # OTIMIZADO: Toler√¢ncia DD ajustada
        "win_rate_target": 0.45,                 # OTIMIZADO: Target win rate refinado
        "momentum_threshold": 0.005,             # OTIMIZADO: Threshold momentum
        "volatility_min": 0.003,                 # OTIMIZADO: Vol mais permissiva
        "volatility_max": 0.015,
    }
    print("[EVAL] ‚ö†Ô∏è Usando configura√ß√µes de trading PPOv1 (fallback)")

# üî• IMPORTAR SISTEMA DE DADOS OTIMIZADO (MESMO DO PPOv1)
def load_optimized_data():
    """
    üöÄ CARREGAR DATASET MASSIVO YAHOO (1.1M BARRAS) OU FALLBACK PARA GOLD_final_nostatic.pkl
    MESMA FUN√á√ÉO DO PPOv1.py
    """
    import time
    
    # üéØ PRIORIDADE 1: Dataset Yahoo massivo (1.1M barras, 15+ anos) - MESMO DO DAYTRADER
    yahoo_cache = "data_cache/GC=F_YAHOO_DAILY_CACHE_20250711_041924.pkl"
    if os.path.exists(yahoo_cache):
        print(f"[YAHOO MASSIVE] üöÄ Carregando dataset Yahoo massivo (1.1M barras)...")
        start_time = time.time()
        df = pd.read_pickle(yahoo_cache)
        load_time = time.time() - start_time
        print(f"[YAHOO MASSIVE] ‚úÖ Dataset Yahoo carregado: {len(df):,} barras")
        print(f"[YAHOO MASSIVE] üìÖ Per√≠odo: {df['time'].min()} at√© {df['time'].max()}")
        print(f"[YAHOO MASSIVE] ‚è±Ô∏è Dura√ß√£o: {(pd.to_datetime(df['time'].max()) - pd.to_datetime(df['time'].min())).days} dias")
        print(f"[YAHOO MASSIVE] ‚ö° Tempo: {load_time:.3f}s")
        print(f"[YAHOO MASSIVE] üéØ Dataset massivo: 15+ anos de dados hist√≥ricos")
        
        # üî• CONVERTER PARA FORMATO PADR√ÉO DO SISTEMA
        df['time'] = pd.to_datetime(df['time'])
        df.set_index('time', inplace=True)
        
        # Renomear colunas para compatibilidade
        column_mapping = {
            'open': 'open_5m',
            'high': 'high_5m', 
            'low': 'low_5m',
            'close': 'close_5m',
            'tick_volume': 'volume_5m'  # üî• CORRE√á√ÉO: usar tick_volume em vez de volume
        }
        df.rename(columns=column_mapping, inplace=True)
        
        # üî• CRIAR COLUNAS DE TIMEFRAMES M√öLTIPLOS (resampling)
        print(f"[YAHOO MASSIVE] üîÑ Criando timeframes m√∫ltiplos via resampling...")
        
        # 15m (agrupar 3 barras de 5m)
        df_15m = df.resample('15T').agg({
            'open_5m': 'first',
            'high_5m': 'max',
            'low_5m': 'min', 
            'close_5m': 'last',
            'volume_5m': 'sum'
        }).rename(columns={
            'open_5m': 'open_15m',
            'high_5m': 'high_15m',
            'low_5m': 'low_15m',
            'close_5m': 'close_15m',
            'volume_5m': 'volume_15m'
        })
        
        # 4h (agrupar 48 barras de 5m)
        df_4h = df.resample('4H').agg({
            'open_5m': 'first',
            'high_5m': 'max',
            'low_5m': 'min',
            'close_5m': 'last', 
            'volume_5m': 'sum'
        }).rename(columns={
            'open_5m': 'open_4h',
            'high_5m': 'high_4h',
            'low_5m': 'low_4h',
            'close_5m': 'close_4h',
            'volume_5m': 'volume_4h'
        })
        
        # üî• COMBINAR TODOS OS TIMEFRAMES
        df_final = pd.concat([df, df_15m, df_4h], axis=1)
        
        # Remover linhas com NaN (in√≠cio dos timeframes maiores)
        df_final = df_final.dropna()
        
        print(f"[YAHOO MASSIVE] ‚úÖ Dataset final criado: {len(df_final):,} barras")
        print(f"[YAHOO MASSIVE] üìä Colunas: {list(df_final.columns)}")
        print(f"[YAHOO MASSIVE] üéØ Timeframes: 5m, 15m, 4h")
        
        return df_final
    
    # üéØ PRIORIDADE 2: Dataset GOLD_final_nostatic.pkl (fallback)
    gold_nostatic_cache = "data_cache/GOLD_final_nostatic.pkl"
    if os.path.exists(gold_nostatic_cache):
        print(f"[FALLBACK] üéØ Carregando dataset GOLD_final_nostatic.pkl...")
        start_time = time.time()
        df = pd.read_pickle(gold_nostatic_cache)
        load_time = time.time() - start_time
        print(f"[FALLBACK] ‚úÖ Dataset GOLD_final_nostatic carregado: {len(df):,} barras")
        print(f"[FALLBACK] üìÖ Per√≠odo: {df.index[0]} at√© {df.index[-1]}")
        print(f"[FALLBACK] ‚è±Ô∏è Dura√ß√£o: {(df.index[-1] - df.index[0]).days} dias")
        print(f"[FALLBACK] ‚ö° Tempo: {load_time:.3f}s")
        return df
    else:
        raise FileNotFoundError("[ERRO CR√çTICO] Nenhum dataset encontrado! Verifique se existe GC=F_YAHOO_DAILY_CACHE_*.pkl ou GOLD_final_nostatic.pkl em 'data_cache/'.")

print("[EVAL] ‚úÖ Sistema de dados otimizado (mesmo do PPOv1) importado")

# üî• SISTEMA DE DETEC√á√ÉO AUTOM√ÅTICA DE MODELOS E POL√çTICAS
def detect_model_type(model_path: str) -> dict:
    """
    üî• DETEC√á√ÉO AUTOM√ÅTICA: Identifica se modelo √© do treinodiff ou AnderV1
    
    Returns:
        dict: Configura√ß√µes espec√≠ficas do modelo detectado
    """
    model_name = os.path.basename(model_path).lower()
    model_dir = os.path.dirname(model_path).lower()
    
    # üî• DETEC√á√ÉO POR NOME/CAMINHO
    is_ander = any(keyword in model_name for keyword in ['ander', 'anderv1']) or \
               any(keyword in model_dir for keyword in ['ander', 'anderv1'])
    
    is_diff = any(keyword in model_name for keyword in ['diff', 'diferenciado', 'treinodiff']) or \
              any(keyword in model_dir for keyword in ['diff', 'diferenciado', 'treinodiff'])
    
    # üî• CONFIGURA√á√ïES ESPEC√çFICAS
    if is_ander:
        config = {
            'type': 'ANDER',
            'initial_balance': 1000,
            'base_lot_size': 0.2,
            'max_lot_size': 0.3,
            'description': 'Modelo AnderV1.py (Portfolio $1000, Lots 0.2-0.3)'
        }
    elif is_diff:
        config = {
            'type': 'DIFF',
            'initial_balance': 500,
            'base_lot_size': 0.02,
            'max_lot_size': 0.03,
            'description': 'Modelo TreinoDiff (Portfolio $500, Lots 0.02-0.03)'
        }
    else:
        # üî• FALLBACK: Detectar por tamanho do arquivo ou outros indicadores
        try:
            file_size_mb = os.path.getsize(model_path) / (1024*1024)
            
            # Heur√≠stica: modelos mais recentes (AnderV1) tendem a ser maiores
            if file_size_mb > 50:  # > 50MB provavelmente AnderV1
                config = {
                    'type': 'ANDER',
                    'initial_balance': 1000,
                    'base_lot_size': 0.2,
                    'max_lot_size': 0.3,
                    'description': 'Modelo detectado como AnderV1 (arquivo grande)'
                }
            else:
                config = {
                    'type': 'DIFF',
                    'initial_balance': 500,
                    'base_lot_size': 0.02,
                    'max_lot_size': 0.03,
                    'description': 'Modelo detectado como TreinoDiff (arquivo menor)'
                }
        except:
            # üî• FALLBACK FINAL: Usar configura√ß√£o DIFF como padr√£o
            config = {
                'type': 'DIFF',
                'initial_balance': 500,
                'base_lot_size': 0.02,
                'max_lot_size': 0.03,
                'description': 'Modelo padr√£o (TreinoDiff)'
            }
    
    print(f"üîç [DETEC√á√ÉO] {config['description']}")
    print(f"    üí∞ Portfolio inicial: ${config['initial_balance']}")
    print(f"    üìä Lot sizes: {config['base_lot_size']} - {config['max_lot_size']}")
    
    return config

def detect_policy_type(model_path: str) -> dict:
    """
    üî• DETEC√á√ÉO AUTOM√ÅTICA DE POL√çTICA: Identifica se modelo usa TwoHeadV3HybridEnhanced, TwoHeadV4Intelligent48h ou TwoHeadV5Intelligent48h
    
    Returns:
        dict: Configura√ß√µes da pol√≠tica detectada
    """
    try:
        # üî• TENTAR CARREGAR O MODELO PARA DETECTAR A POL√çTICA
        print_realtime(f"üîç [POL√çTICA] Detectando pol√≠tica do modelo: {os.path.basename(model_path)}")
        
        # üî• TENTAR COM TwoHeadV7Intuition PRIMEIRO (para modelos do daytrader)
        if TwoHeadV7Intuition:
            try:
                # Configurar policy_kwargs espec√≠ficos para V7 Intuition (sem policy_class)
                policy_kwargs = get_v7_intuition_kwargs()
                
                custom_objects = {
                    'TwoHeadV7Intuition': TwoHeadV7Intuition,
                    'TradingTransformerFeatureExtractor': TradingTransformerFeatureExtractor
                }
                model = RecurrentPPO.load(model_path, custom_objects=custom_objects, policy_kwargs=policy_kwargs)
                policy_name = model.policy.__class__.__name__
                
                if 'TwoHeadV7Intuition' in policy_name:
                    config = {
                        'policy_class': TwoHeadV7Intuition,
                        'policy_name': 'TwoHeadV7Intuition',
                        'description': 'üß† TwoHeadV7Intuition - Unified Backbone + Gradient Mixing + Neural Breathing',
                        'model_config': {
                            'type': 'V7_INTUITION',
                            'initial_balance': 500,
                            'base_lot_size': 0.02,
                            'max_lot_size': 0.03,
                            'description': 'V7 Intuition - Portfolio $500, Lots 0.02-0.03'
                        },
                        'features': [
                            'Unified Backbone (512 dim)',
                            'Shared LSTM (256 hidden)',
                            'Gradient Mixing Cross-Pollination',
                            'Interference Monitoring',
                            'Neural Breathing Pattern',
                            'Adaptive Sharing System',
                            'Enhanced Memory Bank',
                            'Temporal-Spatial Processing',
                            'Multi-Regime Detection',
                            'Dynamic Feature Extraction',
                            'Hierarchical Information Sharing',
                            'Advanced Risk Management'
                        ]
                    }
                    print_realtime(f"‚úÖ [POL√çTICA] Detectada: TwoHeadV7Intuition")
                    print_realtime(f"    üß† Unified Backbone + Gradient Mixing")
                    print_realtime(f"    üîÑ Neural Breathing + Memory Bank")
                    print_realtime(f"    üìä Compat√≠vel com observation space 1480D (74√ó20)")
                    return config
            except Exception as e:
                print_realtime(f"‚ö†Ô∏è [POL√çTICA] TwoHeadV7Intuition n√£o detectada: {str(e)[:100]}...")
        
        # üî• TENTAR COM TwoHeadV5Intelligent48h
        if TwoHeadV5Intelligent48h:
            try:
                custom_objects = {
                    'TwoHeadV5Intelligent48h': TwoHeadV5Intelligent48h,
                    'TradingTransformerFeatureExtractor': TradingTransformerFeatureExtractor
                }
                model = RecurrentPPO.load(model_path, custom_objects=custom_objects)
                policy_name = model.policy.__class__.__name__
                
                if 'TwoHeadV5Intelligent48h' in policy_name:
                    # Verificar se modelo tem Strategic Fusion Layer
                    has_strategic_fusion = hasattr(model.policy, 'strategic_fusion') and getattr(model.policy, 'strategic_fusion_enabled', False)
                    fusion_status = "COM Strategic Fusion Layer" if has_strategic_fusion else "SEM Strategic Fusion Layer"
                    
                    config = {
                        'policy_class': TwoHeadV5Intelligent48h,
                        'policy_name': 'TwoHeadV5Intelligent48h',
                        'description': f'üéØ TwoHeadV5Intelligent48h - Entry Head Ultra-Especializada ({fusion_status})',
                        'features': [
                            '2 LSTM Layers (128 hidden)',
                            '1 GRU Stabilizer',
                            '8 Attention Heads',
                            'Entry Head Ultra-Especializada',
                            '6 Specialized Entry Gates',
                            '10 Quality Scores',
                            f'Strategic Fusion: {"ATIVA" if has_strategic_fusion else "INATIVA"}',
                            'Market Fatigue Detection',
                            'Zero Cooldown Between Orders',
                            'Adaptive Quality Filters',
                            'Dynamic Entry Thresholds',
                            'Ultra-Intelligent Entry Decisions'
                        ]
                    }
                    print(f"‚úÖ [POL√çTICA] Detectada: TwoHeadV5Intelligent48h")
                    print(f"    üéØ Entry Head Ultra-Especializada")
                    print(f"    üß† Strategic Fusion: {fusion_status}")
                    print(f"    üß† 2-LSTM + 1-GRU + 8-Head + 6-Gates + 10-Scores")
                    return config
            except Exception as e:
                print(f"‚ö†Ô∏è [POL√çTICA] TwoHeadV5Intelligent48h n√£o detectada: {str(e)[:100]}...")
        
        
        # üî• TENTAR COM TwoHeadV7Simple PRIMEIRO (mais recente)
        if TwoHeadV7Simple:
            try:
                custom_objects = {
                    'TwoHeadV7Simple': TwoHeadV7Simple,
                    'TradingTransformerFeatureExtractor': TradingTransformerFeatureExtractor
                }
                model = RecurrentPPO.load(model_path, custom_objects=custom_objects)
                policy_name = model.policy.__class__.__name__
                
                if 'TwoHeadV7Simple' in policy_name:
                    config = {
                        'policy_class': TwoHeadV7Simple,
                        'policy_name': 'TwoHeadV7Simple',
                        'description': 'üöÄ TwoHeadV7Simple - Arquitetura Simplificada com Gates Especializados',
                        'features': [
                            '1 LSTM Shared (256 hidden)',
                            'Entry Head com 6 Gates Especializados',
                            'Management Head Simplificado',
                            'Trade Memory Bank',
                            'Temporal Gate (timing)',
                            'Validation Gate (MTF + patterns)',
                            'Risk Gate (risk + regime)',
                            'Market Gate (lookahead + fatigue)',
                            'Quality Gate (4 filtros t√©cnicos)',
                            'Confidence Gate (confian√ßa geral)',
                            'Critic MLP + Memory Buffer',
                            'Arquitetura Simplificada e Eficiente'
                        ]
                    }
                    print(f"‚úÖ [POL√çTICA] Detectada: TwoHeadV7Simple")
                    print(f"    üöÄ Arquitetura Simplificada com Gates Especializados")
                    print(f"    üß† 1-LSTM + 6-Gates + Memory Buffer")
                    return config
            except Exception as e:
                print(f"‚ö†Ô∏è [POL√çTICA] TwoHeadV7Simple n√£o detectada: {str(e)[:100]}...")
        
        # üî• TENTAR COM TwoHeadV6Intelligent48h
        if TwoHeadV6Intelligent48h:
            try:
                custom_objects = {
                    'TwoHeadV6Intelligent48h': TwoHeadV6Intelligent48h,
                    'TradingTransformerFeatureExtractor': TradingTransformerFeatureExtractor
                }
                model = RecurrentPPO.load(model_path, custom_objects=custom_objects)
                policy_name = model.policy.__class__.__name__
                
                if 'TwoHeadV6Intelligent48h' in policy_name:
                    # Verificar se modelo tem Strategic Fusion Layer
                    has_strategic_fusion = hasattr(model.policy, 'strategic_fusion') and getattr(model.policy, 'strategic_fusion_enabled', False)
                    fusion_status = "COM Strategic Fusion Layer" if has_strategic_fusion else "SEM Strategic Fusion Layer"
                    
                    config = {
                        'policy_class': TwoHeadV6Intelligent48h,
                        'policy_name': 'TwoHeadV6Intelligent48h',
                        'description': f'üéØ TwoHeadV6Intelligent48h - Entry Head Simples ({fusion_status})',
                        'features': [
                            '2 LSTM Layers (128 hidden)',
                            '1 GRU Stabilizer',
                            '8 Attention Heads',
                            'Entry Head Simples',
                            '6 Specialized Entry Gates',
                            '10 Quality Scores',
                            f'Strategic Fusion: {"ATIVA" if has_strategic_fusion else "INATIVA"}',
                            'Market Fatigue Detection',
                            'Zero Cooldown Between Orders',
                            'Adaptive Quality Filters',
                            'Dynamic Entry Thresholds',
                            'Intelligent Entry Decisions'
                        ]
                    }
                    print(f"‚úÖ [POL√çTICA] Detectada: TwoHeadV6Intelligent48h")
                    print(f"    üéØ Entry Head Simples")
                    print(f"    üß† Strategic Fusion: {fusion_status}")
                    print(f"    üß† 2-LSTM + 1-GRU + 8-Head + 6-Gates + 10-Scores")
                    return config
            except Exception as e:
                print(f"‚ö†Ô∏è [POL√çTICA] TwoHeadV6Intelligent48h n√£o detectada: {str(e)[:100]}...")
        
        # üî• TENTAR COM TwoHeadV4Intelligent48h
        if TwoHeadV4Intelligent48h:
            try:
                custom_objects = {
                    'TwoHeadV4Intelligent48h': TwoHeadV4Intelligent48h,
                    'TradingTransformerFeatureExtractor': TradingTransformerFeatureExtractor
                }
                model = RecurrentPPO.load(model_path, custom_objects=custom_objects)
                policy_name = model.policy.__class__.__name__
                
                if 'TwoHeadV4Intelligent48h' in policy_name:
                    config = {
                        'policy_class': TwoHeadV4Intelligent48h,
                        'policy_name': 'TwoHeadV4Intelligent48h',
                        'description': 'üöÄ TwoHeadV4Intelligent48h - Policy especializada para trades de at√© 48h',
                        'features': [
                            '2 LSTM Layers (128 hidden)',
                            '1 GRU Stabilizer',
                            '8 Attention Heads',
                            'Temporal Horizon Awareness',
                            'Multi-Timeframe Fusion',
                            'Advanced Pattern Memory',
                            'Dynamic Risk Adaptation',
                            'Market Regime Intelligence',
                            'Predictive Lookahead'
                        ]
                    }
                    print(f"‚úÖ [POL√çTICA] Detectada: TwoHeadV4Intelligent48h")
                    print(f"    üöÄ Policy avan√ßada para trades de 48h")
                    print(f"    üß† 2-LSTM + 1-GRU + 8-Head Attention")
                    return config
            except Exception as e:
                print(f"‚ö†Ô∏è [POL√çTICA] TwoHeadV4Intelligent48h n√£o detectada: {str(e)[:100]}...")
        
        # üî• TENTAR COM TwoHeadV3HybridEnhanced
        if TwoHeadV3HybridEnhanced:
            try:
                custom_objects = {
                    'TwoHeadV3HybridEnhanced': TwoHeadV3HybridEnhanced,
                    'TradingTransformerFeatureExtractor': TradingTransformerFeatureExtractor
                }
                model = RecurrentPPO.load(model_path, custom_objects=custom_objects)
                policy_name = model.policy.__class__.__name__
                
                if 'TwoHeadV3HybridEnhanced' in policy_name:
                    config = {
                        'policy_class': TwoHeadV3HybridEnhanced,
                        'policy_name': 'TwoHeadV3HybridEnhanced',
                        'description': 'üî• TwoHeadV3HybridEnhanced - Policy h√≠brida otimizada',
                        'features': [
                            '2 LSTM Layers (64 hidden)',
                            '1 GRU Stabilizer',
                            '8 Attention Heads',
                            'Pattern Recognition',
                            'Adaptive Learning Rate',
                            'Gradient Clipping',
                            'Feature Weighting',
                            'Dynamic Attention'
                        ]
                    }
                    print(f"‚úÖ [POL√çTICA] Detectada: TwoHeadV3HybridEnhanced")
                    print(f"    üî• Policy h√≠brida otimizada")
                    print(f"    üß† 2-LSTM + 1-GRU + 8-Head Attention")
                    return config
            except Exception as e:
                print(f"‚ö†Ô∏è [POL√çTICA] TwoHeadV3HybridEnhanced n√£o detectada: {str(e)[:100]}...")
        
        # üî• FALLBACK: Tentar carregar sem custom_objects para detectar pol√≠tica padr√£o
        try:
            model = RecurrentPPO.load(model_path)
            policy_name = model.policy.__class__.__name__
            
            config = {
                'policy_class': None,
                'policy_name': policy_name,
                'description': f'üìã Policy padr√£o: {policy_name}',
                'features': ['Policy padr√£o do Stable-Baselines3']
            }
            print(f"‚ö†Ô∏è [POL√çTICA] Policy padr√£o detectada: {policy_name}")
            return config
            
        except Exception as e:
            print(f"‚ùå [POL√çTICA] Erro ao detectar pol√≠tica: {str(e)[:100]}...")
            
            # üî• FALLBACK FINAL: Priorizar TwoHeadV5Intelligent48h (compat√≠vel com PPOv1)
            if TwoHeadV5Intelligent48h:
                config = {
                    'policy_class': TwoHeadV5Intelligent48h,
                    'policy_name': 'TwoHeadV5Intelligent48h',
                    'description': 'üéØ TwoHeadV5Intelligent48h - Policy padr√£o PPOv1 (fallback)',
                    'features': [
                        '2 LSTM Layers (128 hidden)',
                        '1 GRU Stabilizer',
                        '8 Attention Heads',
                        '6 Specialized Entry Gates',
                        '10 Quality Scores',
                        'Market Fatigue Detection',
                        'Zero Cooldown Between Orders',
                        'Adaptive Quality Filters',
                        'Dynamic Entry Thresholds',
                        'Ultra-Intelligent Entry Decisions'
                    ]
                }
                print(f"‚úÖ [POL√çTICA] Usando TwoHeadV5Intelligent48h como padr√£o (compat√≠vel com PPOv1)")
                return config
            elif TwoHeadV4Intelligent48h:
                config = {
                    'policy_class': TwoHeadV4Intelligent48h,
                    'policy_name': 'TwoHeadV4Intelligent48h (fallback)',
                    'description': 'üöÄ TwoHeadV4Intelligent48h - Policy fallback',
                    'features': ['Policy V4 fallback por compatibilidade']
                }
                print(f"‚ö†Ô∏è [POL√çTICA] Usando TwoHeadV4Intelligent48h como fallback")
                return config
            else:
                config = {
                    'policy_class': TwoHeadV3HybridEnhanced,
                    'policy_name': 'TwoHeadV3HybridEnhanced (fallback)',
                    'description': 'üî• TwoHeadV3HybridEnhanced - Policy fallback',
                    'features': ['Policy V3 fallback por compatibilidade']
                }
                print(f"‚ö†Ô∏è [POL√çTICA] Usando TwoHeadV3HybridEnhanced como fallback")
                return config
            
    except Exception as e:
        print(f"‚ùå [POL√çTICA] Erro cr√≠tico na detec√ß√£o: {e}")
        
        # üî• FALLBACK FINAL: Tentar V5, V4, V3 em ordem
        if TwoHeadV5Intelligent48h:
            config = {
                'policy_class': TwoHeadV5Intelligent48h,
                'policy_name': 'TwoHeadV5Intelligent48h (fallback)',
                'description': 'üéØ TwoHeadV5Intelligent48h - Policy de fallback',
                'features': ['Policy V5 de fallback por compatibilidade']
            }
            return config
        elif TwoHeadV4Intelligent48h:
            config = {
                'policy_class': TwoHeadV4Intelligent48h,
                'policy_name': 'TwoHeadV4Intelligent48h (fallback)',
                'description': 'üöÄ TwoHeadV4Intelligent48h - Policy de fallback',
                'features': ['Policy V4 de fallback por compatibilidade']
            }
            return config
        else:
            config = {
                'policy_class': TwoHeadV3HybridEnhanced,
                'policy_name': 'TwoHeadV3HybridEnhanced (fallback)',
                'description': 'üî• TwoHeadV3HybridEnhanced - Policy de fallback',
                'features': ['Policy V3 de fallback por compatibilidade']
            }
            return config

# üî• IMPLEMENTA√á√ÉO STANDALONE COM SUPORTE AO RANGE NOVO
class TradingEnvEvaluator(gym.Env):
    """Ambiente de trading para avalia√ß√£o - 100% COMPAT√çVEL COM TREINODIFERENCIADOPPO.PY"""
    
    MAX_STEPS = 1500  # üî• COMPATIBILIDADE 100%: Mesmo MAX_STEPS do treinodiferenciadoPPO.py (1500)
    
    def __init__(self, df, window_size=20, is_training=False, model_config=None, trading_params=None):
        super(TradingEnvEvaluator, self).__init__()
        
        # üî• USAR APENAS 10-20% DO DATASET PARA AVALIA√á√ÉO R√ÅPIDA
        if len(df) > 50000:  # Se dataset for muito grande
            # Usar √∫ltimos 15% do dataset (dados mais recentes)
            dataset_size = int(len(df) * 0.15)
            self.df = df.iloc[-dataset_size:].copy()
            print(f"üî• DATASET REDUZIDO: {len(self.df):,} barras (15% dos dados mais recentes)")
        else:
            self.df = df.copy()
            print(f"üî• DATASET COMPLETO: {len(self.df):,} barras")
        
        self.window_size = window_size
        self.current_step = window_size
        self.is_training = is_training
        
        # üî• CONFIGURA√á√ÉO DO MODELO DETECTADO (compat√≠vel com PPOv1)
        if model_config is None:
            model_config = {'type': 'PPOv1', 'initial_balance': 500, 'base_lot_size': 0.02, 'max_lot_size': 0.03}
        
        self.initial_balance = model_config.get('initial_balance', 500)
        self.base_lot_size = model_config.get('base_lot_size', 0.02)
        self.max_lot_size = model_config.get('max_lot_size', 0.03)
        self.lot_size = self.base_lot_size  # üî• CORRIGIDO: Definir lot_size inicial
        
        # üî• VARI√ÅVEIS DE ESTADO
        self.portfolio_value = self.initial_balance
        self.realized_balance = self.initial_balance
        self.peak_portfolio = self.initial_balance
        self.positions = []
        self.trades = []
        self.current_drawdown = 0.0
        self.peak_drawdown = 0.0
        self.max_positions = 3
        self.episode_steps = 0
        
        # üî• ACTION SPACE: 11 dimens√µes compat√≠vel com PPOv1.py (TwoHeadV5Intelligent48h)
        # Estrutura: [entry_decision, entry_confidence, temporal_signal, risk_appetite, market_regime_bias, sl1, sl2, sl3, tp1, tp2, tp3]
        self.action_space = spaces.Box(
            low=np.array([0, 0, -1, 0, -1, -3, -3, -3, -3, -3, -3]),
            high=np.array([2, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3]),
            dtype=np.float32
        )
        
        # üî• PAR√ÇMETROS DE TRADING OTIMIZADOS - ALINHADOS COM DAYTRADER.PY
        self.trading_params = trading_params or {}
        self.sl_range_min = self.trading_params.get('sl_range_min', 2.0)   # üöÄ DAYTRADER: 2 pontos (daytrade)
        self.sl_range_max = self.trading_params.get('sl_range_max', 8.0)   # üöÄ DAYTRADER: 8 pontos (daytrade)
        self.tp_range_min = self.trading_params.get('tp_range_min', 3.0)   # üöÄ DAYTRADER: 3 pontos (daytrade)
        self.tp_range_max = self.trading_params.get('tp_range_max', 15.0)  # üöÄ DAYTRADER: 15 pontos (daytrade)
        self.target_trades_per_day = self.trading_params.get('target_trades_per_day', 18)  # üî• ALINHADO PPOv1: 18 trades/dia
        
        # üî• CUSTOS DE TRADING M√çNIMOS PARA TESTE
        self.spread_points = 0.1  # Spread m√≠nimo: 0.1 pontos
        self.commission_per_lot = 0.0  # Sem comiss√£o para teste
        
        # üî• CONFIGURA√á√ÉO DE LOGGING: SEM SPAM
        self.verbose_trading = False  # üî• DESABILITAR SPAM DE TRADING
        self.log_frequency = 500  # Log apenas a cada 500 steps
        
        self.imputer = KNNImputer(n_neighbors=5)
        
        # üî• FEATURES OTIMIZADAS: ALINHADAS COM DAYTRADER.PY (19 base features)
        base_features_5m_15m = [
            'returns', 'volatility_20', 'sma_20', 'sma_50', 'rsi_14', 
            'stoch_k', 'bb_position', 'trend_strength', 'atr_14',
            'ema_12', 'ema_26', 'macd', 'macd_signal', 'macd_hist',
            'bollinger_upper', 'bollinger_lower', 'williams_r', 'cci', 'momentum'
        ]
        
        # üéØ FEATURES DE ALTA QUALIDADE para substituir 4h zeradas
        high_quality_features = [
            'volume_momentum', 'price_position', 'volatility_ratio', 
            'intraday_range', 'market_regime', 'spread_pressure',
            'session_momentum', 'time_of_day', 'tick_momentum'
        ]
        
        self.feature_columns = []
        # Adicionar 5m e 15m (funcionam perfeitamente) - IGUAL AO TREINODIFF
        for tf in ['5m', '15m']:
            self.feature_columns.extend([f"{f}_{tf}" for f in base_features_5m_15m])
        
        # Substituir 4h in√∫teis por features de alta qualidade - IGUAL AO TREINODIFF
        self.feature_columns.extend(high_quality_features)
        
        self._prepare_data()
        n_features = len(self.feature_columns) + self.max_positions * 9  # üî• CORRIGIDO: 9 features por posi√ß√£o (compatibilidade com PPOv1)
        
        # üß† OBSERVATION SPACE: 74 features √ó 20 window = 1480 dimens√µes (compat√≠vel com V7 Intuition)
        # Features: 38 (base 5m+15m) + 9 (high quality) + 27 (positions) = 74 features
        print_realtime(f"[EVAL] üß† Observation Space: {n_features} features √ó {window_size} window = {window_size * n_features} dimens√µes")
        
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(window_size * n_features,), dtype=np.float32
        )
        
        # Estado inicial
        self.realized_balance = self.initial_balance
        self.peak_portfolio_value = self.initial_balance
        self.base_tf = '5m'
        
        # üî• PAR√ÇMETROS DE TRADING OTIMIZADOS - ALINHADOS COM DIFF
        self.trading_params = trading_params or {}
        self.sl_range_min = self.trading_params.get('sl_range_min', 8)   # üî• ALINHADO HEADV6: 8 pontos
        self.sl_range_max = self.trading_params.get('sl_range_max', 25)  # üî• ALINHADO HEADV6: 25 pontos
        self.tp_range_min = self.trading_params.get('tp_range_min', 12)  # üî• ALINHADO HEADV6: 12 pontos
        self.tp_range_max = self.trading_params.get('tp_range_max', 40)  # üî• ALINHADO HEADV6: 40 pontos
        self.target_trades_per_day = self.trading_params.get('target_trades_per_day', 18)  # üî• ALINHADO: 18 trades/dia
        
        # üî• CUSTOS DE TRADING M√çNIMOS PARA TESTE
        self.spread_points = 0.1  # Spread m√≠nimo: 0.1 pontos
        self.commission_per_lot = 0.5  # Comiss√£o m√≠nima por lote
        self.slippage_points = 0.05  # Slippage m√≠nimo
        
        # üî• VARI√ÅVEIS PARA COMPATIBILIDADE - SEM REWARD SYSTEM
        self.steps_since_last_trade = 0
        self.last_action = None
        self.hold_count = 0
        self.episode_steps = 0
        self.win_streak = 0
        self.last_trade_pnl = 0.0
        self.episode_start_time = None
        
        # üî• AMBIENTE LIVRE: 3 POSI√á√ïES SIMULT√ÇNEAS SEM RESTRI√á√ïES
        self.last_trade_step = -10
        
    def _prepare_data(self):
        """Preparar features t√©cnicas com m√∫ltiplos timeframes - IGUAL AO TREINODIFERENCIADOPPO.PY"""
        # Calcular features para 5m e 15m apenas (igual ao treinodiferenciadoPPO.py)
        for tf in ['5m', '15m']:
            close_col = f'close_{tf}' if f'close_{tf}' in self.df.columns else 'close'
            
            if close_col in self.df.columns:
                # Returns
                self.df[f'returns_{tf}'] = self.df[close_col].pct_change().fillna(0)
                
                # Volatilidade
                self.df[f'volatility_20_{tf}'] = self.df[close_col].rolling(window=20).std().fillna(0)
                
                # SMAs
                self.df[f'sma_20_{tf}'] = self.df[close_col].rolling(window=20).mean().fillna(self.df[close_col])
                self.df[f'sma_50_{tf}'] = self.df[close_col].rolling(window=50).mean().fillna(self.df[close_col])
                
                # RSI
                try:
                    import ta
                    self.df[f'rsi_14_{tf}'] = ta.momentum.RSIIndicator(self.df[close_col], window=14).rsi().fillna(50)
                except:
                    # RSI manual
                    delta = self.df[close_col].diff()
                    gain = delta.where(delta > 0, 0).rolling(window=14).mean()
                    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
                    rs = gain / loss
                    self.df[f'rsi_14_{tf}'] = (100 - (100 / (1 + rs))).fillna(50)
                
                # Outros indicadores b√°sicos
                self.df[f'stoch_k_{tf}'] = 50.0  # Simplificado
                self.df[f'atr_14_{tf}'] = self.df[close_col].rolling(window=14).std().fillna(0.01)
                
                # üî• FEATURES ESPEC√çFICAS DO TREINODIFERENCIADOPPO.PY
                self.df[f'bb_position_{tf}'] = 0.5  # Bollinger Band Position (0-1)
                self.df[f'trend_strength_{tf}'] = self.df[close_col].pct_change(periods=5).fillna(0)  # For√ßa de tend√™ncia rolling
            else:
                # Criar colunas com valores padr√£o se n√£o existir close
                base_features_5m_15m = ['returns', 'volatility_20', 'sma_20', 'sma_50', 'rsi_14', 'stoch_k', 'bb_position', 'trend_strength', 'atr_14']
                for feature in base_features_5m_15m:
                    self.df[f'{feature}_{tf}'] = 0.0
        
        # üî• CRIAR FEATURES DE ALTA QUALIDADE (substituem 4h) - IGUAL AO TREINODIFERENCIADOPPO.PY
        close_5m = self.df.get('close_5m', self.df.get('close', pd.Series([2000.0] * len(self.df))))
        
        # Volume momentum (simulado se n√£o tiver volume)
        volume_5m = self.df.get('volume_5m', pd.Series([1000.0] * len(self.df)))
        volume_sma_20 = volume_5m.rolling(window=20).mean()
        self.df['volume_momentum'] = np.where(volume_sma_20 > 0, (volume_5m - volume_sma_20) / volume_sma_20, 0)
        
        # Price position (posi√ß√£o do pre√ßo no range)
        high_20 = close_5m.rolling(window=20).max()
        low_20 = close_5m.rolling(window=20).min()
        self.df['price_position'] = np.where((high_20 - low_20) > 0, (close_5m - low_20) / (high_20 - low_20), 0.5)
        
        # Volatility ratio
        vol_5 = close_5m.rolling(window=5).std()
        vol_20 = close_5m.rolling(window=20).std()
        self.df['volatility_ratio'] = np.where(vol_20 > 0, vol_5 / vol_20, 1.0)
        
        # Intraday range
        self.df['intraday_range'] = close_5m.rolling(window=288).max() - close_5m.rolling(window=288).min()  # 24h range
        
        # Market regime (tend√™ncia vs range)
        sma_short = close_5m.rolling(window=10).mean()
        sma_long = close_5m.rolling(window=50).mean()
        self.df['market_regime'] = np.where(sma_long > 0, (sma_short - sma_long) / sma_long, 0)
        
        # Spread pressure (simulado)
        self.df['spread_pressure'] = close_5m.pct_change().rolling(window=10).std().fillna(0.001)
        
        # Session momentum (momentum da sess√£o)
        self.df['session_momentum'] = close_5m.pct_change(periods=60).fillna(0)  # 5h momentum
        
        # Time of day (hora do dia normalizada 0-1)
        if hasattr(self.df.index, 'hour'):
            self.df['time_of_day'] = self.df.index.hour / 24.0
        else:
            self.df['time_of_day'] = 0.5  # Meio-dia como padr√£o
        
        # Tick momentum (momentum de curto prazo)
        self.df['tick_momentum'] = close_5m.pct_change(periods=3).fillna(0)  # 15min momentum
        
        # Garantir que todas as features existem
        for col in self.feature_columns:
            if col not in self.df.columns:
                self.df[col] = 0.0
        
        # üî• COMPATIBILIDADE 100%: Processamento de dados igual ao treinodiferenciadoPPO.py
        for col in self.feature_columns:
            self.df[col] = self.df[col].replace([np.inf, -np.inf], np.nan)
            Q1 = self.df[col].quantile(0.25)
            Q3 = self.df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 3 * IQR
            upper_bound = Q3 + 3 * IQR
            self.df.loc[:, col] = self.df[col].clip(lower=lower_bound, upper=upper_bound)
        
        # KNNImputer igual ao treinodiferenciadoPPO.py
        base_imputer = KNNImputer(n_neighbors=5)
        base_imputed = base_imputer.fit_transform(self.df[self.feature_columns])
        self.df.loc[:, self.feature_columns] = pd.DataFrame(base_imputed, index=self.df.index, columns=self.feature_columns)
        
        self.processed_data = self.df[self.feature_columns].values
        self.processed_data = np.nan_to_num(self.processed_data, nan=0.0, posinf=1e6, neginf=-1e6)
        
        # Garantir que temos coluna de pre√ßos
        if 'close_5m' not in self.df.columns:
            if 'close' in self.df.columns:
                self.df['close_5m'] = self.df['close']
            else:
                # Criar dados sint√©ticos para teste
                self.df['close_5m'] = 2000.0 + np.cumsum(np.random.randn(len(self.df)) * 0.5)
    
    def reset(self):
        """Reset para avalia√ß√£o"""
        self.current_step = self.window_size
        self.portfolio_value = self.initial_balance
        self.peak_portfolio = self.initial_balance
        self.peak_portfolio_value = self.initial_balance
        self.realized_balance = self.initial_balance
        self.positions = []
        self.returns = []
        self.trades = []
        self.current_drawdown = 0.0
        self.peak_drawdown = 0.0
        self.current_positions = 0
        
        # üî• RESETAR VARI√ÅVEIS DE COMPATIBILIDADE
        self.steps_since_last_trade = 0
        self.last_action = None
        self.hold_count = 0
        self.episode_steps = 0
        self.win_streak = 0
        self.last_trade_pnl = 0.0
        self.episode_start_time = time.time()
        
        # üî• RESETAR VARI√ÅVEIS
        self.last_trade_step = -10
        
        return self._get_observation()
    
    def step(self, action):
        """Executa step com compatibilidade 100% com treinodiferenciadoPPO.py"""
        done = False
        
        # üî• COMPATIBILIDADE 100%: Mesmas condi√ß√µes de t√©rmino do treinodiferenciadoPPO.py
        if self.current_step >= len(self.df) - 1:
            done = True
        if self.episode_steps >= self.MAX_STEPS:
            done = True
        
        # üî• SALVAR ESTADO ANTERIOR PARA M√âTRICAS
        old_state = {
            "portfolio_total_value": self.realized_balance + sum(self._get_position_pnl(pos, self.df[f'close_{self.base_tf}'].iloc[self.current_step]) for pos in self.positions),
            "current_drawdown": self.current_drawdown
        }
        
        # üî• PROCESSAMENTO DE A√á√ïES: 11 DIMENS√ïES COMPAT√çVEL COM PPOV1.PY (TwoHeadV5Intelligent48h)
        # Garantir que action tem 11 dimens√µes
        if not isinstance(action, (list, tuple, np.ndarray)):
            action = np.array([action])
        
        # Pad se necess√°rio para 11 dimens√µes
        if len(action) < 11:
            action = np.pad(action, (0, 11 - len(action)), mode='constant', constant_values=0)
        
        # üî• ESTRUTURA DE A√á√ÉO PPOV1.PY: [entry_decision, entry_confidence, temporal_signal, risk_appetite, market_regime_bias, sl1, sl2, sl3, tp1, tp2, tp3]
        entry_decision = int(action[0])  # 0=hold, 1=long, 2=short
        entry_confidence = float(action[1])  # [0,1] Confian√ßa da entrada
        temporal_signal = float(action[2])  # [-1,1] Sinal temporal
        risk_appetite = float(action[3])  # [0,1] Apetite ao risco
        market_regime_bias = float(action[4])  # [-1,1] Vi√©s do regime de mercado
        sl_adjusts = [action[5], action[6], action[7]]  # SL para pos1, pos2, pos3
        tp_adjusts = [action[8], action[9], action[10]]  # TP para pos1, pos2, pos3
        
        # üî• PRE√áO ATUAL
        current_price = self.df[f'close_{self.base_tf}'].iloc[self.current_step]
        
        # üî• VERIFICAR SL/TP AUTOM√ÅTICO PRIMEIRO (IGUAL TREINODIFERENCIADOPPO.PY)
        for pos in self.positions[:]:  # Usar slice para evitar modifica√ß√£o durante itera√ß√£o
            should_close = False
            close_reason = ""
            
            if 'sl' in pos and pos['sl'] > 0:
                if pos['type'] == 'long' and current_price <= pos['sl']:
                    should_close = True
                    close_reason = "SL hit"
                elif pos['type'] == 'short' and current_price >= pos['sl']:
                    should_close = True
                    close_reason = "SL hit"
                    
            if 'tp' in pos and pos['tp'] > 0 and not should_close:
                if pos['type'] == 'long' and current_price >= pos['tp']:
                    should_close = True
                    close_reason = "TP hit"
                elif pos['type'] == 'short' and current_price <= pos['tp']:
                    should_close = True
                    close_reason = "TP hit"
            
            if should_close:
                self._close_position(pos, current_price, close_reason)
        
        # üî• PROCESSAR ENTRADA DE NOVA POSI√á√ÉO (COMPAT√çVEL COM PPOV1.PY)
        if entry_decision > 0 and len(self.positions) < self.max_positions:
            # Calcular tamanho da posi√ß√£o usando confidence do PPOv1
            lot_size = self._calculate_adaptive_position_size(entry_confidence)
            
            # Criar nova posi√ß√£o
            position = {
                'type': 'long' if entry_decision == 1 else 'short',
                'entry_price': current_price,
                'lot_size': lot_size,
                'entry_step': self.current_step,
                'position_id': len(self.positions)  # ID para rastreamento
            }
            
            # Definir SL/TP inicial para a nova posi√ß√£o
            # Usar o primeiro slot dispon√≠vel dos adjusts
            pos_index = len(self.positions)
            if pos_index < 3:  # Garantir que n√£o exceda max_positions
                sl_adjust = sl_adjusts[pos_index]
                tp_adjust = tp_adjusts[pos_index]
                
                # Converter ajustes [-3,3] para pontos de pre√ßo (IGUAL TREINODIFERENCIADOPPO.PY)
                sl_points = abs(sl_adjust) * 100  # [-3,3] ‚Üí [0,300] pontos
                tp_points = abs(tp_adjust) * 100  # [-3,3] ‚Üí [0,300] pontos
                
                # Converter pontos para diferen√ßa de pre√ßo (OURO: 1 ponto = $1.00)
                sl_price_diff = sl_points * 1.0
                tp_price_diff = tp_points * 1.0
                
                if position['type'] == 'long':
                    position['sl'] = current_price - sl_price_diff
                    position['tp'] = current_price + tp_price_diff
                else:
                    position['sl'] = current_price + sl_price_diff
                    position['tp'] = current_price - tp_price_diff
            else:
                # SL/TP padr√£o se exceder 3 posi√ß√µes
                if position['type'] == 'long':
                    position['sl'] = current_price * 0.98  # 2% SL padr√£o
                    position['tp'] = current_price * 1.04  # 4% TP padr√£o
                else:
                    position['sl'] = current_price * 1.02  # 2% SL padr√£o
                    position['tp'] = current_price * 0.96  # 4% TP padr√£o
            
            # Adicionar nova posi√ß√£o
            self.positions.append(position)
            self.current_positions = len(self.positions)
            
            # Registrar trade
            trade_info = {
                'type': position['type'],
                'entry_price': current_price,
                'lot_size': lot_size,
                'entry_step': self.current_step,
                'sl': position['sl'],
                'tp': position['tp']
            }
            self.trades.append(trade_info)
            
            # Log silencioso
            if self.verbose_trading:
                print(f"üìà {'LONG' if entry_decision == 1 else 'SHORT'} aberto: Pre√ßo={current_price:.5f} | SL={position['sl']:.5f} | TP={position['tp']:.5f} | Pos#{len(self.positions)}/3")
        
        # üî• PROCESSAR GEST√ÉO DE POSI√á√ïES EXISTENTES VIA MANAGER HEAD (IGUAL TREINODIFERENCIADOPPO.PY)
        # Atualizar SL/TP das posi√ß√µes existentes baseado nos adjusts
        for i, pos in enumerate(self.positions):
            if i < 3:  # M√°ximo 3 posi√ß√µes
                sl_adjust = sl_adjusts[i]
                tp_adjust = tp_adjusts[i]
                
                # Converter ajustes para pontos
                sl_points = abs(sl_adjust) * 100
                tp_points = abs(tp_adjust) * 100
                
                # Atualizar SL/TP da posi√ß√£o existente
                sl_price_diff = sl_points * 1.0
                tp_price_diff = tp_points * 1.0
                
                if pos['type'] == 'long':
                    pos['sl'] = pos['entry_price'] - sl_price_diff
                    pos['tp'] = pos['entry_price'] + tp_price_diff
                else:
                    pos['sl'] = pos['entry_price'] + sl_price_diff
                    pos['tp'] = pos['entry_price'] - tp_price_diff
        
        # üî• SISTEMA DE FECHAMENTO AUTOM√ÅTICO POR DURA√á√ÉO (IGUAL TREINODIFERENCIADOPPO.PY)
        for pos in self.positions[:]:
            duration = self.current_step - pos['entry_step']
            if duration > 48:  # 4h m√°ximo por posi√ß√£o
                self._close_position(pos, current_price, "MAX_DURATION")
        
        # üî• ATUALIZAR STEP E PORTFOLIO
        self.current_step += 1
        self.episode_steps += 1
        
        # üî• ATUALIZAR PORTFOLIO VALUE
        unrealized_pnl = self._get_unrealized_pnl()
        self.portfolio_value = self.realized_balance + unrealized_pnl
        
        # üî• ATUALIZAR DRAWDOWN COM LIMITE MATEM√ÅTICO
        if self.portfolio_value > self.peak_portfolio:
            self.peak_portfolio = self.portfolio_value
            self.current_drawdown = 0.0
        else:
            # üî• CORRIGIR MATEM√ÅTICA ABSURDA: Drawdown m√°ximo √© 100%
            if self.peak_portfolio > 0:
                self.current_drawdown = min((self.peak_portfolio - self.portfolio_value) / self.peak_portfolio, 1.0)
            else:
                self.current_drawdown = 0.0
            if self.current_drawdown > self.peak_drawdown:
                self.peak_drawdown = min(self.current_drawdown, 1.0)  # Nunca mais que 100%
        
        # üî• CALCULAR REWARD E INFO (SEMPRE 0 PARA AVALIA√á√ÉO)
        reward, info, _ = self._calculate_reward_and_info(action, old_state)
        
        # üî• OBTER OBSERVA√á√ÉO
        obs = self._get_observation()
        
        # üî• LOGS DE PROGRESSO
        if self.episode_steps % self.log_frequency == 0:
            self._print_progress_metrics()
        
        return obs, reward, done, info
    
    def _get_observation(self):
        """Retorna a observa√ß√£o atual do ambiente"""
        if self.current_step < self.window_size:
            return np.zeros(self.observation_space.shape, dtype=np.float32)
        
        if self.current_step >= len(self.df):
            return np.zeros(self.observation_space.shape, dtype=np.float32)
            
        # Preparar observa√ß√£o das posi√ß√µes (compat√≠vel com PPOv1)
        positions_obs = np.zeros((self.max_positions, 9))  # 9 features por posi√ß√£o para compatibilidade com PPOv1
        current_price = self.df[f'close_{self.base_tf}'].iloc[self.current_step]
        
        for i in range(self.max_positions):
            if i < len(self.positions):
                pos = self.positions[i]
                positions_obs[i, 0] = 1  # status aberta
                positions_obs[i, 1] = 0 if pos['type'] == 'long' else 1  # tipo
                positions_obs[i, 2] = (pos['entry_price'] - min(self.df[f'close_{self.base_tf}'])) / (max(self.df[f'close_{self.base_tf}']) - min(self.df[f'close_{self.base_tf}']))  # pre√ßo normalizado
                positions_obs[i, 3] = self._get_position_pnl(pos, current_price)  # PnL atual
                positions_obs[i, 4] = pos.get('sl', 0)  # SL
                positions_obs[i, 5] = pos.get('tp', 0)  # TP
                positions_obs[i, 6] = (self.current_step - pos['entry_step']) / len(self.df)  # dura√ß√£o normalizada
                positions_obs[i, 7] = pos.get('lot_size', 0.02)  # tamanho da posi√ß√£o
                positions_obs[i, 8] = pos.get('position_id', i)  # ID da posi√ß√£o
            else:
                positions_obs[i, :] = 0  # slot vazio
        
        # üî• COMPATIBILIDADE 100%: Observa√ß√£o igual ao mainppo1.py
        obs_market = self.processed_data[self.current_step - self.window_size:self.current_step]
        tile_positions = np.tile(positions_obs.flatten(), (self.window_size, 1))
        
        # Verifica√ß√µes de compatibilidade
        assert obs_market.shape[0] == tile_positions.shape[0], f"obs_market shape: {obs_market.shape}, tile_positions shape: {tile_positions.shape}"
        obs = np.concatenate([obs_market, tile_positions], axis=1)
        flat_obs = obs.flatten().astype(np.float32)
        
        # Verifica√ß√µes de seguran√ßa
        assert isinstance(flat_obs, np.ndarray), f"flat_obs n√£o √© np.ndarray: {type(flat_obs)}"
        assert flat_obs.ndim == 1, f"flat_obs n√£o √© 1D: shape={flat_obs.shape}"
        assert flat_obs.shape == self.observation_space.shape, f"flat_obs.shape {flat_obs.shape} != observation_space.shape {self.observation_space.shape}"
        assert flat_obs.dtype == np.float32, f"flat_obs.dtype {flat_obs.dtype} != np.float32"
        
        return flat_obs
    
    def _get_position_pnl(self, pos, current_price):
        """üî• COMPATIBILIDADE 100%: C√°lculo PnL igual ao mainppo1.py"""
        if pos['type'] == 'long':
            return (current_price - pos['entry_price']) * pos['lot_size'] * 100
        else:
            return (pos['entry_price'] - current_price) * pos['lot_size'] * 100
    
    def _get_unrealized_pnl(self):
        """
        Calcula o PnL n√£o realizado de todas as posi√ß√µes abertas.
        M√©todo necess√°rio para compatibilidade com reward_system.py
        """
        if not self.positions:
            return 0.0
        
        current_price = self.df[f'close_{self.base_tf}'].iloc[self.current_step]
        total_unrealized = 0.0
        
        for pos in self.positions:
            pnl = self._get_position_pnl(pos, current_price)
            total_unrealized += pnl
            
        return total_unrealized
    
    def _close_position(self, position, current_price, reason="MODEL_CLOSE"):
        """
        Fecha uma posi√ß√£o espec√≠fica.
        """
        try:
            # Calcular PnL
            pnl = self._get_position_pnl(position, current_price)
            self.realized_balance += pnl
            
            # Encontrar o trade correspondente a esta posi√ß√£o
            matching_trade = None
            for trade in reversed(self.trades):
                if (trade.get('entry_step') == position['entry_step'] and 
                    trade.get('type') == position['type'] and 
                    'exit_step' not in trade):
                    matching_trade = trade
                    break
            
            if matching_trade:
                matching_trade.update({
                    'exit_price': current_price,
                    'exit_step': self.current_step,
                    'pnl_usd': pnl,
                    'duration': self.current_step - position['entry_step'],
                    'exit_reason': reason
                })
                
            # Log silencioso
            if self.verbose_trading:
                print(f"üéØ {reason}: {position['type'].upper()} | PnL: ${pnl:+.2f} | Pre√ßo: {current_price:.5f}")
            
            # Remover posi√ß√£o
            if position in self.positions:
                self.positions.remove(position)
                self.current_positions = len(self.positions)
                
        except Exception as e:
            print(f"‚ùå Erro ao fechar posi√ß√£o: {e}")
    
    def _calculate_adaptive_position_size(self, action_confidence=1.0):
        """
        üöÄ MELHORIA #8: Position sizing adaptativo baseado em confian√ßa e volatilidade
        """
        try:
            # Obter volatilidade atual (ATR normalizado)
            current_step = min(self.current_step, len(self.df) - 1)
            atr_5m = self.df['atr_14_5m'].iloc[current_step] if 'atr_14_5m' in self.df.columns else 0.001
            volatility = atr_5m / self.df['close_5m'].iloc[current_step] if self.df['close_5m'].iloc[current_step] > 0 else 0.001
            
            # Normalizar volatilidade (0.001 = baixa, 0.01 = alta)
            volatility = max(min(volatility, 0.02), 0.0005)  # Limitar entre 0.05% e 2%
            
            # Calcular confian√ßa baseada na for√ßa do sinal
            # action_confidence vem da for√ßa da a√ß√£o do modelo (0-1)
            confidence_multiplier = min(action_confidence * 1.5, 1.5)  # Max 1.5x
            
            # Calcular divisor de volatilidade (maior volatilidade = menor posi√ß√£o)
            volatility_divisor = max(volatility * 100, 0.5)  # Min 0.5x
            
            # Tamanho final
            size = self.base_lot_size * confidence_multiplier / volatility_divisor
            
            # Aplicar limites
            final_size = max(min(size, self.max_lot_size), 0.01)  # Entre 0.01 e 0.08
            
            return final_size
            
        except Exception as e:
            # Fallback para tamanho base em caso de erro
            return self.base_lot_size
    
    def _check_entry_filters(self, action_type):
        """
        üöÄ MELHORIA #2: Filtros de entrada balanceados (n√£o muito restritivos)
        """
        try:
            current_step = min(self.current_step, len(self.df) - 1)
            
            # Filtro 1: Momentum b√°sico (usando features existentes)
            momentum_5m = self.df.get('momentum_5_5m', pd.Series([0])).iloc[current_step]
            momentum_15m = self.df.get('momentum_5_15m', pd.Series([0])).iloc[current_step]
            
            if action_type == 1:  # Long
                momentum_signals = [momentum_5m > 0.0005, momentum_15m > 0.0002]  # üî• AFROUXADO: Era 0.001 e 0.0005
            else:  # Short
                momentum_signals = [momentum_5m < -0.0005, momentum_15m < -0.0002]  # üî• AFROUXADO: Era -0.001 e -0.0005
            
            momentum_confirmations = sum(momentum_signals)
            
            # Filtro 2: Volatilidade n√£o extrema
            volatility_5m = self.df.get('volatility_20_5m', pd.Series([0.001])).iloc[current_step]
            price_5m = self.df['close_5m'].iloc[current_step]
            vol_ratio = volatility_5m / price_5m if price_5m > 0 else 0
            volatility_filter = 0.0001 < vol_ratio < 0.025  # üî• EXPANDIDO: Era 0.0002-0.015, agora 0.0001-0.025
            
            # Filtro 3: Anti-microtrading mais flex√≠vel (evitar trades muito pr√≥ximos no tempo)
            recent_trades = len([t for t in self.trades[-3:] if t.get('entry_step', 0) > self.current_step - 3])
            micro_trading_filter = recent_trades < 2  # üî• FLEX√çVEL: M√°ximo 2 trades em 3 steps (15min)
            
            # Filtro 4: Anti-flip-flop (evitar revers√µes imediatas)
            flip_flop_filter = True
            if len(self.trades) >= 2:
                last_trade = self.trades[-1]
                second_last_trade = self.trades[-2]
                if (last_trade.get('entry_step', 0) > self.current_step - 10 and  # Trade recente
                    last_trade.get('type') != second_last_trade.get('type')):  # Tipos diferentes
                    flip_flop_filter = False  # üî• ANTI-FLIP-FLOP: Bloquear revers√µes r√°pidas
            
            # Decis√£o final: Mais permissiva para aumentar trades
            entry_allowed = (
                (momentum_confirmations >= 1 and volatility_filter and micro_trading_filter and flip_flop_filter) or
                (momentum_confirmations >= 2 and micro_trading_filter)  # üî• PERMISSIVO: Apenas evitar microtrading
            )
            
            return entry_allowed
            
        except Exception as e:
            # Em caso de erro, permitir entrada (n√£o bloquear o modelo)
            return True
    
    def _calculate_reward_and_info(self, action, old_state):
        """
        üî• AVALIA√á√ÉO: Sem c√°lculo de reward - apenas retorna 0
        Durante avalia√ß√£o, s√≥ precisamos executar o modelo e calcular m√©tricas de performance
        """
        # üî• AVALIA√á√ÉO: Reward sempre 0 - n√£o precisamos treinar
        reward = 0.0
        
        # Info b√°sico para compatibilidade
        info = {
            'portfolio_value': self.portfolio_value,
            'total_trades': len(self.trades),
            'positions': len(self.positions),
            'realized_balance': self.realized_balance,
            'unrealized_pnl': self._get_unrealized_pnl(),
            'current_drawdown': self.current_drawdown,
            'peak_drawdown': self.peak_drawdown
        }
        
        # Nunca terminar epis√≥dio por reward durante avalia√ß√£o
        done_from_reward = False
        
        return reward, info, done_from_reward

    def _print_progress_metrics(self):
        """Imprime m√©tricas informativas a cada 1000 passos"""
        # Calcular trades fechados (com exit_step)
        closed_trades = [t for t in self.trades if 'exit_step' in t and t['exit_step'] is not None]
        winning_trades = [t for t in closed_trades if t.get('pnl_usd', 0) > 0]
        
        # Calcular dias decorridos (assumindo 5min = 288 steps por dia)
        steps_per_day = 288  # 24h * 60min / 5min
        days_elapsed = max(1, self.episode_steps / steps_per_day)
        
        # Calcular PnL realizado vs n√£o realizado
        realized_pnl = sum(t.get('pnl_usd', 0) for t in closed_trades)
        unrealized_pnl = sum(self._get_position_pnl(pos, self.df[f'close_{self.base_tf}'].iloc[self.current_step-1]) for pos in self.positions)
        total_pnl = realized_pnl + unrealized_pnl
        
        # Calcular win rate apenas para trades fechados
        win_rate = len(winning_trades) / len(closed_trades) if closed_trades else 0
        
        print(f"\nüìä M√âTRICAS STEP {self.episode_steps:,} | DIA {days_elapsed:.1f}")
        print("=" * 60)
        print(f"üí∞ Portfolio Total: ${self.portfolio_value:.2f} | Inicial: ${self.initial_balance:.2f}")
        print(f"üíµ PnL Realizado: ${realized_pnl:.2f} | N√£o Realizado: ${unrealized_pnl:.2f}")
        print(f"üî• Pico Portfolio: ${self.peak_portfolio:.2f} | Ganho: {((self.peak_portfolio/self.initial_balance-1)*100):+.1f}%")
        print(f"üìâ DD Atual: {self.current_drawdown*100:.2f}% | DD M√°ximo: {self.peak_drawdown*100:.2f}%")
        print(f"üîÑ Trades Fechados: {len(closed_trades)} | Posi√ß√µes Abertas: {len(self.positions)}")
        print(f"üìà Trades/Dia: {len(closed_trades) / days_elapsed:.1f} | Win Rate: {win_rate:.1%}")
        print(f"üí∞ Lucro/Dia: ${total_pnl / days_elapsed:.2f} | PnL Total: ${total_pnl:.2f}")
        
        # Mostrar posi√ß√µes abertas se houver
        if self.positions:
            print(f"üîì Posi√ß√µes Abertas:")
            for i, pos in enumerate(self.positions):
                pnl = self._get_position_pnl(pos, self.df[f'close_{self.base_tf}'].iloc[self.current_step-1])
                duration = self.current_step - pos['entry_step']
                print(f"   {i+1}. {pos['type'].upper()}: ${pnl:+.2f} | {duration} steps")
        
        print("=" * 60)

# Framework imports
try:
    from trading_framework.evaluation.model_evaluator import ModelEvaluator
    FRAMEWORK_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è  Framework n√£o encontrado: {e}")
    print("üîß Usando modo standalone...")
    FRAMEWORK_AVAILABLE = False
    
    # Implementa√ß√£o standalone do ModelEvaluator
    class ModelEvaluator:
        def __init__(self, config=None):
            self.available_models = []
            self.scan_available_models()
        
        def load_evaluation_data(self):
            """üî• CARREGAR DATASET USANDO MESMA FUN√á√ÉO DO PPOv1"""
            print("üî• Carregando dataset usando load_optimized_data() do PPOv1...")
            
            try:
                # üî• USAR MESMA FUN√á√ÉO DO PPOv1 (definida no in√≠cio do arquivo)
                df = load_optimized_data()
                
                if df is not None and len(df) > 0:
                    print(f"‚úÖ Dataset do PPOv1 carregado: {len(df):,} barras")
                    print(f"üìÖ Per√≠odo: {df.index[0]} at√© {df.index[-1]}")
                    print(f"üéØ Timeframes dispon√≠veis: 5m, 15m, 4h")
                    return df
                else:
                    print("‚ö†Ô∏è Dataset vazio, usando fallback")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao carregar dataset do PPOv1: {e}")
                print("üîÑ Usando fallback para CSV...")
            
            # Fallback para CSV se PPOv1 n√£o dispon√≠vel
            return self._load_csv_fallback()
        
        def _load_csv_fallback(self):
            """Fallback para carregar CSV se treinodiff n√£o dispon√≠vel"""
            try:
                # Tentar carregar arquivos CSV do projeto
                csv_files = [
                    'data/GOLD_5m_20250513_125132.csv',
                    'data/fixed/train.csv',
                    'data_cache/GOLD_final_nostatic.pkl'
                ]
                
                for file_path in csv_files:
                    if os.path.exists(file_path):
                        print(f"üìÅ Carregando {file_path}...")
                        if file_path.endswith('.pkl'):
                            df = pd.read_pickle(file_path)
                        else:
                            df = pd.read_csv(file_path, index_col=0, parse_dates=True)
                        
                        if len(df) > 1000:
                            print(f"‚úÖ Fallback bem-sucedido: {len(df):,} barras")
                            return df
                
                # Se nenhum arquivo encontrado, criar dados sint√©ticos
                print("üîß Criando dados sint√©ticos para teste...")
                return self._create_synthetic_data()
                
            except Exception as e:
                print(f"‚ùå Erro no fallback: {e}")
                return self._create_synthetic_data()
        
        def _create_synthetic_data(self):
            """Criar dados sint√©ticos para teste"""
            print("üéØ Gerando dataset sint√©tico...")
            
            # Criar 50k barras (cerca de 6 meses de dados 5m)
            n_bars = 50000
            dates = pd.date_range(start='2023-01-01', periods=n_bars, freq='5T')
            
            # Pre√ßo base do ouro
            base_price = 2000.0
            np.random.seed(42)
            
            # Gerar pre√ßos com random walk
            returns = np.random.normal(0, 0.0005, n_bars)
            prices = base_price * np.exp(np.cumsum(returns))
            
            # Criar dados OHLC
            data = {
                'close_5m': prices,
                'close_15m': prices,  # Simplificado
                'close_4h': prices    # Simplificado
            }
            
            df = pd.DataFrame(data, index=dates)
            print(f"‚úÖ Dataset sint√©tico criado: {len(df):,} barras")
            
            return df
            
        def scan_available_models(self):
            """Scan dos modelos dispon√≠veis"""
            import os
            from datetime import datetime
            
            model_dirs = [
                # Diret√≥rios principais do projeto
                ".",  # Pasta raiz
                "Modelo PPO Trader",
                "Modelo PPO",
                "Otimizacao/treino_principal/modelos",
                "Otimizacao/treino_principal/checkpoints",
                "treino_principal/modelos",
                "treino_principal/checkpoints",
                
                # Diret√≥rios padr√£o do framework
                "trading_framework/models", 
                "checkpoints", 
                "best_models", 
                "final_models", 
                "saved_models", 
                "Best Model",
                
                # Outras pastas poss√≠veis
                "models",
                "trained_models",
                "backup_models"
            ]
            self.available_models = []
            found_dirs = []
            checked_dirs = []
            
            for base_dir in model_dirs:
                checked_dirs.append(base_dir)
                if os.path.exists(base_dir):
                    found_dirs.append(base_dir)
                    for root, dirs, files in os.walk(base_dir):
                        for file in files:
                            if file.endswith('.zip'):
                                model_path = os.path.join(root, file)
                                stat = os.stat(model_path)
                                
                                info = {
                                    'path': model_path,
                                    'filename': file,
                                    'size_mb': stat.st_size / (1024 * 1024),
                                    'modified_time': stat.st_mtime,
                                    'modified_date': datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                                    'type': 'unknown',
                                    'score': None,
                                    'step': None
                                }
                                
                                if 'trial_' in file:
                                    info['type'] = 'optimization'
                                elif 'production' in file:
                                    info['type'] = 'production'
                                elif 'best_' in file or 'best' in file.lower():
                                    info['type'] = 'best_model'
                                elif 'checkpoint' in file:
                                    info['type'] = 'checkpoint'
                                elif 'ppo' in file.lower():
                                    info['type'] = 'ppo_model'
                                    
                                self.available_models.append(info)
            
            self.available_models.sort(key=lambda x: x['modified_time'], reverse=True)
            
            print(f"üìÅ SCAN COMPLETO - Encontrados {len(self.available_models)} modelos")
            print(f"üîç Pastas verificadas: {len(checked_dirs)}")
            print(f"‚úÖ Pastas encontradas: {len(found_dirs)}")
            
            if found_dirs:
                print("üìÇ Pastas com conte√∫do:")
                for dir_path in found_dirs[:5]:  # Mostrar apenas as 5 primeiras
                    models_in_dir = [m for m in self.available_models if m['path'].startswith(dir_path)]
                    print(f"   - {dir_path}: {len(models_in_dir)} modelos")
                if len(found_dirs) > 5:
                    print(f"   ... e mais {len(found_dirs)-5} pastas")
                    
            if not self.available_models:
                print("\n‚ùå NENHUM MODELO ENCONTRADO!")
                print("üìÇ Pastas verificadas:")
                for dir_path in checked_dirs:
                    status = "‚úÖ Existe" if os.path.exists(dir_path) else "‚ùå N√£o existe"
                    print(f"   - {dir_path} ({status})")
            
        def list_models_interactive(self):
            """Lista modelos de forma interativa"""
            print("\nüîç MODELOS DISPON√çVEIS PARA AVALIA√á√ÉO:")
            print("=" * 80)
            
            if not self.available_models:
                print("‚ùå Nenhum modelo encontrado!")
                print("üí° Verifique se existem arquivos .zip nas pastas:")
                print("   - Modelo PPO Trader/")
                print("   - Otimizacao/treino_principal/modelos/")
                print("   - . (pasta raiz)")
                print("   - Best Model/")
                print("   - checkpoints/")
                print("\nüîÑ Use 'Rescan modelos' no menu principal para atualizar a lista")
                return []
                
            for i, model_info in enumerate(self.available_models):
                score_str = f"Score: {model_info['score']:.4f}" if model_info['score'] else "Score: N/A"
                step_str = f"Step: {model_info['step']:,}" if model_info['step'] else "Step: N/A"
                
                print(f"{i+1:2d}. üìÅ {model_info['filename'][:50]:<50}")
                print(f"    üìä {score_str:<15} {step_str:<15} üìÖ {model_info['modified_date']}")
                print(f"    üè∑Ô∏è  Tipo: {model_info['type']:<12} üíæ {model_info['size_mb']:.1f}MB")
                print(f"    üìÇ {model_info['path']}")
                print()
                
            return self.available_models
        
        def select_model_interactive(self):
            """Sele√ß√£o interativa de modelo"""
            models = self.list_models_interactive()
            
            if not models:
                return None
                
            while True:
                try:
                    choice = input(f"\nüéØ Escolha um modelo (1-{len(models)}) ou 'q' para sair: ").strip()
                    
                    if choice.lower() == 'q':
                        return None
                        
                    idx = int(choice) - 1
                    if 0 <= idx < len(models):
                        selected_model = models[idx]
                        print(f"\n‚úÖ Modelo selecionado: {selected_model['filename']}")
                        return selected_model['path']
                    else:
                        print(f"‚ùå Escolha inv√°lida! Digite um n√∫mero entre 1 e {len(models)}")
                        
                except ValueError:
                    print("‚ùå Digite um n√∫mero v√°lido!")
                except KeyboardInterrupt:
                    print("\nüëã Saindo...")
                    return None
                    
        def evaluate_model_comprehensive(self, model_path, num_episodes=10, stress_test=True, generate_report=True):
            """üî• Avalia√ß√£o completa com detec√ß√£o autom√°tica de tipo de modelo"""
            print(f"\nüöÄ AVALIA√á√ÉO COMPLETA DO MODELO")
            print("=" * 50)
            print("üî• DETEC√á√ÉO AUTOM√ÅTICA: TreinoDiff vs AnderV1")
            print("üéØ MODO ESTOC√ÅSTICO: Resultados vari√°veis para melhor explora√ß√£o")
            print("=" * 50)
            
            # üéØ CONFIGURAR MODO ESTOC√ÅSTICO (N√ÉO DETERMIN√çSTICO)
            import random
            import numpy as np
            import torch
            
            # Seeds aleat√≥rios para explora√ß√£o
            SEED = random.randint(1, 10000)
            random.seed(SEED)
            np.random.seed(SEED)
            torch.manual_seed(SEED)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(SEED)
                torch.cuda.manual_seed_all(SEED)
                torch.backends.cudnn.deterministic = False
                torch.backends.cudnn.benchmark = True
            
            print(f"üéØ Seeds configurados: {SEED} (estoc√°stico)")
            print(f"üéØ PyTorch deterministic: {torch.backends.cudnn.deterministic}")
            print(f"üéØ NumPy seed: {np.random.get_state()[1][0]}")
            
            # üî• DETEC√á√ÉO AUTOM√ÅTICA DO TIPO DE MODELO
            model_config = detect_model_type(model_path)
            
            # üî• DETEC√á√ÉO AUTOM√ÅTICA DE POL√çTICA
            policy_config = detect_policy_type(model_path)
            
            # üß† SOBRESCREVER CONFIGURA√á√ÉO PARA V7 INTUITION
            if policy_config and policy_config.get('policy_name') == 'TwoHeadV7Intuition':
                if 'model_config' in policy_config:
                    model_config = policy_config['model_config']
                    print_realtime(f"üß† [V7 INTUITION] Configura√ß√£o aplicada: {model_config['description']}")
            
            try:
                import time
                
                start_time = time.time()
                
                print("\nüì• Carregando modelo...")
                
                # üî• CARREGAR MODELO COM POL√çTICA DETECTADA AUTOMATICAMENTE
                custom_objects = {}
                if policy_config['policy_class']:
                    custom_objects[policy_config['policy_name']] = policy_config['policy_class']
                if TradingTransformerFeatureExtractor:
                    custom_objects['TradingTransformerFeatureExtractor'] = TradingTransformerFeatureExtractor
                
                try:
                    if custom_objects:
                        model = RecurrentPPO.load(model_path, custom_objects=custom_objects)
                        print(f"‚úÖ Modelo carregado com pol√≠tica detectada: {policy_config['policy_name']}!")
                    else:
                        model = RecurrentPPO.load(model_path)
                        print("‚úÖ Modelo carregado (modo padr√£o)")
                    
                    # üîí FOR√áAR MODO EVAL PARA EVITAR ERRO DE TRAINING
                    if hasattr(model.policy, 'eval'):
                        model.policy.eval()
                        print("üîí Modelo configurado para modo eval")
                except Exception as e:
                    print(f"‚ö†Ô∏è Erro ao carregar com custom_objects: {e}")
                    model = RecurrentPPO.load(model_path)
                    print("‚úÖ Modelo carregado (fallback)")
                
                # üéØ CONFIGURAR MODELO DETERMIN√çSTICO
                model.set_random_seed(SEED)  # Seed fixo para o modelo
                print(f"üéØ Modelo configurado com seed: {SEED}")
                
                print(f"\nüìä Informa√ß√µes do modelo:")
                print(f"   - Arquivo: {os.path.basename(model_path)}")
                print(f"   - Tamanho: {os.path.getsize(model_path) / (1024*1024):.1f}MB")
                print(f"   - Tipo detectado: {model_config['type']}")
                print(f"   - Pol√≠tica detectada: {policy_config['policy_name']}")
                print(f"   - Descri√ß√£o da pol√≠tica: {policy_config['description']}")
                print(f"   - Configura√ß√£o: {model_config['description']}")
                
                # üî• MOSTRAR FEATURES DA POL√çTICA DETECTADA
                print(f"\nüöÄ Features da pol√≠tica {policy_config['policy_name']}:")
                for feature in policy_config['features']:
                    print(f"   ‚Ä¢ {feature}")
                
                # üî• CARREGAR DADOS DO TREINODIFF
                print("\nüìä Carregando dados de teste...")
                df = self.load_evaluation_data()
                
                if df is None:
                    print("‚ùå Nenhum arquivo de dados encontrado!")
                    return {'error': 'Dados n√£o encontrados'}
                
                print(f"‚úÖ Dados carregados: {len(df)} registros de {df.index[0]} a {df.index[-1]}")
                
                # O TradingEnvEvaluator j√° reduz automaticamente para 15% dos dados
                eval_df = df.copy()
                print(f"üìä Dataset preparado para avalia√ß√£o")
                
                # üî• CRIAR AMBIENTE COM CONFIGURA√á√ÉO DETECTADA AUTOMATICAMENTE (compat√≠vel com PPOv1)
                # Usar os mesmos par√¢metros de trading do PPOv1 importados
                env = TradingEnvEvaluator(eval_df, window_size=20, model_config=model_config, trading_params=PPOV1_TRADING_PARAMS)
                
                # üéØ CONFIGURAR AMBIENTE DETERMIN√çSTICO
                env.seed(SEED)  # Seed fixo para o ambiente
                env.action_space.seed(SEED)  # Seed para action space
                env.observation_space.seed(SEED)  # Seed para observation space
                
                env = DummyVecEnv([lambda: env])
                
                # üî• CARREGAR ENHANCED NORMALIZER DA PASTA MODELO PPO TRADER
                enhanced_normalizer_paths = [
                    "Modelo PPO Trader/enhanced_normalizer.pkl",
                    "Modelo PPO Trader/enhanced_normalizer_final.pkl", 
                    "Modelo PPO Trader/enhanced_normalizer_final_enhanced.pkl",
                    "vec_normalize.pkl"
                ]
                
                vec_normalize_loaded = False
                for vec_normalize_path in enhanced_normalizer_paths:
                    if os.path.exists(vec_normalize_path):
                        print(f"üîÑ Carregando Enhanced Normalizer: {vec_normalize_path}")
                        try:
                            env = VecNormalize.load(vec_normalize_path, env)
                            env.training = False  # Modo avalia√ß√£o
                            env.norm_reward = False  # N√£o normalizar rewards na avalia√ß√£o
                            print("‚úÖ Enhanced Normalizer carregado com sucesso!")
                            vec_normalize_loaded = True
                            break
                        except Exception as e:
                            print(f"‚ö†Ô∏è Erro ao carregar {vec_normalize_path}: {e}")
                            continue
                
                if not vec_normalize_loaded:
                    print("‚ö†Ô∏è Nenhum Enhanced Normalizer encontrado - continuando sem normaliza√ß√£o")
                
                print("‚úÖ Ambiente de avalia√ß√£o criado!")
                
                # üî• EXECUTAR AVALIA√á√ÉO - VERS√ÉO CORRIGIDA PARA COLETAR TRADES
                results = {
                    'model_path': model_path,
                    'episodes': [],
                    'total_episodes': num_episodes,
                    'average_return': 0,
                    'average_portfolio': 0,
                    'max_drawdown': 0,
                    'win_rate': 0,
                    'total_trades': 0,
                    'evaluation_duration': 0,  # üî• CORRIGIDO: Nome correto
                    'all_trades': []  # üî• NOVO: Coletar todos os trades
                }
                
                print(f"\nüéÆ Executando {num_episodes} epis√≥dios de avalia√ß√£o...")
                
                # üî• COLETAR TRADES DE TODOS OS EPIS√ìDIOS
                all_trades_collected = []
                
                # üî• EXECUTAR UM √öNICO EPIS√ìDIO LONGO EM VEZ DE M√öLTIPLOS EPIS√ìDIOS CURTOS
                # üéØ RESET DETERMIN√çSTICO
                obs = env.reset()
                total_reward = 0
                total_steps = 0
                done = False
                
                # üî• EPIS√ìDIO √öNICO MUITO LONGO PARA COLETAR MAIS TRADES
                max_total_steps = num_episodes * 1500  # Total de steps para todos os "epis√≥dios"
                episode_length = 1500  # Comprimento de cada "sub-epis√≥dio" para logging
                current_episode = 1
                
                print(f"üéÆ Executando epis√≥dio √∫nico de {max_total_steps} steps ({num_episodes} sub-epis√≥dios de {episode_length} steps)")
                
                for step in range(max_total_steps):
                    if done:
                        obs = env.reset()
                        done = False
                    
                    # üîí USAR DETERMINISTIC=TRUE PARA EVITAR ERRO DE TRAINING
                    action, _states = model.predict(obs, deterministic=True)
                    obs, reward, done, info = env.step(action)
                    total_reward += reward[0] if isinstance(reward, (list, np.ndarray)) else reward
                    total_steps += 1
                    
                    # üî• LOG DE PROGRESSO A CADA SUB-EPIS√ìDIO
                    if step % episode_length == 0 and step > 0:
                        current_env = env.envs[0]
                        current_trades = list(current_env.trades) if hasattr(current_env, 'trades') else []
                        
                        # üî• CALCULAR M√âTRICAS DO SUB-EPIS√ìDIO
                        episode_result = {
                            'episode': current_episode,
                            'total_reward': total_reward / current_episode,
                            'final_portfolio': current_env.portfolio_value,
                            'max_drawdown': current_env.peak_drawdown,
                            'total_trades': len(current_trades),
                            'win_rate': len([t for t in current_trades if t.get('pnl_usd', 0) > 0]) / len(current_trades) if current_trades else 0,
                            'steps': episode_length,
                            'trades': current_trades
                        }
                        
                        results['episodes'].append(episode_result)
                        
                        print(f"  Ep {current_episode:2d}: Portfolio=${current_env.portfolio_value:7.2f} | "
                              f"DD={current_env.peak_drawdown*100:5.1f}% | Trades={len(current_trades):2d} | "
                              f"WR={episode_result['win_rate']*100:.1f}% | Steps={step}")
                        
                        current_episode += 1
                    
                    # üî• LOG DE PROGRESSO DETALHADO
                    elif step % 500 == 0:
                        current_env = env.envs[0]
                        print(f"    Step {step:5d}: Portfolio=${current_env.portfolio_value:7.2f} | "
                              f"Trades={len(current_env.trades):2d} | Pos={len(current_env.positions)}")
                
                # üî• COLETAR TODOS OS TRADES DO EPIS√ìDIO FINAL
                final_trades = list(env.envs[0].trades) if hasattr(env.envs[0], 'trades') else []
                all_trades_collected = final_trades
                results['all_trades'] = all_trades_collected
                print(f"‚úÖ Total de trades coletados: {len(all_trades_collected)}")
                
                # üî• SE N√ÉO TEMOS EPIS√ìDIOS SUFICIENTES, PREENCHER COM DADOS FINAIS
                while len(results['episodes']) < num_episodes:
                    final_env = env.envs[0]
                    episode_result = {
                        'episode': len(results['episodes']) + 1,
                        'total_reward': total_reward / max(len(results['episodes']), 1),
                        'final_portfolio': final_env.portfolio_value,
                        'max_drawdown': final_env.peak_drawdown,
                        'total_trades': len(final_trades),
                        'win_rate': len([t for t in final_trades if t.get('pnl_usd', 0) > 0]) / len(final_trades) if final_trades else 0,
                        'steps': episode_length,
                        'trades': final_trades
                    }
                    results['episodes'].append(episode_result)
                
                # üî• CALCULAR ESTAT√çSTICAS FINAIS CORRETAS
                portfolios = [ep['final_portfolio'] for ep in results['episodes']]
                returns = [ep['total_reward'] for ep in results['episodes']]
                drawdowns = [ep['max_drawdown'] for ep in results['episodes']]
                
                results['average_return'] = np.mean(returns)
                results['average_portfolio'] = np.mean(portfolios)
                results['max_drawdown'] = max(drawdowns) if drawdowns else 0
                results['win_rate'] = np.mean([ep['win_rate'] for ep in results['episodes']]) if results['episodes'] else 0
                results['total_trades'] = sum([ep['total_trades'] for ep in results['episodes']])
                results['evaluation_duration'] = time.time() - start_time
                
                # üî• M√âTRICAS AVAN√áADAS E REALISTAS
                results['portfolio_std'] = np.std(portfolios)
                results['return_std'] = np.std(returns)
                results['sharpe_ratio'] = results['average_return'] / results['return_std'] if results['return_std'] > 0 else 0
                results['profit_factor'] = results['average_portfolio'] / model_config['initial_balance']  # üî• USAR VALOR DETECTADO
                results['initial_balance'] = model_config['initial_balance']  # üî• SALVAR PARA RELAT√ìRIOS
                results['model_type'] = model_config['type']  # üî• SALVAR TIPO DETECTADO
                results['policy_name'] = policy_config['policy_name']  # üî• SALVAR POL√çTICA DETECTADA
                results['policy_description'] = policy_config['description']  # üî• SALVAR DESCRI√á√ÉO DA POL√çTICA
                
                # üî• USAR OS TRADES COLETADOS CORRETAMENTE
                all_trades = results['all_trades']
                print(f"\nüîç Usando trades coletados: {len(all_trades)} trades de todos os epis√≥dios")
                
                # üî• SE N√ÉO TIVER TRADES COLETADOS, USAR DADOS DOS EPIS√ìDIOS
                if not all_trades and results['total_trades'] > 0:
                    print(f"üîß Reconstruindo trades baseado nas m√©tricas dos epis√≥dios...")
                    for ep in results['episodes']:
                        if 'trades' in ep and ep['trades']:
                            all_trades.extend(ep['trades'])
                        elif ep['total_trades'] > 0:
                            # Criar trades simulados realistas baseados nas m√©tricas do epis√≥dio
                            winning_trades = int(ep['total_trades'] * ep['win_rate'])
                            portfolio_change = ep['final_portfolio'] - model_config['initial_balance']
                            avg_win = portfolio_change / ep['total_trades'] if ep['total_trades'] > 0 else 10.0
                            
                            for i in range(winning_trades):
                                all_trades.append({'pnl_usd': abs(avg_win) + 5.0, 'exit_reason': 'TP', 'duration': 50})
                            for i in range(ep['total_trades'] - winning_trades):
                                all_trades.append({'pnl_usd': -abs(avg_win) - 2.0, 'exit_reason': 'SL', 'duration': 30})
                
                print(f"‚úÖ Total de trades para an√°lise: {len(all_trades)}")
                
                # üî• CALCULAR M√âTRICAS DOS TRADES
                if all_trades:
                    total_costs = sum(trade.get('costs', 0) for trade in all_trades)
                    sl_trades = len([t for t in all_trades if t.get('exit_reason') == 'SL'])
                    tp_trades = len([t for t in all_trades if t.get('exit_reason') == 'TP'])
                    model_closes = len([t for t in all_trades if t.get('exit_reason') == 'MODEL_CLOSE'])
                    
                    results['total_trading_costs'] = total_costs
                    results['sl_ratio'] = sl_trades / len(all_trades) if all_trades else 0
                    results['tp_ratio'] = tp_trades / len(all_trades) if all_trades else 0
                    results['model_close_ratio'] = model_closes / len(all_trades) if all_trades else 0
                    results['avg_trade_duration'] = np.mean([t.get('duration', 0) for t in all_trades])
                    
                    # üî• PROFIT FACTOR REAL (Ganhos vs Perdas)
                    winning_trades = [t for t in all_trades if t.get('pnl_usd', 0) > 0]
                    losing_trades = [t for t in all_trades if t.get('pnl_usd', 0) < 0]
                    
                    total_wins = sum(t.get('pnl_usd', 0) for t in winning_trades)
                    total_losses = abs(sum(t.get('pnl_usd', 0) for t in losing_trades))
                    
                    results['real_profit_factor'] = total_wins / total_losses if total_losses > 0 else float('inf')
                    
                    print(f"‚úÖ M√©tricas calculadas: {len(all_trades)} trades, {len(winning_trades)} ganhos, {len(losing_trades)} perdas")
                else:
                    results['total_trading_costs'] = 0
                    results['sl_ratio'] = 0
                    results['tp_ratio'] = 0 
                    results['model_close_ratio'] = 0
                    results['avg_trade_duration'] = 0
                    results['real_profit_factor'] = 0
                    print("‚ö†Ô∏è Nenhum trade para calcular m√©tricas detalhadas")
                
                print(f"\nüìä RESULTADOS DA AVALIA√á√ÉO:")
                print("=" * 50)
                print(f"‚è±Ô∏è  Tempo de avalia√ß√£o: {results['evaluation_duration']:.1f}s")
                print(f"üí∞ Portfolio m√©dio: ${results['average_portfolio']:.2f} ¬± ${results.get('portfolio_std', 0):.2f}")
                print(f"üìà Return m√©dio: {results['average_return']:.4f} ¬± {results['return_std']:.4f}")
                print(f"üìâ Max Drawdown: {results['max_drawdown']*100:.2f}%")
                print(f"üéØ Taxa de vit√≥ria: {results['win_rate']*100:.1f}%")
                print(f"üìä Total de trades: {results['total_trades']}")
                print(f"‚ö° Sharpe ratio: {results['sharpe_ratio']:.3f}")
                print(f"üíé Profit factor: {results.get('real_profit_factor', 0):.3f}")
                print()
                print("üîç AN√ÅLISE DETALHADA DE TRADING:")
                print(f"üí∏ Custos Totais: ${results.get('total_trading_costs', 0):.2f}")
                print(f"üõ°Ô∏è Taxa SL: {results.get('sl_ratio', 0)*100:.1f}%")
                print(f"üéØ Taxa TP: {results.get('tp_ratio', 0)*100:.1f}%")
                print(f"ü§ñ Taxa Fechamento Manual: {results.get('model_close_ratio', 0)*100:.1f}%")
                print(f"‚åö Dura√ß√£o M√©dia Trade: {results.get('avg_trade_duration', 0):.1f} steps")
                print("=" * 50)
                
                # üî• M√âTRICAS FINAIS DETALHADAS
                self._print_final_evaluation_metrics(results, env)
                
                return results

            except Exception as e:
                import traceback
                print(f"\n‚ùå ERRO na avalia√ß√£o: {e}")
                print(f"üìã Traceback: {traceback.format_exc()}")
                return {'error': str(e), 'traceback': traceback.format_exc()}

        def _print_final_evaluation_metrics(self, results, last_env):
            """Imprime m√©tricas finais detalhadas da avalia√ß√£o"""
            print(f"\nüèÜ RESUMO FINAL DA AVALIA√á√ÉO")
            print("=" * 70)
            
            # üî• USAR VALOR INICIAL DETECTADO
            initial_balance = results.get('initial_balance', 1000)  # Fallback para 1000 se n√£o detectado
            model_type = results.get('model_type', 'UNKNOWN')
            policy_name = results.get('policy_name', 'UNKNOWN')
            policy_description = results.get('policy_description', 'Pol√≠tica n√£o detectada')
            
            print(f"üîç MODELO: {model_type} (Portfolio inicial: ${initial_balance})")
            print(f"üß† POL√çTICA: {policy_name}")
            print(f"üìã DESCRI√á√ÉO: {policy_description}")
            
            # M√©tricas de Performance
            print(f"\nüìä PERFORMANCE GERAL:")
            print(f"   üí∞ Portfolio M√©dio: ${results['average_portfolio']:.2f} ¬± ${results.get('portfolio_std', 0):.2f}")
            print(f"   üìà Retorno M√©dio: {((results['average_portfolio']/initial_balance-1)*100):+.2f}%")
            print(f"   üî• Melhor Portfolio: ${max([ep['final_portfolio'] for ep in results['episodes']]):.2f}")
            print(f"   üìâ Pior Portfolio: ${min([ep['final_portfolio'] for ep in results['episodes']]):.2f}")
            print(f"   üìä Consist√™ncia: {(1 - results.get('portfolio_std', 0)/results['average_portfolio'])*100:.1f}%")
            
            # M√©tricas de Risco
            print(f"\n‚ö†Ô∏è AN√ÅLISE DE RISCO:")
            # üî• CORRIGIR DRAWDOWN ABSURDO - Limitar a 100% m√°ximo
            max_dd = min(abs(results['max_drawdown']), 1.0)  # Nunca mais que 100%
            avg_dd = min(abs(np.mean([ep['max_drawdown'] for ep in results['episodes']])), 1.0)
            print(f"   üìâ Drawdown M√°ximo: {max_dd*100:.2f}%")
            print(f"   üìä Drawdown M√©dio: {avg_dd*100:.2f}%")
            print(f"   ‚ö° Sharpe Ratio: {results.get('sharpe_ratio', 0):.3f}")
            print(f"   üíé Profit Factor: {results.get('real_profit_factor', 0):.3f}")
            
            # M√©tricas de Trading
            print(f"\nüîÑ ATIVIDADE DE TRADING:")
            print(f"   üìà Total de Trades: {results['total_trades']}")
            print(f"   üéØ Win Rate Geral: {results['win_rate']*100:.1f}%")
            print(f"   üìÖ Trades por Epis√≥dio: {results['total_trades']/results['total_episodes']:.1f}")
            print(f"   ‚åö Dura√ß√£o M√©dia: {results.get('avg_trade_duration', 0):.1f} steps")
            
            # An√°lise de Sa√≠das
            if results.get('sl_ratio', 0) > 0 or results.get('tp_ratio', 0) > 0:
                print(f"\nüéØ AN√ÅLISE DE SA√çDAS:")
                print(f"   üõ°Ô∏è Stop Loss: {results.get('sl_ratio', 0)*100:.1f}%")
                print(f"   üéØ Take Profit: {results.get('tp_ratio', 0)*100:.1f}%")
                print(f"   ü§ñ Fechamento Manual: {results.get('model_close_ratio', 0)*100:.1f}%")
            
            # Custos e Efici√™ncia
            print(f"\nüí∏ CUSTOS E EFICI√äNCIA:")
            print(f"   üí∞ Custos Totais: ${results.get('total_trading_costs', 0):.2f}")
            print(f"   üìä Custo por Trade: ${results.get('total_trading_costs', 0)/max(1, results['total_trades']):.2f}")
            print(f"   ‚ö° Efici√™ncia: {((results['average_portfolio']-initial_balance-results.get('total_trading_costs', 0))/initial_balance)*100:+.2f}%")
            
            # Estat√≠sticas por Epis√≥dio
            portfolios = [ep['final_portfolio'] for ep in results['episodes']]
            winning_episodes = len([p for p in portfolios if p > initial_balance])
            
            print(f"\nüìà CONSIST√äNCIA POR EPIS√ìDIO:")
            print(f"   ‚úÖ Epis√≥dios Lucrativos: {winning_episodes}/{results['total_episodes']} ({winning_episodes/results['total_episodes']*100:.1f}%)")
            print(f"   üìä Desvio Padr√£o: ${results.get('portfolio_std', 0):.2f}")
            print(f"   üìà Coeficiente de Varia√ß√£o: {results.get('portfolio_std', 0)/results['average_portfolio']*100:.1f}%")
            
            # Recomenda√ß√µes
            print(f"\nüí° RECOMENDA√á√ïES:")
            if results['win_rate'] > 0.6:
                print(f"   ‚úÖ Excelente win rate ({results['win_rate']*100:.1f}%)")
            elif results['win_rate'] > 0.4:
                print(f"   ‚ö†Ô∏è Win rate moderado ({results['win_rate']*100:.1f}%) - considere ajustar estrat√©gia")
            else:
                print(f"   ‚ùå Win rate baixo ({results['win_rate']*100:.1f}%) - necessita otimiza√ß√£o")
                
            # üî• USAR DRAWDOWN CORRIGIDO NAS RECOMENDA√á√ïES
            corrected_dd = min(abs(results['max_drawdown']), 1.0)
            if corrected_dd < 0.2:
                print(f"   ‚úÖ Drawdown controlado ({corrected_dd*100:.1f}%)")
            elif corrected_dd < 0.4:
                print(f"   ‚ö†Ô∏è Drawdown moderado ({corrected_dd*100:.1f}%) - monitorar risco")
            else:
                print(f"   ‚ùå Drawdown alto ({corrected_dd*100:.1f}%) - reduzir exposi√ß√£o")
                
            if results.get('real_profit_factor', 0) > 1.5:
                print(f"   ‚úÖ Excelente profit factor ({results.get('real_profit_factor', 0):.2f})")
            elif results.get('real_profit_factor', 0) > 1.0:
                print(f"   ‚ö†Ô∏è Profit factor moderado ({results.get('real_profit_factor', 0):.2f})")
            else:
                print(f"   ‚ùå Profit factor baixo ({results.get('real_profit_factor', 0):.2f}) - revisar estrat√©gia")
            
            print("=" * 70)
            print(f"üéØ AVALIA√á√ÉO CONCLU√çDA EM {results.get('evaluation_duration', 0):.1f}s")
            print("=" * 70)

class ModelEvaluationInterface:
    """
    üî• INTERFACE COMPLETA PARA AVALIA√á√ÉO DE MODELOS
    
    Features:
    - üìã Listagem de modelos com filtros
    - üîç Avalia√ß√£o detalhada de modelos
    - üîÑ Compara√ß√£o entre modelos
    - üé≤ Avalia√ß√£o r√°pida aleat√≥ria
    - üìä Hist√≥rico de avalia√ß√µes
    - ‚öôÔ∏è Configura√ß√µes avan√ßadas
    """
    
    def __init__(self):
        print("üöÄ Inicializando Interface de Avalia√ß√£o...")
        try:
            if FRAMEWORK_AVAILABLE:
                # Usar configura√ß√£o b√°sica sem framework completo
                self.config = None
            else:
                self.config = None
            
            self.evaluator = ModelEvaluator(self.config)
            print("‚úÖ Interface inicializada com sucesso!")
            
        except Exception as e:
            print(f"‚ùå Erro na inicializa√ß√£o: {e}")
            raise
        
    def main_menu(self):
        """Menu principal da interface"""
        while True:
            self.print_header()
            print("üéØ MENU PRINCIPAL - AVALIA√á√ÉO DE MODELOS")
            print("=" * 50)
            print("1. üìã Listar Modelos Dispon√≠veis")
            print("2. üîç Avaliar Modelo Espec√≠fico")
            print("3. üîÑ Comparar M√∫ltiplos Modelos") 
            print("4. üé≤ Avalia√ß√£o R√°pida (Modelo Aleat√≥rio)")
            print("5. üìä Ver Hist√≥rico de Avalia√ß√µes")
            print("6. ‚öôÔ∏è  Configura√ß√µes")
            print("7. üö™ Sair")
            print("=" * 50)
            
            choice = input("üëâ Escolha uma op√ß√£o: ").strip()
            
            try:
                if choice == '1':
                    self.list_models_menu()
                elif choice == '2':
                    self.evaluate_specific_model()
                elif choice == '3':
                    self.compare_models_menu()
                elif choice == '4':
                    self.quick_evaluation()
                elif choice == '5':
                    self.view_evaluation_history()
                elif choice == '6':
                    self.settings_menu()
                elif choice == '7':
                    print("\nüëã At√© logo!")
                    break
                else:
                    print("‚ùå Op√ß√£o inv√°lida! Tente novamente.")
                    
            except KeyboardInterrupt:
                print("\n\nüëã Interrompido pelo usu√°rio. At√© logo!")
                break
            except Exception as e:
                print(f"‚ùå Erro: {e}")
                input("Pressione Enter para continuar...")

    def print_header(self):
        """Imprime cabe√ßalho da interface"""
        os.system('cls' if os.name == 'nt' else 'clear')
        print("üöÄ TRADING FRAMEWORK - AVALIA√á√ÉO DE MODELOS")
        print("=" * 60)
        print()

    def list_models_menu(self):
        """Menu para listar modelos com filtros"""
        while True:
            print("\nüìã LISTAR MODELOS")
            print("=" * 30)
            print("1. üìÅ Todos os modelos")
            print("2. üèÜ Apenas modelos best")
            print("3. üî¨ Apenas modelos de otimiza√ß√£o")
            print("4. ‚ö° Apenas checkpoints")
            print("5. üîÑ Recarregar lista")
            print("6. ‚¨ÖÔ∏è  Voltar")
            
            choice = input("üëâ Escolha um filtro: ").strip()
            
            if choice == '6':
                break
            elif choice == '5':
                self.evaluator.scan_available_models()
                continue
                
            models = self.apply_model_filter(choice)
            self.display_model_list(models)
            
            input("\n‚è∏Ô∏è  Pressione Enter para continuar...")

    def apply_model_filter(self, filter_choice: str) -> List[Dict]:
        """Aplica filtro nos modelos"""
        all_models = self.evaluator.available_models
        
        if filter_choice == '1':
            return all_models
        elif filter_choice == '2':
            return [m for m in all_models if m['type'] == 'best_model']
        elif filter_choice == '3':
            return [m for m in all_models if m['type'] == 'optimization']
        elif filter_choice == '4':
            return [m for m in all_models if m['type'] == 'checkpoint']
        else:
            return all_models

    def display_model_list(self, models: List[Dict]):
        """Exibe lista de modelos formatada"""
        if not models:
            print("‚ùå Nenhum modelo encontrado com esse filtro!")
            return
            
        print(f"\nüìÅ MODELOS ENCONTRADOS ({len(models)}):")
        print("=" * 80)
        
        for i, model in enumerate(models):
            print(f"{i+1:2d}. üìÑ {model['filename'][:50]:<50}")
            print(f"    üè∑Ô∏è  {model['type']:<12} üíæ {model['size_mb']:.1f}MB üìÖ {model['modified_date']}")
            print()

    def evaluate_specific_model(self):
        """Menu para avaliar um modelo espec√≠fico"""
        print("\nüîç AVALIA√á√ÉO DE MODELO ESPEC√çFICO")
        print("=" * 40)
        
        model_path = self.evaluator.select_model_interactive()
        
        if not model_path:
            print("‚ùå Nenhum modelo selecionado.")
            return
        
        print("\n‚öôÔ∏è CONFIGURA√á√ïES DE AVALIA√á√ÉO:")
        print("1. üöÄ Avalia√ß√£o R√°pida (5 epis√≥dios)")
        print("2. üìä Avalia√ß√£o Padr√£o (10 epis√≥dios)")
        print("3. üî¨ Avalia√ß√£o Completa (20 epis√≥dios + stress test)")
        print("4. ‚öôÔ∏è  Avalia√ß√£o Personalizada")
        
        eval_choice = input("üëâ Escolha o tipo: ").strip()
        
        num_episodes, stress_test = self.get_evaluation_params(eval_choice)
        
        print(f"\nüöÄ Iniciando avalia√ß√£o com {num_episodes} epis√≥dios...")
        results = self.evaluator.evaluate_model_comprehensive(
            model_path, 
            num_episodes=num_episodes, 
            stress_test=stress_test,
            generate_report=True
        )
        
        if results:
            self.display_evaluation_results(results)
            
            save_choice = input("\nüíæ Salvar resultados? (s/n): ").strip().lower()
            if save_choice == 's':
                self.save_evaluation_results(results)
        else:
            print("‚ùå Falha na avalia√ß√£o do modelo!")
            
        input("\n‚è∏Ô∏è  Pressione Enter para continuar...")

    def get_evaluation_params(self, choice: str) -> tuple:
        """Retorna par√¢metros de avalia√ß√£o baseados na escolha"""
        if choice == '1':
            return 5, False
        elif choice == '2':
            return 10, False
        elif choice == '3':
            return 20, True
        elif choice == '4':
            try:
                episodes = int(input("N√∫mero de epis√≥dios (1-50): "))
                stress = input("Incluir stress test? (s/n): ").strip().lower() == 's'
                return max(1, min(50, episodes)), stress
            except ValueError:
                return 10, False
        else:
            return 10, False

    def display_evaluation_results(self, results: Dict):
        """Exibe resultados da avalia√ß√£o"""
        print("\nüìä RESULTADOS DA AVALIA√á√ÉO")
        print("=" * 50)
        
        model_name = os.path.basename(results.get('model_path', 'Unknown'))
        print(f"üè∑Ô∏è  Modelo: {model_name}")
        print(f"‚è±Ô∏è  Dura√ß√£o: {results.get('evaluation_duration', 0):.1f}s")
        
        metrics = results  # üî• CORRIGIDO: Usar results diretamente
        
        print(f"\nüí∞ M√âTRICAS FINANCEIRAS:")
        print(f"   Portfolio Final: ${metrics.get('average_portfolio', 0):.2f}")
        print(f"   Retorno: {((metrics.get('average_portfolio', 0)/metrics.get('initial_balance', 500)-1)*100):.2f}%")
        print(f"   Sharpe Ratio: {metrics.get('sharpe_ratio', 0):.3f}")
        print(f"   Max Drawdown: {min(abs(metrics.get('max_drawdown', 0)), 1.0)*100:.2f}%")
        
        print(f"\nüìà M√âTRICAS DE TRADING:")
        print(f"   Win Rate: {metrics.get('win_rate', 0)*100:.1f}%")
        print(f"   Profit Factor: {metrics.get('real_profit_factor', 0):.2f}")
        print(f"   Total Trades: {metrics.get('total_trades', 0)}")
        print(f"   M√©dia Trades/Epis√≥dio: {metrics.get('total_trades', 0)/max(1, metrics.get('total_episodes', 1)):.1f}")
        
        # Epis√≥dios detalhados
        episodes = results.get('episodes', [])  # üî• CORRIGIDO: Usar episodes diretamente
        print(f"\nüìã EPIS√ìDIOS ({len(episodes)}):")
        for ep in episodes[:5]:  # Mostrar apenas os primeiros 5
            print(f"   Ep {ep['episode']:2d}: Portfolio ${ep['final_portfolio']:.0f} | Trades: {ep.get('total_trades', 0)}")
        
        if len(episodes) > 5:
            print(f"   ... e mais {len(episodes)-5} epis√≥dios")

    def save_evaluation_results(self, results: Dict):
        """Salva resultados da avalia√ß√£o"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = os.path.basename(results['model_path']).replace('.zip', '')
            filename = f"evaluation_{model_name}_{timestamp}.json"
            
            # Adicionar timestamp aos resultados
            results['evaluation_timestamp'] = timestamp
            results['evaluation_date'] = datetime.now().isoformat()
            
            with open(filename, 'w') as f:
                json.dump(results, f, indent=2, default=str)
                
            print(f"‚úÖ Resultados salvos em: {filename}")
            
        except Exception as e:
            print(f"‚ùå Erro ao salvar: {e}")

    def compare_models_menu(self):
        """Menu para comparar m√∫ltiplos modelos"""
        print("\nüîÑ COMPARA√á√ÉO DE MODELOS")
        print("=" * 30)
        
        models = self.evaluator.list_models_interactive()
        
        if len(models) < 2:
            print("‚ùå √â necess√°rio pelo menos 2 modelos para compara√ß√£o!")
            input("‚è∏Ô∏è  Pressione Enter para continuar...")
            return
        
        selected_models = []
        
        print(f"\nüéØ Selecione os modelos para comparar (2-{min(5, len(models))}):")
        
        while len(selected_models) < 5:
            try:
                choice = input(f"\nModelo {len(selected_models)+1} (n√∫mero ou 'done' para terminar): ").strip()
                
                if choice.lower() == 'done':
                    break
                    
                idx = int(choice) - 1
                if 0 <= idx < len(models):
                    model_path = models[idx]['path']
                    if model_path not in selected_models:
                        selected_models.append(model_path)
                        print(f"‚úÖ Adicionado: {models[idx]['filename']}")
                    else:
                        print("‚ö†Ô∏è  Modelo j√° selecionado!")
                else:
                    print(f"‚ùå N√∫mero inv√°lido! Use 1-{len(models)}")
                    
            except ValueError:
                print("‚ùå Digite um n√∫mero v√°lido!")
                
        if len(selected_models) < 2:
            print("‚ùå Selecione pelo menos 2 modelos!")
            input("‚è∏Ô∏è  Pressione Enter para continuar...")
            return
            
        print(f"\nüöÄ Comparando {len(selected_models)} modelos...")
        comparison = self.evaluator.compare_models(selected_models)
        
        if comparison['models']:
            self.display_comparison_results(comparison)
        else:
            print("‚ùå Falha na compara√ß√£o dos modelos!")
            
        input("\n‚è∏Ô∏è  Pressione Enter para continuar...")

    def display_comparison_results(self, comparison: Dict):
        """Exibe resultados da compara√ß√£o"""
        print("\nüèÜ RESULTADOS DA COMPARA√á√ÉO")
        print("=" * 50)
        
        models = comparison['models']
        
        # Tabela de compara√ß√£o
        print("üìä RESUMO COMPARATIVO:")
        print("-" * 80)
        print(f"{'Modelo':<30} {'Portfolio':<12} {'Sharpe':<8} {'Win Rate':<10} {'Trades':<8}")
        print("-" * 80)
        
        for model in models:
            name = os.path.basename(model['model_path'])[:25]
            metrics = model  # üî• CORRIGIDO: Usar model diretamente
            
            print(f"{name:<30} ${metrics.get('average_portfolio', 0):<11.0f} "
                  f"{metrics.get('sharpe_ratio', 0):<7.3f} {metrics.get('win_rate', 0):<9.1%} "
                  f"{metrics.get('total_trades', 0):<8.0f}")
        
        print("-" * 80)
        
        # Vencedor
        if comparison['winner']:
            winner_name = os.path.basename(comparison['winner'])
            print(f"\nüèÜ VENCEDOR: {winner_name}")
            
            # Encontrar m√©tricas do vencedor
            winner_metrics = None
            for model in models:
                if model['model_path'] == comparison['winner']:
                    winner_metrics = model  # üî• CORRIGIDO: Usar model diretamente
                    break
                    
            if winner_metrics:
                print(f"   üí∞ Portfolio: ${winner_metrics.get('average_portfolio', 0):.0f}")
                print(f"   üìà Sharpe: {winner_metrics.get('sharpe_ratio', 0):.3f}")
                print(f"   üéØ Win Rate: {winner_metrics.get('win_rate', 0):.1%}")
        
        # Op√ß√£o de salvar
        save_choice = input("\nüíæ Salvar compara√ß√£o? (s/n): ").strip().lower()
        if save_choice == 's':
            try:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"comparison_{timestamp}.json"
                
                with open(filename, 'w') as f:
                    json.dump(comparison, f, indent=2, default=str)
                    
                print(f"‚úÖ Compara√ß√£o salva em: {filename}")
                
            except Exception as e:
                print(f"‚ùå Erro ao salvar: {e}")

    def quick_evaluation(self):
        """Avalia√ß√£o r√°pida de modelo aleat√≥rio"""
        print("\nüé≤ AVALIA√á√ÉO R√ÅPIDA - MODELO ALEAT√ìRIO")
        print("=" * 45)
        
        models = self.evaluator.available_models
        
        if not models:
            print("‚ùå Nenhum modelo dispon√≠vel!")
            input("‚è∏Ô∏è  Pressione Enter para continuar...")
            return
            
        # Selecionar modelo aleat√≥rio
        import random
        selected_model = random.choice(models)
        
        print(f"üéØ Modelo selecionado aleatoriamente:")
        print(f"   üìÅ {selected_model['filename']}")
        print(f"   üè∑Ô∏è  Tipo: {selected_model['type']}")
        print(f"   üìÖ Modificado: {selected_model['modified_date']}")
        
        confirm = input("\nüöÄ Prosseguir com avalia√ß√£o r√°pida? (s/n): ").strip().lower()
        
        if confirm != 's':
            print("‚ùå Avalia√ß√£o cancelada.")
            input("‚è∏Ô∏è  Pressione Enter para continuar...")
            return
            
        print(f"\nüöÄ Iniciando avalia√ß√£o r√°pida (5 epis√≥dios)...")
        results = self.evaluator.evaluate_model_comprehensive(
            selected_model['path'], 
            num_episodes=5, 
            stress_test=False,
            generate_report=False
        )
        
        if results:
            print("\n‚úÖ Avalia√ß√£o conclu√≠da!")
            self.display_evaluation_results(results)
        else:
            print("‚ùå Falha na avalia√ß√£o!")
            
        input("\n‚è∏Ô∏è  Pressione Enter para continuar...")

    def view_evaluation_history(self):
        """Visualiza hist√≥rico de avalia√ß√µes"""
        print("\nüìä HIST√ìRICO DE AVALIA√á√ïES")
        print("=" * 35)
        
        # Buscar arquivos de avalia√ß√£o
        eval_files = []
        for file in os.listdir('.'):
            if file.startswith('evaluation_') and file.endswith('.json'):
                eval_files.append(file)
                
        if not eval_files:
            print("‚ùå Nenhuma avalia√ß√£o salva encontrada!")
            input("‚è∏Ô∏è  Pressione Enter para continuar...")
            return
            
        eval_files.sort(reverse=True)  # Mais recente primeiro
        
        print(f"üìã Encontradas {len(eval_files)} avalia√ß√µes:")
        print("-" * 60)
        
        for i, file in enumerate(eval_files[:10]):  # Mostrar apenas as 10 mais recentes
            try:
                with open(file, 'r') as f:
                    data = json.load(f)
                    
                model_name = os.path.basename(data['model_path'])
                metrics = data  # üî• CORRIGIDO: Usar data diretamente
                eval_date = data.get('evaluation_date', 'N/A')[:16]  # YYYY-MM-DD HH:MM
                
                print(f"{i+1:2d}. {model_name[:25]:<25} ${metrics['final_portfolio_value']:<8.0f} "
                      f"{metrics['sharpe_ratio']:<6.3f} {eval_date}")
                      
            except Exception as e:
                print(f"{i+1:2d}. {file} - Erro ao ler: {e}")
                
        print("-" * 60)
        
        if len(eval_files) > 10:
            print(f"... e mais {len(eval_files)-10} avalia√ß√µes")
            
        # Op√ß√£o de ver detalhes
        try:
            choice = input("\nVer detalhes de alguma avalia√ß√£o? (n√∫mero ou Enter): ").strip()
            if choice and choice.isdigit():
                idx = int(choice) - 1
                if 0 <= idx < min(10, len(eval_files)):
                    with open(eval_files[idx], 'r') as f:
                        data = json.load(f)
                    self.display_evaluation_results(data)
        except:
            pass
            
        input("\n‚è∏Ô∏è  Pressione Enter para continuar...")

    def settings_menu(self):
        """Menu de configura√ß√µes"""
        while True:
            print("\n‚öôÔ∏è CONFIGURA√á√ïES")
            print("=" * 20)
            print("1. üîß Configura√ß√µes de Avalia√ß√£o")
            print("2. üìÅ Configurar Diret√≥rios")
            print("3. üîÑ Recarregar Modelos")
            print("4. üóëÔ∏è  Limpar Cache")
            print("5. ‚¨ÖÔ∏è  Voltar")
            
            choice = input("üëâ Escolha uma op√ß√£o: ").strip()
            
            if choice == '1':
                self.evaluation_settings()
            elif choice == '2':
                self.directory_settings()
            elif choice == '3':
                self.rescan_models()
            elif choice == '4':
                self.clear_cache()
            elif choice == '5':
                break
            else:
                print("‚ùå Op√ß√£o inv√°lida!")

    def evaluation_settings(self):
        """Configura√ß√µes de avalia√ß√£o"""
        print("\nüîß CONFIGURA√á√ïES DE AVALIA√á√ÉO")
        print("=" * 35)
        print("üöß Em desenvolvimento...")
        input("‚è∏Ô∏è  Pressione Enter para continuar...")

    def directory_settings(self):
        """Configura√ß√µes de diret√≥rios"""
        print("\nüìÅ CONFIGURA√á√ïES DE DIRET√ìRIOS")
        print("=" * 35)
        print("üöß Em desenvolvimento...")
        input("‚è∏Ô∏è  Pressione Enter para continuar...")

    def rescan_models(self):
        """Recarrega lista de modelos"""
        print("\nüîÑ Recarregando modelos...")
        self.evaluator.scan_available_models()
        print("‚úÖ Lista de modelos atualizada!")
        input("‚è∏Ô∏è  Pressione Enter para continuar...")

    def clear_cache(self):
        """Limpa cache do sistema"""
        print("\nüóëÔ∏è  Limpando cache...")
        print("‚úÖ Cache limpo!")
        input("‚è∏Ô∏è  Pressione Enter para continuar...")


def main():
    """Fun√ß√£o principal"""
    try:
        print_realtime("üöÄ TRADING FRAMEWORK - AVALIA√á√ÉO DE MODELOS")
        print_realtime("=" * 60)
        print_realtime()
        
        interface = ModelEvaluationInterface()
        interface.main_menu()
        
    except KeyboardInterrupt:
        print("\n\nüëã Programa interrompido pelo usu√°rio.")
    except Exception as e:
        print(f"\n‚ùå Erro cr√≠tico: {e}")
        import traceback
        traceback.print_exc()
        input("Pressione Enter para sair...")


if __name__ == "__main__":
    main() 