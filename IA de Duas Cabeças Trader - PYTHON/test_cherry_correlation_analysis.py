"""
üìä AN√ÅLISE MATEM√ÅTICA COMPLETA - CHERRY REWARD SYSTEM
Testa correla√ß√µes, intensidade de sinal e adequa√ß√£o matem√°tica para guiar o modelo
"""

import sys
import os
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.metrics import r2_score
import pandas as pd

sys.path.append(r'D:\Projeto')

from trading_framework.rewards.reward_system_simple import create_simple_reward_system

class MockEnv:
    def __init__(self, dd_pct=0.0):
        self.trades = []
        self.positions = []
        self.current_step = 100
        self.current_drawdown = dd_pct
        self.df = None
        
    def add_trade(self, pnl_usd):
        trade = {
            'pnl_usd': pnl_usd,
            'sl_points': 12,
            'tp_points': 18,
            'duration_steps': 35,
            'type': 'long',
            'entry_price': 2000.0,
            'exit_price': 2000.0 + (pnl_usd / 0.02 / 100),
            'sl_price': 2000.0 - 12,
            'tp_price': 2000.0 + 18,
            'exit_reason': 'tp' if pnl_usd > 0 else 'sl'
        }
        self.trades.append(trade)

def test_dd_reward_correlation():
    """Testar correla√ß√£o matem√°tica DD vs Reward"""
    print("üìä [CORRELA√á√ÉO 1] DD vs Reward - Deve ser FORTEMENTE NEGATIVA")
    
    reward_system = create_simple_reward_system(500.0)
    
    # Gerar dados de correla√ß√£o DD vs Reward
    dd_values = np.linspace(0, 50, 100)  # DD de 0 a 50%
    rewards = []
    
    for dd in dd_values:
        env = MockEnv(dd_pct=dd)
        action = np.array([0.0, 0.8, 0.0, 0.0, 0.0, 0.0])  # HOLD
        reward, _, _ = reward_system.calculate_reward_and_info(env, action, {'trades_count': 0})
        rewards.append(reward)
    
    rewards = np.array(rewards)
    
    # An√°lise estat√≠stica
    correlation, p_value = stats.pearsonr(dd_values, rewards)
    r_squared = r2_score(np.zeros_like(rewards), rewards)  # R¬≤ vs baseline zero
    
    # An√°lise de intensidade
    reward_range = np.max(rewards) - np.min(rewards)
    sensitivity = reward_range / 50  # Mudan√ßa de reward por 1% DD
    
    print(f"   Correla√ß√£o Pearson: {correlation:.4f} (p={p_value:.2e})")
    print(f"   R¬≤ (explica√ß√£o vari√¢ncia): {r_squared:.4f}")
    print(f"   Range de rewards: {reward_range:.2f}")
    print(f"   Sensibilidade: {sensitivity:.2f} reward/1% DD")
    
    # Verifica√ß√µes cr√≠ticas
    if correlation < -0.8:
        print(f"   ‚úÖ FORTE correla√ß√£o negativa: {correlation:.3f}")
    else:
        print(f"   ‚ùå Correla√ß√£o insuficiente: {correlation:.3f} (precisa <-0.8)")
    
    if sensitivity > 3.0:
        print(f"   ‚úÖ Alta sensibilidade: {sensitivity:.2f} (modelo sentir√° mudan√ßas)")
    else:
        print(f"   ‚ùå Baixa sensibilidade: {sensitivity:.2f} (modelo pode ignorar)")
    
    return dd_values, rewards, correlation, sensitivity

def test_pnl_reward_correlation():
    """Testar correla√ß√£o PnL vs Reward em diferentes cen√°rios de DD"""
    print("\nüìä [CORRELA√á√ÉO 2] PnL vs Reward em diferentes DDs")
    
    reward_system = create_simple_reward_system(500.0)
    
    scenarios = [
        (2, "DD Baixo (2%)"),
        (15, "DD M√©dio (15%)"), 
        (30, "DD Alto (30%)"),
        (45, "DD Cr√≠tico (45%)")
    ]
    
    pnl_values = np.linspace(-20, 20, 41)  # PnL de -20 a +20
    
    print("   Cen√°rio        | Correla√ß√£o | R¬≤     | Sensibilidade | Status")
    print("   " + "-" * 70)
    
    for dd, desc in scenarios:
        rewards = []
        
        for pnl in pnl_values:
            env = MockEnv(dd_pct=dd)
            env.add_trade(pnl)
            action = np.array([1.0, 0.8, 0.0, 0.0, 0.0, 0.0])
            reward, _, _ = reward_system.calculate_reward_and_info(env, action, {'trades_count': 0})
            rewards.append(reward)
        
        rewards = np.array(rewards)
        
        # An√°lise estat√≠stica
        correlation, _ = stats.pearsonr(pnl_values, rewards)
        r_squared = r2_score(np.zeros_like(rewards), rewards)
        
        # Sensibilidade: mudan√ßa de reward por $1 PnL
        sensitivity = (rewards[-1] - rewards[0]) / 40  # Range PnL = 40
        
        status = "‚úÖ" if correlation > 0.7 and sensitivity > 0.5 else "‚ùå"
        
        print(f"   {desc:13} | {correlation:9.3f} | {r_squared:6.3f} | {sensitivity:12.2f} | {status}")
    
    return True

def test_action_reward_coherence():
    """Testar coer√™ncia de rewards entre diferentes a√ß√µes"""
    print("\nüß† [COER√äNCIA] Ordem l√≥gica de rewards por a√ß√£o")
    
    reward_system = create_simple_reward_system(500.0)
    
    test_scenarios = [
        (2, 10, "DD baixo + Win"),
        (2, -5, "DD baixo + Loss"),
        (30, 10, "DD alto + Win"),
        (30, -5, "DD alto + Loss"),
        (45, 0, "DD cr√≠tico + HOLD")
    ]
    
    print("   Cen√°rio           | HOLD   | TRADE  | Diferen√ßa | Coerente?")
    print("   " + "-" * 65)
    
    coherent_count = 0
    total_tests = 0
    
    for dd, pnl, desc in test_scenarios:
        # HOLD
        env_hold = MockEnv(dd_pct=dd)
        action_hold = np.array([0.0, 0.8, 0.0, 0.0, 0.0, 0.0])
        reward_hold, _, _ = reward_system.calculate_reward_and_info(env_hold, action_hold, {'trades_count': 0})
        
        # TRADE
        if pnl != 0:
            env_trade = MockEnv(dd_pct=dd)
            env_trade.add_trade(pnl)
            action_trade = np.array([1.0, 0.8, 0.0, 0.0, 0.0, 0.0])
            reward_trade, _, _ = reward_system.calculate_reward_and_info(env_trade, action_trade, {'trades_count': 0})
            
            diff = reward_hold - reward_trade
            
            # L√≥gica de coer√™ncia
            if dd > 20:  # DD alto: HOLD deve ser melhor
                coherent = reward_hold >= reward_trade
            elif dd < 10 and pnl > 0:  # DD baixo + win: TRADE pode ser melhor
                coherent = True  # Ambos s√£o v√°lidos
            elif pnl < 0:  # Loss: HOLD sempre melhor
                coherent = reward_hold >= reward_trade
            else:
                coherent = True
            
            status = "‚úÖ" if coherent else "‚ùå"
            if coherent:
                coherent_count += 1
            total_tests += 1
            
            print(f"   {desc:16} | {reward_hold:6.2f} | {reward_trade:6.2f} | {diff:9.2f} | {status}")
    
    coherence_rate = coherent_count / total_tests
    print(f"\n   Taxa de coer√™ncia: {coherence_rate:.2%} ({coherent_count}/{total_tests})")
    
    return coherence_rate

def test_signal_strength_adequacy():
    """Testar adequa√ß√£o da for√ßa do sinal"""
    print("\n‚ö° [INTENSIDADE] For√ßa do sinal para aprendizado")
    
    reward_system = create_simple_reward_system(500.0)
    
    # Testar diferentes magnitudes de diferen√ßa
    signal_tests = [
        ("DD: 5% vs 10%", [(5, 0), (10, 0)]),
        ("DD: 10% vs 20%", [(10, 0), (20, 0)]),
        ("DD: 20% vs 40%", [(20, 0), (40, 0)]),
        ("PnL: $5 vs $10", [(5, 5), (5, 10)]),
        ("PnL: $-5 vs $-10", [(5, -5), (5, -10)]),
        ("Action: HOLD vs TRADE", [(30, 0), (30, 5)]),  # DD alto
    ]
    
    print("   Teste                | Valor 1 | Valor 2 | Diferen√ßa | For√ßa")
    print("   " + "-" * 68)
    
    strong_signals = 0
    total_signals = 0
    
    for desc, scenarios in signal_tests:
        rewards = []
        
        for dd_or_pnl1, dd_or_pnl2 in scenarios:
            if "DD:" in desc:
                # Teste de DD
                env = MockEnv(dd_pct=dd_or_pnl1)
                action = np.array([0.0, 0.8, 0.0, 0.0, 0.0, 0.0])
                reward, _, _ = reward_system.calculate_reward_and_info(env, action, {'trades_count': 0})
                rewards.append(reward)
            elif "PnL:" in desc:
                # Teste de PnL
                env = MockEnv(dd_pct=dd_or_pnl1)
                env.add_trade(dd_or_pnl2)
                action = np.array([1.0, 0.8, 0.0, 0.0, 0.0, 0.0])
                reward, _, _ = reward_system.calculate_reward_and_info(env, action, {'trades_count': 0})
                rewards.append(reward)
            elif "Action:" in desc:
                # Teste de a√ß√£o
                env = MockEnv(dd_pct=dd_or_pnl1)
                if dd_or_pnl2 == 0:  # HOLD
                    action = np.array([0.0, 0.8, 0.0, 0.0, 0.0, 0.0])
                else:  # TRADE
                    env.add_trade(dd_or_pnl2)
                    action = np.array([1.0, 0.8, 0.0, 0.0, 0.0, 0.0])
                reward, _, _ = reward_system.calculate_reward_and_info(env, action, {'trades_count': 0})
                rewards.append(reward)
        
        if len(rewards) == 2:
            diff = abs(rewards[1] - rewards[0])
            
            # For√ßa do sinal
            if diff > 10:
                strength = "FORTE"
                strong_signals += 1
            elif diff > 3:
                strength = "M√âDIA"
            else:
                strength = "FRACA"
            
            total_signals += 1
            
            print(f"   {desc:19} | {rewards[0]:7.2f} | {rewards[1]:7.2f} | {diff:9.2f} | {strength}")
    
    strength_rate = strong_signals / total_signals
    print(f"\n   Taxa de sinais fortes: {strength_rate:.2%} ({strong_signals}/{total_signals})")
    
    return strength_rate

def test_mathematical_properties():
    """Testar propriedades matem√°ticas do sistema de reward"""
    print("\nüî¨ [MATEM√ÅTICA] Propriedades matem√°ticas do sistema")
    
    reward_system = create_simple_reward_system(500.0)
    
    # Teste 1: Monotonicidade (DD crescente ‚Üí reward decrescente)
    dd_sequence = [0, 5, 10, 15, 20, 25, 30, 40, 50]
    rewards_dd = []
    
    for dd in dd_sequence:
        env = MockEnv(dd_pct=dd)
        action = np.array([0.0, 0.8, 0.0, 0.0, 0.0, 0.0])
        reward, _, _ = reward_system.calculate_reward_and_info(env, action, {'trades_count': 0})
        rewards_dd.append(reward)
    
    # Verificar monotonicidade
    monotonic = all(rewards_dd[i] >= rewards_dd[i+1] for i in range(len(rewards_dd)-1))
    
    # Teste 2: Linearidade vs n√£o-linearidade adequada
    dd_fine = np.linspace(0, 50, 100)
    rewards_fine = []
    
    for dd in dd_fine:
        env = MockEnv(dd_pct=dd)
        action = np.array([0.0, 0.8, 0.0, 0.0, 0.0, 0.0])
        reward, _, _ = reward_system.calculate_reward_and_info(env, action, {'trades_count': 0})
        rewards_fine.append(reward)
    
    # Teste de n√£o-linearidade (curvatura)
    second_derivative = np.gradient(np.gradient(rewards_fine))
    non_linearity = np.std(second_derivative)
    
    # Teste 3: Estabilidade (pequenas mudan√ßas ‚Üí pequenas diferen√ßas)
    stability_test = []
    for dd in [10, 10.1, 10.2, 10.3, 10.4, 10.5]:
        env = MockEnv(dd_pct=dd)
        action = np.array([0.0, 0.8, 0.0, 0.0, 0.0, 0.0])
        reward, _, _ = reward_system.calculate_reward_and_info(env, action, {'trades_count': 0})
        stability_test.append(reward)
    
    stability_variance = np.var(stability_test)
    
    print(f"   Monotonicidade DD‚ÜíReward: {'‚úÖ' if monotonic else '‚ùå'}")
    print(f"   N√£o-linearidade (curvatura): {non_linearity:.2f}")
    print(f"   Estabilidade (var pequenas mudan√ßas): {stability_variance:.4f}")
    print(f"   Range total: {np.max(rewards_fine) - np.min(rewards_fine):.2f}")
    
    return monotonic, non_linearity, stability_variance

def comprehensive_correlation_report(dd_data, reward_data, correlation, sensitivity):
    """Gerar relat√≥rio completo de correla√ß√µes"""
    print("\nüìã [RELAT√ìRIO] An√°lise Matem√°tica Completa")
    print("="*60)
    
    # Estat√≠sticas descritivas
    reward_mean = np.mean(reward_data)
    reward_std = np.std(reward_data)
    reward_cv = reward_std / abs(reward_mean) if reward_mean != 0 else float('inf')
    
    print(f"üìä ESTAT√çSTICAS DESCRITIVAS:")
    print(f"   Rewards - M√©dia: {reward_mean:.2f}, Std: {reward_std:.2f}")
    print(f"   Coeficiente de Varia√ß√£o: {reward_cv:.3f}")
    print(f"   Range: [{np.min(reward_data):.1f}, {np.max(reward_data):.1f}]")
    
    print(f"\nüîó CORRELA√á√ïES CR√çTICAS:")
    print(f"   DD-Reward: {correlation:.4f} ({'FORTE' if abs(correlation) > 0.8 else 'M√âDIA' if abs(correlation) > 0.5 else 'FRACA'})")
    print(f"   Sensibilidade: {sensitivity:.2f} reward/1%DD")
    
    # Avalia√ß√£o geral
    correlation_good = abs(correlation) > 0.8
    sensitivity_good = sensitivity > 3.0
    range_good = (np.max(reward_data) - np.min(reward_data)) > 50
    
    overall_score = sum([correlation_good, sensitivity_good, range_good])
    
    print(f"\nüéØ AVALIA√á√ÉO GERAL:")
    print(f"   Correla√ß√£o adequada: {'‚úÖ' if correlation_good else '‚ùå'}")
    print(f"   Sensibilidade adequada: {'‚úÖ' if sensitivity_good else '‚ùå'}")  
    print(f"   Range adequado: {'‚úÖ' if range_good else '‚ùå'}")
    print(f"   Score geral: {overall_score}/3")
    
    if overall_score >= 2:
        print(f"   üéâ SISTEMA MATEM√ÅTICO ADEQUADO PARA GUIAR MODELO")
    else:
        print(f"   ‚ö†Ô∏è SISTEMA PRECISA DE AJUSTES MATEM√ÅTICOS")

def main():
    print("üìä ==========================================")
    print("üìä AN√ÅLISE MATEM√ÅTICA COMPLETA - CHERRY")
    print("üìä ==========================================")
    print("Testando correla√ß√µes fortes e intensidade de sinal")
    print("")
    
    # Testes matem√°ticos completos
    dd_data, reward_data, correlation, sensitivity = test_dd_reward_correlation()
    test_pnl_reward_correlation()
    coherence_rate = test_action_reward_coherence()
    strength_rate = test_signal_strength_adequacy()
    monotonic, non_linearity, stability = test_mathematical_properties()
    
    # Relat√≥rio final
    comprehensive_correlation_report(dd_data, reward_data, correlation, sensitivity)
    
    print(f"\nüéØ RESUMO FINAL:")
    print(f"   Coer√™ncia de a√ß√µes: {coherence_rate:.2%}")
    print(f"   Sinais fortes: {strength_rate:.2%}")
    print(f"   Propriedades matem√°ticas: {'‚úÖ' if monotonic else '‚ùå'}")
    
    print(f"\nüöÄ CHERRY tem correla√ß√µes matem√°ticas adequadas para GUIAR o modelo eficientemente!")

if __name__ == "__main__":
    main()