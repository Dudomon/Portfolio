# üî¨ AGGREGATION GRADIENT ANALYSIS

## PROBLEMA CONFIRMADO
Mesmo ap√≥s redu√ß√£o dr√°stica de 26 ‚Üí 8 layers, gradientes continuam morrendo:
- Step 6000: 33.5% zeros ‚úÖ (melhoria inicial)
- Step 8000: 65.6% zeros ‚ùå (degradou)  
- Step 10000: 66.6% zeros ‚ùå (plateau)
- Step 12000: 66.7% zeros ‚ùå (sem melhoria)

## ROOT CAUSE: GLOBAL AVERAGE POOLING

### Problema Matem√°tico
```python
# ATUAL - Mata gradientes uniformemente
x_pooled = x.mean(dim=1)  # [batch, seq_len, d_model] ‚Üí [batch, d_model]
```

**Por que mata gradientes:**
1. Cada timestep recebe gradient `‚àÇL/‚àÇx_t = (1/seq_len) * ‚àÇL/‚àÇx_pooled`
2. Com seq_len=20: cada timestep recebe apenas 5% do gradient signal
3. **temporal_projection** precisa aprender de TODOS timesteps mas recebe signal dilu√≠do

### An√°lise do Fluxo
```
temporal_projection ‚Üí transformer ‚Üí MEAN POOLING ‚Üí aggregator ‚Üí output
     ‚Üë                                    |
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ gradient √ó 0.05 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## SOLU√á√ÉO: LEARNABLE AGGREGATION

### Op√ß√£o 1: Weighted Temporal Aggregation
```python
class LearnableTemporalAggregator(nn.Module):
    def __init__(self, seq_len, d_model):
        super().__init__()
        # Learnable weights para cada timestep
        self.temporal_weights = nn.Parameter(torch.ones(seq_len) / seq_len)
        self.gate = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # x: [batch, seq_len, d_model]
        # Normalizar weights (softmax)
        weights = F.softmax(self.temporal_weights, dim=0)
        
        # Weighted aggregation
        weighted_x = x * weights.unsqueeze(0).unsqueeze(-1)
        aggregated = weighted_x.sum(dim=1)
        
        # Gating mechanism
        gate_values = self.gate(aggregated)
        return aggregated * gate_values
```

### Op√ß√£o 2: Attention-based Aggregation
```python
class AttentionAggregator(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.query = nn.Linear(d_model, 1)
        self.scale = d_model ** -0.5
        
    def forward(self, x):
        # x: [batch, seq_len, d_model]
        # Compute attention scores
        scores = self.query(x).squeeze(-1)  # [batch, seq_len]
        weights = F.softmax(scores * self.scale, dim=-1)
        
        # Weighted sum
        aggregated = torch.einsum('bs,bsd->bd', weights, x)
        return aggregated
```

### Op√ß√£o 3: Learnable Pooling (MAIS SIMPLES)
```python
class LearnablePooling(nn.Module):
    def __init__(self, seq_len):
        super().__init__()
        # Start com uniform mas learnable
        self.weights = nn.Parameter(torch.ones(seq_len))
        
    def forward(self, x):
        # x: [batch, seq_len, d_model]
        w = F.softmax(self.weights, dim=0)
        # Weighted mean preserva gradients espec√≠ficos
        return torch.einsum('s,bsd->bd', w, x)
```

## BENEF√çCIOS ESPERADOS

1. **Gradient Flow**: Timesteps importantes recebem mais gradient
2. **Aprendizado**: Modelo aprende quais timesteps s√£o relevantes
3. **Preserva√ß√£o**: temporal_projection recebe signal forte dos timesteps cr√≠ticos

## IMPLEMENTA√á√ÉO PROPOSTA

1. Substituir `x.mean(dim=1)` por `LearnablePooling`
2. Inicializar com weights uniformes (n√£o quebrar funcionalidade)
3. Monitorar gradient flow para temporal_projection

## M√âTRICAS DE SUCESSO
- Gradient zeros < 10% em temporal_projection 
- Gradient flow mais forte para timesteps recentes
- Converg√™ncia mais r√°pida do modelo