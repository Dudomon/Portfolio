#!/usr/bin/env python3
"""
üîç DEBUG: INVESTIGAR CORRELA√á√ÉO REWARD vs PERFORMANCE
An√°lise profunda do que est√° causando baixa correla√ß√£o
"""
import sys
import os
sys.path.append("D:/Projeto")

import numpy as np
import pandas as pd

def debug_reward_correlation():
    """Debug espec√≠fico da correla√ß√£o reward vs performance"""
    
    print("üîç DEBUG: CORRELA√á√ÉO REWARD vs PERFORMANCE")
    print("=" * 60)
    
    try:
        from trading_framework.rewards.reward_daytrade_v2 import BalancedDayTradingRewardCalculator
        from daytrader import TradingEnv
        
        # Dataset pequeno para an√°lise detalhada
        dataset_path = "D:/Projeto/data/GC_YAHOO_ENHANCED_V3_BALANCED_20250804_192226.csv"
        df = pd.read_csv(dataset_path)
        
        # Processar dataset
        if 'time' in df.columns:
            df['timestamp'] = pd.to_datetime(df['time'])
            df.set_index('timestamp', inplace=True)
            df.drop('time', axis=1, inplace=True)
        
        df = df.rename(columns={
            'open': 'open_5m',
            'high': 'high_5m',
            'low': 'low_5m', 
            'close': 'close_5m',
            'tick_volume': 'volume_5m'
        })
        
        # Usar subset muito pequeno para debug
        test_df = df.head(1000).copy()
        print(f"üìä Dataset debug: {len(test_df):,} barras")
        
        # Configurar ambiente
        trading_params = {
            'base_lot_size': 0.02,
            'max_lot_size': 0.03,
            'initial_balance': 500.0,
            'target_trades_per_day': 18,
            'stop_loss_range': (2.0, 8.0),
            'take_profit_range': (3.0, 15.0)
        }
        
        env = TradingEnv(
            test_df,
            window_size=20,
            is_training=True,
            initial_balance=500.0,
            trading_params=trading_params
        )
        
        print("‚úÖ Ambiente debug criado")
        
        # FOR√áAR TRADES ESPEC√çFICOS para testar reward correlation
        print("\nüéØ TESTE 1: SIMULA√á√ÉO DE TRADES ESPEC√çFICOS")
        print("-" * 50)
        
        obs = env.reset()
        
        # Dados para an√°lise
        rewards_log = []
        portfolio_log = []
        trades_log = []
        actions_log = []
        
        portfolio_log.append(env.portfolio_value)
        
        # Simular sequ√™ncia espec√≠fica: alguns holds, depois trades for√ßados
        test_actions = [
            # Holds iniciais
            *[np.array([0, 0.5, 0, 0.5, 0, 0, 0, 0, 0, 0, 0], dtype=np.float32) for _ in range(10)],
            
            # Trades for√ßados com qualidade alta
            np.array([1, 0.9, 0.5, 0.8, 0.2, 0, 0, 0, 0, 0, 0], dtype=np.float32),  # LONG alto
            np.array([0, 0.5, 0, 0.5, 0, 0, 0, 0, 0, 0, 0], dtype=np.float32),      # HOLD
            np.array([2, 0.8, -0.3, 0.7, -0.1, 0, 0, 0, 0, 0, 0], dtype=np.float32), # SHORT alto
            
            # Mais holds
            *[np.array([0, 0.5, 0, 0.5, 0, 0, 0, 0, 0, 0, 0], dtype=np.float32) for _ in range(20)],
            
            # Mais trades
            np.array([1, 0.7, 0.3, 0.6, 0.1, 0, 0, 0, 0, 0, 0], dtype=np.float32),  # LONG m√©dio
            np.array([2, 0.6, -0.4, 0.8, -0.2, 0, 0, 0, 0, 0, 0], dtype=np.float32), # SHORT m√©dio
            
            # Final com holds
            *[np.array([0, 0.5, 0, 0.5, 0, 0, 0, 0, 0, 0, 0], dtype=np.float32) for _ in range(50)]
        ]
        
        print(f"   üìä Executando {len(test_actions)} a√ß√µes espec√≠ficas...")
        
        for i, action in enumerate(test_actions):
            if i >= 100:  # Limite para n√£o exceder dataset
                break
                
            old_portfolio = env.portfolio_value
            old_trades = len(getattr(env, 'trades', []))
            
            obs, reward, done, info = env.step(action)
            
            new_portfolio = env.portfolio_value
            new_trades = len(getattr(env, 'trades', []))
            
            # Log detalhado
            rewards_log.append(reward)
            portfolio_log.append(new_portfolio)
            
            action_type = "HOLD" if action[0] == 0 else ("LONG" if action[0] == 1 else "SHORT")
            quality = action[1]
            
            actions_log.append({
                'step': i,
                'action_type': action_type,
                'quality': quality,
                'reward': reward,
                'portfolio_before': old_portfolio,
                'portfolio_after': new_portfolio,
                'portfolio_change': new_portfolio - old_portfolio,
                'new_trades': new_trades - old_trades
            })
            
            # Log trades espec√≠ficos
            if new_trades > old_trades:
                recent_trades = env.trades[-(new_trades - old_trades):]
                for trade in recent_trades:
                    trade_pnl = trade.get('pnl_usd', trade.get('pnl', 0))
                    trades_log.append({
                        'step': i,
                        'pnl': trade_pnl,
                        'reward': reward
                    })
                    print(f"      [TRADE] Step {i}: PnL=${trade_pnl:.3f}, Reward={reward:.4f}")
            
            if done:
                break
        
        # AN√ÅLISE DETALHADA
        print(f"\nüìä AN√ÅLISE DETALHADA ({len(rewards_log)} steps)")
        print("-" * 50)
        
        rewards = np.array(rewards_log)
        portfolios = np.array(portfolio_log)
        
        # Portfolio changes
        portfolio_changes = np.diff(portfolios, prepend=portfolios[0])
        
        print(f"   üìà Rewards: mean={rewards.mean():.4f}, std={rewards.std():.4f}")
        print(f"   üí∞ Portfolio changes: mean=${portfolio_changes.mean():.3f}, std=${portfolio_changes.std():.3f}")
        
        # Correla√ß√£o detalhada
        if len(rewards) > 1 and len(portfolio_changes) > 1:
            correlation = np.corrcoef(rewards[1:], portfolio_changes[1:])[0, 1]
            print(f"   üîó Correla√ß√£o Reward vs Portfolio Change: {correlation:.4f}")
            
            # An√°lise s√≥ dos trades
            if trades_log:
                trade_rewards = [t['reward'] for t in trades_log]
                trade_pnls = [t['pnl'] for t in trades_log]
                
                if len(trade_rewards) > 1:
                    trade_correlation = np.corrcoef(trade_rewards, trade_pnls)[0, 1]
                    print(f"   üíº Correla√ß√£o Trade Rewards vs Trade PnL: {trade_correlation:.4f}")
                    
                    print(f"\nüìã AN√ÅLISE DE TRADES INDIVIDUAIS:")
                    for i, trade in enumerate(trades_log):
                        expected_reward = trade['pnl'] / 500.0 * 200.0  # C√°lculo esperado
                        print(f"      Trade {i+1}: PnL=${trade['pnl']:.3f}, Reward={trade['reward']:.4f}, Expected={expected_reward:.4f}")
            
        # An√°lise por tipo de a√ß√£o
        print(f"\nüéÆ AN√ÅLISE POR TIPO DE A√á√ÉO:")
        
        hold_actions = [a for a in actions_log if a['action_type'] == 'HOLD']
        long_actions = [a for a in actions_log if a['action_type'] == 'LONG'] 
        short_actions = [a for a in actions_log if a['action_type'] == 'SHORT']
        
        print(f"   ‚ö™ HOLD: {len(hold_actions)} actions")
        if hold_actions:
            hold_rewards = [a['reward'] for a in hold_actions]
            hold_portfolio_changes = [a['portfolio_change'] for a in hold_actions]
            print(f"      Reward m√©dio: {np.mean(hold_rewards):.4f}")
            print(f"      Portfolio change m√©dio: ${np.mean(hold_portfolio_changes):.3f}")
        
        print(f"   üü¢ LONG: {len(long_actions)} actions")
        if long_actions:
            long_rewards = [a['reward'] for a in long_actions]
            long_portfolio_changes = [a['portfolio_change'] for a in long_actions]
            print(f"      Reward m√©dio: {np.mean(long_rewards):.4f}")
            print(f"      Portfolio change m√©dio: ${np.mean(long_portfolio_changes):.3f}")
            
        print(f"   üî¥ SHORT: {len(short_actions)} actions")
        if short_actions:
            short_rewards = [a['reward'] for a in short_actions]
            short_portfolio_changes = [a['portfolio_change'] for a in short_actions]
            print(f"      Reward m√©dio: {np.mean(short_rewards):.4f}")
            print(f"      Portfolio change m√©dio: ${np.mean(short_portfolio_changes):.3f}")
        
        # DIAGN√ìSTICO FINAL
        print(f"\nüî¨ DIAGN√ìSTICO:")
        print("-" * 50)
        
        if correlation < 0.1:
            print("   ‚ùå PROBLEMA: Correla√ß√£o ainda muito baixa")
            print("   üí° POSS√çVEIS CAUSAS:")
            print("      1. Alive bonus ainda dominando")
            print("      2. Componentes n√£o-PnL mascarando signal")
            print("      3. Escala ainda inadequada")
            print("      4. VecNormalize destruindo correla√ß√£o")
        elif correlation < 0.3:
            print("   ‚ö†Ô∏è MELHORIA: Correla√ß√£o baixa mas positiva")
            print("   üí° PR√ìXIMOS PASSOS: Aumentar ainda mais peso do PnL")
        else:
            print("   ‚úÖ SUCESSO: Correla√ß√£o boa!")
            
        return correlation > 0.3
        
    except Exception as e:
        print(f"‚ùå ERRO: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = debug_reward_correlation()
    print(f"\n{'‚úÖ CORRELA√á√ÉO BOA' if success else '‚ùå CORRELA√á√ÉO AINDA BAIXA'}")