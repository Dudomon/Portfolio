"""
üî¨ TESTE FINAL: V3 Brutal Balance ap√≥s SL/TP Rework

Verifica se o reward system mant√©m balance perfeito ap√≥s as mudan√ßas:
- HARD CAP de TP em 25 pontos
- Gaming penalties
- TP realism bonus
- Shaping weight 70/30
"""

import numpy as np
import sys
from pathlib import Path

# Adicionar path do projeto
sys.path.append(str(Path(__file__).parent))

from trading_framework.rewards.reward_daytrade_v3_brutal import BrutalMoneyReward

class MockEnv:
    """Mock environment para testes"""
    def __init__(self):
        self.current_step = 0
        self.positions = []
        self.trades = []
        self.portfolio_value = 1000.0
        self.realized_balance = 1000.0
        self.peak_portfolio = 1000.0
        self.current_atr = 15.0

        # Mock dataframe com features
        import pandas as pd
        self.df = pd.DataFrame({
            'support_resistance': [0.5] * 100,  # SL zone quality
            'breakout_strength': [0.5] * 100,   # TP target quality
            'market_structure': [0.4] * 100     # Volatility spike
        })

def test_balance_equal_opposite():
    """
    TESTE CR√çTICO: Gain vs Loss Balance

    LONG gain +$10 deve ter reward OPOSTO de SHORT loss -$10
    """
    print("\n" + "="*70)
    print("üî¨ TESTE 1: BALANCE GAIN/LOSS (LONG vs SHORT)")
    print("="*70)

    reward_system = BrutalMoneyReward(initial_balance=1000.0)
    env = MockEnv()

    # Cen√°rio 1: LONG com gain de $10
    env.realized_balance = 1010.0
    env.portfolio_value = 1010.0
    env.peak_portfolio = 1010.0

    reward_long_gain, info_long, _ = reward_system.calculate_reward_and_info(
        env, action=np.array([0.0, 0.5, 0.0, 0.0]), old_state={}
    )

    # Cen√°rio 2: SHORT com loss de -$10
    env.realized_balance = 990.0
    env.portfolio_value = 990.0
    env.peak_portfolio = 1000.0

    reward_short_loss, info_short, _ = reward_system.calculate_reward_and_info(
        env, action=np.array([0.0, 0.5, 0.0, 0.0]), old_state={}
    )

    # Verificar balance
    ratio = abs(reward_long_gain / reward_short_loss) if reward_short_loss != 0 else 0

    print(f"\nüìä RESULTADOS:")
    print(f"  LONG +$10 ‚Üí Reward: {reward_long_gain:+.6f}")
    print(f"  SHORT -$10 ‚Üí Reward: {reward_short_loss:+.6f}")
    print(f"  Ratio: {ratio:.6f}")

    if 0.95 <= ratio <= 1.05:
        print(f"  ‚úÖ BALANCEADO (ratio {ratio:.4f} ‚âà 1.0)")
        return True
    else:
        print(f"  ‚ùå DESBALANCEADO (ratio {ratio:.4f} != 1.0)")
        return False


def test_gaming_penalty_impact():
    """
    Testa se gaming penalty est√° funcionando
    """
    print("\n" + "="*70)
    print("üî¨ TESTE 2: GAMING PENALTY (SL MIN + TP MAX)")
    print("="*70)

    reward_system = BrutalMoneyReward(initial_balance=1000.0)
    env = MockEnv()

    # Posi√ß√£o com SL m√≠nimo + TP m√°ximo (GAMING)
    env.positions = [{
        'entry_price': 2000.0,
        'sl': 1990.0,  # 10 pontos (m√≠nimo)
        'tp': 2025.0,  # 25 pontos (m√°ximo)
        'type': 'long',
        'duration': 20  # 20 steps
    }]

    reward_system.step_counter = 25  # Trigger cache
    reward, info, _ = reward_system.calculate_reward_and_info(
        env, action=np.array([0.0, 0.5, 0.0, 0.0]), old_state={}
    )

    gaming_penalty = info.get('sltp_gaming_penalty', 0.0)

    print(f"\nüìä RESULTADOS:")
    print(f"  Posi√ß√£o: SL 10pts + TP 25pts (GAMING)")
    print(f"  Gaming Penalty: {gaming_penalty:+.6f}")

    if gaming_penalty < -0.1:
        print(f"  ‚úÖ PENALIDADE ATIVA (penalty < -0.1)")
        return True
    else:
        print(f"  ‚ùå PENALIDADE FRACA (penalty {gaming_penalty:.4f})")
        return False


def test_tp_realism_bonus():
    """
    Testa se TP realism bonus est√° funcionando
    """
    print("\n" + "="*70)
    print("üî¨ TESTE 3: TP REALISM BONUS")
    print("="*70)

    reward_system = BrutalMoneyReward(initial_balance=1000.0)
    env = MockEnv()

    # Feature indica resist√™ncia pr√≥xima
    env.df['breakout_strength'] = [0.8] * 100  # TP target quality ALTO

    # Posi√ß√£o com TP pr√≥ximo (REALISTA)
    env.positions = [{
        'entry_price': 2000.0,
        'sl': 1985.0,  # 15 pontos
        'tp': 2018.0,  # 18 pontos (1.2 ATR, realista)
        'type': 'long',
        'duration': 10
    }]

    reward_system.step_counter = 25  # Trigger cache
    reward, info, _ = reward_system.calculate_reward_and_info(
        env, action=np.array([0.0, 0.5, 0.0, 0.0]), old_state={}
    )

    tp_realism = info.get('tp_realism_bonus', 0.0)

    print(f"\nüìä RESULTADOS:")
    print(f"  TP Target Quality: 0.8 (resist√™ncia pr√≥xima)")
    print(f"  TP: 18 pontos (1.2 ATR)")
    print(f"  TP Realism Bonus: {tp_realism:+.6f}")

    if tp_realism > 0.02:
        print(f"  ‚úÖ BONUS ATIVO (bonus > 0.02)")
        return True
    else:
        print(f"  ‚ö†Ô∏è  BONUS BAIXO (bonus {tp_realism:.4f})")
        return True  # Ainda OK, n√£o √© erro


def test_shaping_weight():
    """
    Testa se shaping weight est√° em 30% (vs 70% PnL)
    """
    print("\n" + "="*70)
    print("üî¨ TESTE 4: SHAPING WEIGHT (70/30)")
    print("="*70)

    reward_system = BrutalMoneyReward(initial_balance=1000.0)
    env = MockEnv()

    # PnL significativo
    env.realized_balance = 1050.0
    env.portfolio_value = 1050.0
    env.peak_portfolio = 1050.0

    reward, info, _ = reward_system.calculate_reward_and_info(
        env, action=np.array([0.0, 0.5, 0.0, 0.0]), old_state={}
    )

    pnl_component = info.get('pnl_component', 0.0)
    shaping_component = info.get('shaping_component', 0.0)

    total = abs(pnl_component) + abs(shaping_component)
    if total > 0:
        pnl_pct = abs(pnl_component) / total * 100
        shaping_pct = abs(shaping_component) / total * 100
    else:
        pnl_pct = shaping_pct = 0

    print(f"\nüìä RESULTADOS:")
    print(f"  PnL Component: {pnl_component:+.6f} ({pnl_pct:.1f}%)")
    print(f"  Shaping Component: {shaping_component:+.6f} ({shaping_pct:.1f}%)")

    if 65 <= pnl_pct <= 75:
        print(f"  ‚úÖ DISTRIBUI√á√ÉO CORRETA (70/30)")
        return True
    else:
        print(f"  ‚ùå DISTRIBUI√á√ÉO INCORRETA (deveria ser 70/30)")
        return False


def main():
    """Executar todos os testes"""
    print("\n" + "="*70)
    print("üî¨ TESTE DE BALANCEAMENTO V3 BRUTAL - P√ìS SL/TP REWORK")
    print("="*70)

    tests = [
        ("Balance Gain/Loss", test_balance_equal_opposite),
        ("Gaming Penalty", test_gaming_penalty_impact),
        ("TP Realism Bonus", test_tp_realism_bonus),
        ("Shaping Weight 70/30", test_shaping_weight),
    ]

    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"\n‚ùå ERRO no teste '{test_name}': {e}")
            results.append((test_name, False))

    # Resumo final
    print("\n" + "="*70)
    print("üìä RESUMO FINAL")
    print("="*70)

    for test_name, passed in results:
        status = "‚úÖ PASSOU" if passed else "‚ùå FALHOU"
        print(f"  {status} - {test_name}")

    passed_count = sum(1 for _, p in results if p)
    total_count = len(results)

    print(f"\nTotal: {passed_count}/{total_count} testes passaram")

    if passed_count == total_count:
        print("\nüéâ TODOS OS TESTES PASSARAM! V3 Brutal est√° balanceado.")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {total_count - passed_count} teste(s) falharam.")
        return 1


if __name__ == "__main__":
    exit(main())
