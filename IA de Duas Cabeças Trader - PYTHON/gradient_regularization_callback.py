#!/usr/bin/env python3
"""
üîß CALLBACK DE REGULARIZA√á√ÉO DE GRADIENTES
Integra o sistema de regulariza√ß√£o no treinamento PPO
"""

import numpy as np
import torch
from stable_baselines3.common.callbacks import BaseCallback
from typing import Dict, Any
from gradient_regularization import GradientRegularizer, ActivationHook


class GradientRegularizationCallback(BaseCallback):
    """Callback para aplicar regulariza√ß√£o de gradientes durante treinamento"""
    
    def __init__(self, 
                 regularizer: GradientRegularizer,
                 apply_freq: int = 1,  # Aplicar a cada step
                 health_check_freq: int = 1000,
                 verbose: int = 0):
        super().__init__(verbose)
        self.regularizer = regularizer
        self.apply_freq = apply_freq
        self.health_check_freq = health_check_freq
        self.step_count = 0
        
        # Hook para ativa√ß√µes
        self.activation_hook = ActivationHook()
        self.hooks_registered = False
        
    def _on_training_start(self) -> None:
        """Executado no in√≠cio do treinamento"""
        try:
            if hasattr(self.model, 'policy') and not self.hooks_registered:
                self.activation_hook.register_hooks(self.model.policy)
                self.hooks_registered = True
                print("üîß Hooks de ativa√ß√£o registrados na policy")
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao registrar hooks: {e}")
    
    def _on_step(self) -> bool:
        """Executado a cada step do treinamento"""
        self.step_count += 1
        
        try:
            # 1. Aplicar regulariza√ß√£o de gradientes a cada step
            if self.step_count % self.apply_freq == 0:
                if hasattr(self.model, 'policy'):
                    reg_stats = self.regularizer.apply_gradient_regularization(self.model.policy)
                    
                    # Log apenas se aplicou regulariza√ß√£o
                    if reg_stats.get('regularization_applied', False) and self.verbose > 0:
                        print(f"üîß Step {self.num_timesteps}: Regulariza√ß√£o aplicada")
                        print(f"   Dead gradients: {reg_stats['dead_gradients']}/{reg_stats['total_params']}")
            
            # 2. Health check peri√≥dico
            if self.step_count % self.health_check_freq == 0:
                self._perform_health_check()
                
        except Exception as e:
            if self.verbose > 0:
                print(f"‚ö†Ô∏è Erro na regulariza√ß√£o: {e}")
        
        return True
    
    def _perform_health_check(self):
        """Realizar verifica√ß√£o de sa√∫de do modelo"""
        try:
            if hasattr(self.model, 'policy'):
                health_stats = self.regularizer.check_model_health(self.model.policy)
                
                # Log health check
                if self.verbose > 0:
                    print(f"\nüîç HEALTH CHECK - Step {self.num_timesteps}")
                    print(f"   Health Score: {health_stats['health_score']:.3f}")
                    print(f"   Zero params: {health_stats['zero_parameters']}/{health_stats['total_parameters']} ({100*health_stats['zero_parameters']/max(health_stats['total_parameters'],1):.1f}%)")
                    print(f"   Parameter norm: {health_stats['parameter_norm']:.4f}")
                
                # Alerta se sa√∫de baixa
                if health_stats['health_score'] < 0.8:
                    print(f"‚ö†Ô∏è ALERTA: Sa√∫de do modelo baixa ({health_stats['health_score']:.3f})")
                    
                    # Aplicar corre√ß√£o mais agressiva
                    if health_stats['health_score'] < 0.5:
                        self._apply_emergency_regularization()
                        
        except Exception as e:
            print(f"‚ö†Ô∏è Erro no health check: {e}")
    
    def _apply_emergency_regularization(self):
        """Aplicar regulariza√ß√£o emergencial para modelo em estado cr√≠tico"""
        try:
            print("üö® APLICANDO REGULARIZA√á√ÉO EMERGENCIAL")
            
            if hasattr(self.model, 'policy'):
                reinitialized_count = 0
                # Re-inicializar par√¢metros com muitos zeros
                for name, param in self.model.policy.named_parameters():
                    if param.data is not None:
                        zero_ratio = (torch.abs(param.data) < 1e-8).float().mean().item()
                        
                        if zero_ratio > 0.3:  # üî• CORRE√á√ÉO: >30% zeros j√° √© cr√≠tico
                            print(f"üîß Re-inicializando {name} (zero ratio: {zero_ratio:.1%})")
                            reinitialized_count += 1
                            
                            if 'bias' in name:
                                # üî• CORRE√á√ÉO: Bias mais forte para quebrar simetria
                                if 'attention' in name or 'self_attn' in name:
                                    torch.nn.init.normal_(param.data, mean=0.0, std=0.02)
                                else:
                                    torch.nn.init.uniform_(param.data, -0.05, 0.05)
                            elif 'weight' in name:
                                # üî• CORRE√á√ÉO: Weight initialization mais robusta
                                if param.data.dim() >= 2:
                                    if 'attention' in name or 'transformer' in name:
                                        torch.nn.init.xavier_normal_(param.data, gain=1.0)
                                    else:
                                        torch.nn.init.kaiming_normal_(param.data, mode='fan_in', nonlinearity='relu')
                                else:
                                    torch.nn.init.normal_(param.data, 0.0, 0.05)
                                    
            print(f"‚úÖ Regulariza√ß√£o emergencial aplicada - {reinitialized_count} par√¢metros re-inicializados")
            
        except Exception as e:
            print(f"‚ùå Erro na regulariza√ß√£o emergencial: {e}")
    
    def _on_training_end(self) -> None:
        """Executado no final do treinamento"""
        try:
            # Remover hooks
            if self.hooks_registered:
                self.activation_hook.remove_hooks()
                print("üîß Hooks de ativa√ß√£o removidos")
            
            # Health check final
            if hasattr(self.model, 'policy'):
                final_health = self.regularizer.check_model_health(self.model.policy)
                print(f"\nüèÅ HEALTH CHECK FINAL:")
                print(f"   Health Score: {final_health['health_score']:.3f}")
                print(f"   Zero params: {100*final_health['zero_parameters']/max(final_health['total_parameters'],1):.1f}%")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao finalizar regulariza√ß√£o: {e}")


def create_gradient_regularization_callback(
    regularizer,
    apply_freq: int = 1,
    health_check_freq: int = 1000,
    verbose: int = 0
) -> GradientRegularizationCallback:
    """Factory function para criar callback de regulariza√ß√£o"""
    return GradientRegularizationCallback(
        regularizer=regularizer,
        apply_freq=apply_freq,
        health_check_freq=health_check_freq,
        verbose=verbose
    )

if __name__ == "__main__":
    print("üîß Gradient Regularization Callback - Integra regulariza√ß√£o no treinamento PPO")