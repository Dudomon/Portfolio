"""
游댠 TWOHHEAD POLICY ORIGINAL RESTAURADA - ARQUITETURA ROBUSTA COMPLETA

Pol칤tica customizada extremamente robusta para PPO trading.
- ARQUITETURA ORIGINAL: Muito mais par칙metros e capacidade de aprendizado
- LSTM ROBUSTO: 2 camadas, hidden_size=128, dropout=0.2  
- MLP ROBUSTO: Camadas densas maiores (512->256->128->64)
- ATTENTION MECHANISM: Multi-head attention para capturar depend칡ncias
- RESIDUAL CONNECTIONS: Skip connections para gradientes est치veis
- LAYER NORMALIZATION: Normaliza칞칚o em todas as camadas
- INICIALIZA칂츾O XAVIER: Inicializa칞칚o robusta dos pesos
"""

import torch
import torch.nn as nn
import numpy as np
from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor, MlpExtractor
from stable_baselines3.common.distributions import CategoricalDistribution
from typing import Dict, Any, Optional, Tuple, Union
import gym

class TwoHeadPolicy(RecurrentActorCriticPolicy):
    """
    游꿢 TWO HEAD POLICY H칈BRIDA EST츼VEL - MELHOR DOS DOIS MUNDOS
    
    Arquitetura h칤brida que combina:
    - 1 camada LSTM robusta (128 hidden) para mem칩ria longa
    - 1 camada GRU est치vel para estabilidade
    - Dropout adaptativo para evitar overfitting
    - Layer normalization para gradientes est치veis
    """
    
    def __init__(self, observation_space, action_space, lr_schedule, 
                 use_sde=False, log_std_init=0.0, full_std=True, 
                 sde_net_arch=None, use_expln=False, squash_output=False,
                 features_extractor_class=None, features_extractor_kwargs=None,
                 normalize_images=True, lstm_hidden_size=128, n_lstm_layers=1, **kwargs):
        
        # 游꿢 CONFIGURA칂칏ES H칈BRIDAS EST츼VEIS
        self.lstm_hidden_size = lstm_hidden_size  # 128 (robusto mas est치vel)
        self.n_lstm_layers = n_lstm_layers  # 1 (est치vel)
        
        # 游꿢 FEATURES EXTRACTOR OTIMIZADO
        if features_extractor_class is None:
            from ..extractors.transformer_extractor import TradingTransformerFeatureExtractor
            features_extractor_class = TradingTransformerFeatureExtractor
            
        if features_extractor_kwargs is None:
            features_extractor_kwargs = {'features_dim': 128}  # Restaurado para 128
        
        # 游댠 CONFIGURA칂칏ES H칈BRIDAS ROBUSTAS
        self.features_dim = 128  # SEMPRE 128 para TransformerFeatureExtractor
        self.attention_heads = 4  # Reduzido para estabilidade
        
        # 游댠 ARQUITETURA H칈BRIDA - EQUILIBRADA
        if 'net_arch' not in kwargs:
            kwargs['net_arch'] = [dict(pi=[256, 128, 64], vf=[256, 128, 64])]  # Reduzido mas robusto
        
        # 游댠 CONFIGURA칂칏ES LSTM H칈BRIDAS EST츼VEIS
        print(f"游꿢 TwoHeadPolicy H칈BRIDA EST츼VEL: features_dim={self.features_dim}, lstm_hidden={self.lstm_hidden_size}, n_layers={self.n_lstm_layers}")
        
        super().__init__(
            observation_space, action_space, lr_schedule,
            use_sde=use_sde, log_std_init=log_std_init, full_std=full_std,
            sde_net_arch=sde_net_arch, use_expln=use_expln, squash_output=squash_output,
            features_extractor_class=features_extractor_class,
            features_extractor_kwargs=features_extractor_kwargs,
            normalize_images=normalize_images,
            lstm_hidden_size=self.lstm_hidden_size,
            n_lstm_layers=self.n_lstm_layers,
            **kwargs
        )
        
        # 游댠 COMPONENTES H칈BRIDOS EST츼VEIS
        self._build_hybrid_components()
        
        # 游댠 INICIALIZA칂츾O EST츼VEL
        self._initialize_stable_weights()
        
        # Debug info
        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"游꿢 TwoHeadPolicy H칈BRIDA EST츼VEL: {total_params:,} par칙metros")
    
    def _build_hybrid_components(self):
        """游꿢 CONSTR칍I COMPONENTES H칈BRIDOS EST츼VEIS"""
        
        # 游꿢 GRU EST츼VEL PARA COMPLEMENTAR LSTM
        self.gru_layer = nn.GRU(
            input_size=self.lstm_hidden_size,
            hidden_size=self.lstm_hidden_size,
            num_layers=1,
            batch_first=True,
            dropout=0.0  # 游댠 CORRE칂츾O: Sem dropout para 1 camada
        )
        
        # 游꿢 LAYER NORMALIZATION PARA ESTABILIDADE
        self.lstm_norm = nn.LayerNorm(self.lstm_hidden_size)
        self.gru_norm = nn.LayerNorm(self.lstm_hidden_size)
        
        # 游꿢 ATTENTION EST츼VEL (REDUZIDO)
        self.attention = nn.MultiheadAttention(
            embed_dim=self.lstm_hidden_size,
            num_heads=self.attention_heads,  # 4 heads (reduzido)
            dropout=0.05,  # Dropout baixo
            batch_first=True
        )
        
        # 游꿢 FEATURE FUSION EST츼VEL
        fusion_input = self.features_dim + self.lstm_hidden_size * 2  # LSTM + GRU
        self.feature_fusion = nn.Sequential(
            nn.Linear(fusion_input, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.1),  # Dropout baixo
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.05)  # Dropout muito baixo
        )
        
        # 游꿢 RESIDUAL CONNECTIONS EST츼VEIS
        self.residual_policy = nn.Linear(128, 64)
        self.residual_value = nn.Linear(128, 64)
        
        print(f"游꿢 Componentes h칤bridos: LSTM({self.lstm_hidden_size}) + GRU({self.lstm_hidden_size}) + Attention({self.attention_heads} heads)")
    
    def _initialize_stable_weights(self):
        """游꿢 INICIALIZA칂츾O EST츼VEL PARA EVITAR EXPLOS츾O DE GRADIENTES"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # Xavier uniform com escala reduzida
                nn.init.xavier_uniform_(module.weight, gain=0.5)  # Gain reduzido
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.LayerNorm):
                nn.init.constant_(module.bias, 0)
                nn.init.constant_(module.weight, 1.0)
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.xavier_uniform_(param.data, gain=0.5)  # Gain reduzido
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(param.data, gain=0.5)  # Gain reduzido
                    elif 'bias' in name:
                        param.data.fill_(0)
                        # Forget gate bias = 1 (est치vel)
                        n = param.size(0)
                        param.data[n//4:n//2].fill_(1)
            elif isinstance(module, nn.GRU):
                for name, param in module.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.xavier_uniform_(param.data, gain=0.5)
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(param.data, gain=0.5)
                    elif 'bias' in name:
                        param.data.fill_(0)
        
        print(f"游꿢 Inicializa칞칚o est치vel aplicada (gain=0.5)")

def create_two_head_policy(**kwargs):
    """游댠 FACTORY FUNCTION PARA CRIAR TWOHHEAD POLICY ORIGINAL"""
    return TwoHeadPolicy


def get_default_policy_kwargs():
    """游꿢 CONFIGURA칂칏ES PADR츾O PARA TWOHHEAD POLICY H칈BRIDA EST츼VEL"""
    return {
        'lstm_hidden_size': 128,  # 游꿢 H칈BRIDO: 128 (robusto mas est치vel)
        'n_lstm_layers': 1,       # 游꿢 H칈BRIDO: 1 camada (est치vel)
        'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])],  # 游꿢 H칈BRIDO: Equilibrado
    } 