"""
üöÄ TwoHeadV5Intelligent48h - Policy com Entry Head Ultra-Especializada
Baseada na V4 com Entry Head que aproveita TODAS as melhorias inteligentes

MELHORIAS V5:
‚úÖ Entry Head Ultra-Especializada (aproveita 100% da V4)
‚úÖ 6 Gates especializados (temporal, validation, risk, market, quality, confidence)
‚úÖ 10 Scores especializados para m√°xima seletividade
‚úÖ Market Fatigue Detector integrado
‚úÖ Quality Filters especializados
‚úÖ Thresholds adaptativos
‚úÖ SEM cooldown temporal (removido conforme solicitado)
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Any, Dict, List, Optional, Type, Union
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
# Fallback para PyTorchObs
try:
    from stable_baselines3.common.type_aliases import PyTorchObs
except ImportError:
    import torch
    PyTorchObs = torch.Tensor
# Imports corretos para RecurrentPPO
try:
    from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy
    from sb3_contrib.common.recurrent.type_aliases import RNNStates
except ImportError:
    from stable_baselines3.common.policies import RecurrentActorCriticPolicy
    from stable_baselines3.common.recurrent_policies import RNNStates

from stable_baselines3.common.utils import obs_as_tensor
from trading_framework.extractors.transformer_extractor import TradingTransformerFeatureExtractor

# Strategic Fusion Layer - Importar da V6
try:
    from trading_framework.policies.two_head_v6_intelligent_48h import (
        ConflictResolutionCore,
        TemporalCoordinationCore, 
        AdaptiveLearningCore,
        StrategicFusionLayer
    )
    STRATEGIC_FUSION_AVAILABLE = True
    print("[V5 INFO] Strategic Fusion Layer importada da V6 com sucesso!")
except ImportError as e:
    STRATEGIC_FUSION_AVAILABLE = False
    print(f"[V5 WARNING] Strategic Fusion Layer n√£o dispon√≠vel: {e}")


class SimplifiedUltraEntryHead48h(nn.Module):
    """
    üöÄ ENTRY HEAD SIMPLIFICADA PARA TRADES 48H
    
    SIMPLIFICA√á√ïES IMPLEMENTADAS:
    - 20 attention heads ‚Üí 4 attention heads consolidados
    - 13 redes neurais ‚Üí 3 componentes principais
    - 4 buffers de mem√≥ria ‚Üí 1 buffer unificado
    - Multiplica√ß√£o de gates ‚Üí Weighted combination
    - Mant√©m TODA a intelig√™ncia core
    
    INTELIG√äNCIA MANTIDA:
    - Temporal Horizon Awareness
    - Multi-Timeframe Fusion
    - Dynamic Risk Adaptation
    - Market Regime Intelligence
    - Advanced Pattern Memory
    - Predictive Lookahead
    """
    
    def __init__(self, base_features_dim=128, lstm_hidden_size=128):
        super().__init__()
        
        self.base_features_dim = base_features_dim
        self.lstm_hidden_size = lstm_hidden_size
        
        # üöÄ 1. CONSOLIDATED ATTENTION (20 heads ‚Üí 4 heads)
        # Unifica: entry_attention + temporal + validation + risk + regime
        self.consolidated_attention = nn.MultiheadAttention(
            embed_dim=base_features_dim,
            num_heads=4,  # Reduzido de 8 para 4
            dropout=0.1,
            batch_first=True
        )
        self.attention_norm = nn.LayerNorm(base_features_dim)
        
        # üöÄ 2. TEMPORAL & RISK ANALYZER (Unifica horizon + risk + regime)
        # Processa horizon_embedding (8) + risk_embedding (8) + regime_embedding (8)
        self.temporal_risk_analyzer = nn.Sequential(
            nn.Linear(base_features_dim + 24, 96),  # +24 dos embeddings
            nn.ReLU(),
            nn.LayerNorm(96),
            nn.Dropout(0.1),
            nn.Linear(96, 48),
            nn.ReLU(),
            nn.Linear(48, 3),  # 3 scores: temporal, risk, regime
            nn.Sigmoid()
        )
        
        # üöÄ 3. MARKET INTELLIGENCE PROCESSOR (Unifica MTF + Pattern + Lookahead)
        # Processa timeframe_fusion (128) + pattern_memory (192) + lookahead (1)
        self.market_intelligence = nn.Sequential(
            nn.Linear(base_features_dim + 321, 128),  # +321 dos componentes
            nn.ReLU(),
            nn.LayerNorm(128),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 4),  # 4 scores: mtf, pattern, lookahead, quality
            nn.Sigmoid()
        )
        
        # üöÄ 4. QUALITY & CONFIDENCE ESTIMATOR (Unifica 4 filters + confidence)
        # Processa momentum, volatility, volume, trend_strength, confidence
        self.quality_confidence = nn.Sequential(
            nn.Linear(base_features_dim, 64),
            nn.ReLU(),
            nn.LayerNorm(64),
            nn.Dropout(0.1),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 2),  # 2 scores: quality_composite, confidence
            nn.Sigmoid()
        )
        
        # üöÄ 5. UNIFIED MEMORY BUFFER (4 buffers ‚Üí 1 buffer)
        # Substitui: recent_trades_memory + pattern memories
        self.register_buffer('unified_memory', torch.zeros(50, 8))  # 50 slots, 8 features
        self.register_buffer('memory_ptr', torch.zeros(1, dtype=torch.long))
        
        # üöÄ 6. MARKET FATIGUE DETECTOR (Simplificado)
        self.fatigue_detector = nn.Sequential(
            nn.Linear(8, 16),  # Reduzido de 20 para 16
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )
        
        # üöÄ 7. WEIGHTED COMBINATION NETWORK (Substitui multiplica√ß√£o de gates)
        # Aprende pesos para combinar os 9 scores de forma inteligente
        self.score_weights = nn.Parameter(torch.ones(9) / 9, requires_grad=True)  # Inicializa balanceado
        
        # üöÄ 8. FINAL DECISION NETWORK (Simplificado)
        # Entrada: base_features + 9 scores + 1 weighted_score
        final_input_dim = base_features_dim + 10  # +10 para scores
        
        self.final_decision_network = nn.Sequential(
            nn.Linear(final_input_dim, 192),  # Reduzido de 256
            nn.ReLU(),
            nn.LayerNorm(192),
            nn.Dropout(0.12),
            nn.Linear(192, 96),  # Reduzido de 128
            nn.ReLU(),
            nn.LayerNorm(96),
            nn.Linear(96, 64)  # Output final (compat√≠vel com V4)
        )
        
        # üöÄ THRESHOLDS H√çBRIDOS V6: Para sigmoid nos gates individuais + hard threshold final
        # Valores moderados que funcionam bem com sigmoid*10 (gradientes suaves)
        self.adaptive_threshold_main = nn.Parameter(torch.tensor(0.45), requires_grad=True)     # Quality/Confidence: 45% - EQUILIBRADO
        self.adaptive_threshold_risk = nn.Parameter(torch.tensor(0.50), requires_grad=True)     # Risk: 50% - MAIS RESTRITIVO
        self.adaptive_threshold_regime = nn.Parameter(torch.tensor(0.40), requires_grad=True)   # Temporal/Market: 40% - EQUILIBRADO
    
    def update_trade_memory(self, trade_result, trade_duration, entry_confidence, volatility, volume):
        """Atualiza mem√≥ria unificada de trades recentes"""
        ptr = int(self.memory_ptr.item())
        
        # Unifica informa√ß√µes de trade + padr√µes em um √∫nico buffer
        device = self.unified_memory.device
        self.unified_memory[ptr] = torch.tensor([
            trade_result, trade_duration, entry_confidence, volatility, volume, 
            0.0, 0.0, 0.0  # Slots para padr√µes detectados
        ], device=device)
        
        self.memory_ptr[0] = (ptr + 1) % 50
    
    def _extract_market_context(self, base_features, all_scores):
        """üîí MARKET CONTEXT REFATORADO: Interpreta√ß√£o robusta e din√¢mica das features"""
        try:
            batch_size = base_features.shape[0]
            device = base_features.device
            
            # üîí VALIDA√á√ÉO DE ENTRADA
            if base_features.shape[1] < 2:  # M√≠nimo necess√°rio: pre√ßo + volatilidade
                return self._get_default_market_context(batch_size, device)
            
            # üöÄ REFATORA√á√ÉO: INTERPRETA√á√ÉO INTELIGENTE DAS FEATURES
            # Feature 0: Price-based (assumindo pre√ßo normalizado)
            price_feature = base_features[:, 0:1]
            
            # Feature 1: Volatility (detec√ß√£o autom√°tica de range)
            volatility_raw = torch.abs(base_features[:, 1:2])
            # Detec√ß√£o inteligente: se valores s√£o muito pequenos (<0.1), assume normalizado
            is_normalized_vol = (volatility_raw.max() < 0.1).item()
            if is_normalized_vol:
                volatility_estimate = torch.clamp(volatility_raw, 0.0001, 1.0)  # Range 0-100%
            else:
                volatility_estimate = torch.clamp(volatility_raw, 0.01, 50.0)  # Range 1-5000%
            
            # Feature 2: Volume/Momentum (com fallback)
            if base_features.shape[1] > 2:
                volume_momentum = torch.abs(base_features[:, 2:3])
                volume_factor = torch.clamp(volume_momentum, 0.1, 5.0)
            else:
                volume_factor = torch.ones_like(price_feature)
            
            # Feature 3: Trend/Direction (com fallback)
            if base_features.shape[1] > 3:
                trend_feature = base_features[:, 3:4]
                trend_strength = torch.abs(trend_feature)
            else:
                trend_feature = torch.zeros_like(price_feature)
                trend_strength = torch.zeros_like(price_feature)
            
            # Feature 4: RSI (com fallback)
            if base_features.shape[1] > 4:
                rsi_raw = base_features[:, 4:5]
                # Detec√ß√£o inteligente: se valores s√£o pequenos (<2), assume normalizado
                is_normalized_rsi = (rsi_raw.max() < 2.0).item()
                if is_normalized_rsi:
                    rsi_estimate = torch.clamp(rsi_raw * 100, 0.0, 100.0)
                else:
                    rsi_estimate = torch.clamp(rsi_raw, 0.0, 100.0)
            else:
                rsi_estimate = torch.full_like(price_feature, 50.0)  # RSI neutro
            
            # üöÄ C√ÅLCULOS DIN√ÇMICOS E ROBUSTOS
            # Vol factor: adaptativo baseado na detec√ß√£o de range com varia√ß√£o for√ßada
            if is_normalized_vol:
                # Adicionar varia√ß√£o baseada em outras features para evitar valores fixos
                vol_base = volatility_estimate * 10
                vol_variation = torch.abs(price_feature) * 2.0 + torch.abs(trend_feature) * 3.0
                vol_factor = torch.clamp(vol_base + vol_variation, 0.1, 20.0)
            else:
                vol_factor = torch.clamp(volatility_estimate, 0.1, 20.0)
            
            # Extreme factor: m√∫ltiplas condi√ß√µes para detec√ß√£o
            extreme_conditions = [
                (rsi_estimate > 80) | (rsi_estimate < 20),  # RSI extremo
                vol_factor > 10.0,  # Alta volatilidade
                trend_strength > 0.8,  # Tend√™ncia forte
                volume_factor > 3.0  # Volume alto
            ]
            
            # Extreme factor progressivo: 0.5 (muito extremo) at√© 1.0 (normal)
            extreme_count = sum(condition.float() for condition in extreme_conditions)
            extreme_factor = torch.clamp(1.0 - (extreme_count * 0.125), 0.5, 1.0)
            
            # Quality factor: baseado em m√∫ltiplos indicadores
            quality_components = [
                torch.clamp(all_scores.mean(dim=-1, keepdim=True), 0.0, 1.0),  # Scores m√©dios
                torch.clamp(1.0 - (vol_factor - 1.0) / 10.0, 0.0, 1.0),  # Penaliza vol extrema
                torch.clamp(trend_strength, 0.0, 1.0),  # For√ßa da tend√™ncia
                torch.clamp(volume_factor / 5.0, 0.0, 1.0)  # Volume normalizado
            ]
            quality_factor = torch.clamp(torch.stack(quality_components).mean(dim=0), 0.3, 1.2)
            
            # Adaptability factor: combina√ß√£o inteligente com mais varia√ß√£o
            adaptability_factor = torch.clamp(
                vol_factor * extreme_factor * quality_factor * 0.1, 0.1, 3.0
            )
            
            return {
                'volatility_factor': vol_factor,
                'extreme_factor': extreme_factor,
                'quality_factor': quality_factor,
                'adaptability_factor': adaptability_factor,
                'raw_volatility': volatility_estimate,
                'raw_rsi': rsi_estimate,
                'is_extreme_setup': extreme_mask.float(),
                'batch_size': batch_size
            }
            
        except Exception as e:
            # Fallback total em caso de erro
            return self._get_default_market_context(batch_size, base_features.device)
    
    def _get_default_market_context(self, batch_size, device):
        """üîí FALLBACK DIN√ÇMICO: Contexto padr√£o com varia√ß√£o baseada em ru√≠do"""
        # Adicionar ru√≠do para criar varia√ß√£o nos valores padr√£o
        noise = torch.randn((batch_size, 1), device=device) * 0.1
        
        return {
            'volatility_factor': torch.clamp(torch.full((batch_size, 1), 1.0, device=device) + noise, 0.3, 3.0),
            'extreme_factor': torch.clamp(torch.full((batch_size, 1), 1.0, device=device) + noise * 0.5, 0.5, 1.0),
            'quality_factor': torch.clamp(torch.full((batch_size, 1), 1.0, device=device) + noise * 0.3, 0.5, 1.2),
            'adaptability_factor': torch.clamp(torch.full((batch_size, 1), 0.8, device=device) + noise * 0.8, 0.2, 2.0),
            'raw_volatility': torch.clamp(torch.full((batch_size, 1), 0.01, device=device) + noise * 0.02, 0.005, 0.1),
            'raw_rsi': torch.clamp(torch.full((batch_size, 1), 50.0, device=device) + noise * 20, 20.0, 80.0),
            'is_extreme_setup': torch.zeros((batch_size, 1), device=device),
            'batch_size': batch_size
        }
    
    def _adaptive_threshold(self, base_threshold, market_context, gate_type):
        """üîí THRESHOLDS ADAPTATIVOS ROBUSTOS: L√≥gica clara e garantida"""
        try:
            # üîí EXTRA√á√ÉO SEGURA DE FATORES
            vol_factor = market_context['volatility_factor']
            extreme_factor = market_context['extreme_factor']
            
            # üîí CONVERS√ÉO SEGURA PARA TENSOR
            if isinstance(base_threshold, float):
                base_threshold = torch.tensor(base_threshold, device=vol_factor.device)
            
            # üöÄ REFATORA√á√ÉO: THRESHOLDS DIN√ÇMICOS MULTI-FATOR
            
            # Obter fatores adicionais do contexto
            quality_factor = market_context.get('quality_factor', torch.ones_like(vol_factor))
            adaptability_factor = market_context.get('adaptability_factor', torch.ones_like(vol_factor))
            
            if gate_type == 'risk':
                # Risk: Din√¢mico baseado em m√∫ltiplos fatores
                # Fatores de redu√ß√£o progressivos
                vol_reduction = torch.where(vol_factor > 8.0,
                                          torch.full_like(vol_factor, 0.60),   # 40% redu√ß√£o vol muito alta
                                          torch.where(vol_factor > 4.0,
                                                    torch.full_like(vol_factor, 0.75),   # 25% redu√ß√£o vol alta
                                                    torch.where(vol_factor > 2.0,
                                                              torch.full_like(vol_factor, 0.85),   # 15% redu√ß√£o vol m√©dia
                                                              torch.where(vol_factor < 0.5,
                                                                        torch.full_like(vol_factor, 0.90),   # 10% redu√ß√£o vol baixa
                                                                        torch.full_like(vol_factor, 1.0)))))  # sem redu√ß√£o vol normal
                
                # Fator de qualidade: menor qualidade = threshold mais alto
                quality_adjustment = torch.clamp(quality_factor, 0.7, 1.3)
                
                adapted = base_threshold * vol_reduction * extreme_factor * quality_adjustment
                return torch.clamp(adapted, 0.02, 0.6)  # Range mais amplo
                
            elif gate_type == 'main':
                # Quality/Confidence: Baseado em qualidade e extremos
                # Redu√ß√£o baseada em qualidade
                quality_reduction = torch.where(quality_factor > 0.9,
                                              torch.full_like(quality_factor, 0.80),  # 20% redu√ß√£o alta qualidade
                                              torch.where(quality_factor > 0.7,
                                                        torch.full_like(quality_factor, 0.85),  # 15% redu√ß√£o qualidade m√©dia
                                                        torch.where(quality_factor < 0.5,
                                                                  torch.full_like(quality_factor, 1.1),   # 10% aumento baixa qualidade
                                                                  torch.full_like(quality_factor, 1.0))))  # sem altera√ß√£o normal
                
                # Ajuste por extremos
                extreme_adjustment = torch.where(extreme_factor < 0.7,
                                               torch.full_like(extreme_factor, 0.75),  # 25% redu√ß√£o muito extremo
                                               torch.where(extreme_factor < 0.9,
                                                         torch.full_like(extreme_factor, 0.90),  # 10% redu√ß√£o extremo
                                                         torch.full_like(extreme_factor, 1.0)))  # sem redu√ß√£o normal
                
                adapted = base_threshold * quality_reduction * extreme_adjustment
                return torch.clamp(adapted, 0.02, 0.5)  # Range mais restritivo para qualidade
                
            elif gate_type == 'regime':
                # Temporal/Market: Baseado em adaptabilidade
                # Redu√ß√£o baseada em adaptabilidade
                adaptability_reduction = torch.where(adaptability_factor > 1.5,
                                                   torch.full_like(adaptability_factor, 0.70),  # 30% redu√ß√£o alta adaptabilidade
                                                   torch.where(adaptability_factor > 1.0,
                                                             torch.full_like(adaptability_factor, 0.80),  # 20% redu√ß√£o adaptabilidade m√©dia
                                                             torch.where(adaptability_factor < 0.5,
                                                                       torch.full_like(adaptability_factor, 1.15),  # 15% aumento baixa adaptabilidade
                                                                       torch.full_like(adaptability_factor, 1.0))))  # sem altera√ß√£o normal
                
                # Ajuste por volatilidade
                vol_adjustment = torch.clamp(1.0 - (vol_factor - 1.0) * 0.05, 0.8, 1.2)
                
                adapted = base_threshold * adaptability_reduction * vol_adjustment
                return torch.clamp(adapted, 0.03, 0.7)  # Range intermedi√°rio
                
            else:
                # Fallback com varia√ß√£o baseada em contexto
                context_adjustment = torch.clamp(adaptability_factor * quality_factor, 0.8, 1.2)
                return torch.clamp(base_threshold * context_adjustment, 0.05, 0.8)
                
        except Exception as e:
            # Fallback total em caso de erro - usar adaptive threshold
            if gate_type == 'risk':
                return torch.full_like(market_context['volatility_factor'], self.adaptive_threshold_risk.item())
            elif gate_type == 'main':
                return torch.full_like(market_context['volatility_factor'], self.adaptive_threshold_main.item())
            else:
                return torch.full_like(market_context['volatility_factor'], self.adaptive_threshold_regime.item())
    
    def _calculate_volatility_bonus(self, market_context):
        """üîí BONIFICA√á√ïES DE VOLATILIDADE ROBUSTAS: L√≥gica clara e garantida"""
        try:
            vol_factor = market_context['volatility_factor']
            
            # üîí L√ìGICA MAIS SELETIVA: VOLATILIDADE = NOSSA MAIOR ALIADA
            # Alta vol (>1.3) = oportunidade de lucro = +15% boost
            # Baixa vol (<0.6) = baixo risco = +10% boost
            # Vol normal (0.6-1.3) = sem bonus
            
            high_vol_bonus = torch.where(vol_factor > 1.3, 
                                       torch.full_like(vol_factor, 0.15),  # +15% para vol realmente alta
                                       torch.zeros_like(vol_factor))
            
            low_vol_bonus = torch.where(vol_factor < 0.6,
                                      torch.full_like(vol_factor, 0.10),  # +10% para vol realmente baixa
                                      torch.zeros_like(vol_factor))
            
            # Garantir que os bonuses n√£o se sobrep√µem
            total_bonus = torch.maximum(high_vol_bonus, low_vol_bonus)
            
            return torch.clamp(total_bonus, 0.0, 0.20)  # Max 20% bonus
            
        except Exception as e:
            # Fallback: sem bonus
            return torch.zeros_like(market_context['volatility_factor'])
    
    def _calculate_setup_bonus(self, market_context):
        """üîí BONIFICA√á√ïES DE SETUP ROBUSTAS: L√≥gica clara e garantida"""
        try:
            is_extreme = market_context['is_extreme_setup']
            
            # üîí L√ìGICA MAIS SELETIVA: Setup extremo = +12% boost
            setup_bonus = torch.where(is_extreme > 0.5,
                                    torch.full_like(is_extreme, 0.12),  # +12% para setup extremo
                                    torch.zeros_like(is_extreme))
            
            return torch.clamp(setup_bonus, 0.0, 0.12)  # Max 12% bonus
            
        except Exception as e:
            # Fallback: sem bonus
            return torch.zeros_like(market_context['volatility_factor'])
    
    def _calculate_composite_gate_score(self, gates, market_context):
        """üöÄ SISTEMA COMPOSITE REFATORADO: Pontua√ß√£o din√¢mica e realista"""
        try:
            # üîí VALIDA√á√ÉO DE ENTRADA
            if not gates or not isinstance(gates, dict):
                return torch.zeros_like(market_context['volatility_factor'])
            
            # üîí EXTRA√á√ÉO SEGURA DE FATORES
            vol_factor = market_context['volatility_factor']
            quality_factor = market_context.get('quality_factor', torch.ones_like(vol_factor))
            adaptability_factor = market_context.get('adaptability_factor', torch.ones_like(vol_factor))
            extreme_factor = market_context['extreme_factor']
            
            # üöÄ PESOS DIN√ÇMICOS baseados em m√∫ltiplos fatores
            vol_mean = vol_factor.mean().item()
            quality_mean = quality_factor.mean().item()
            adaptability_mean = adaptability_factor.mean().item()
            
            # Determinar regime de mercado baseado em m√∫ltiplos fatores
            is_high_vol = vol_mean > 3.0
            is_high_quality = quality_mean > 0.8
            is_high_adaptability = adaptability_mean > 1.2
            
            # Pesos adaptativos baseados no regime
            if is_high_vol and is_high_quality:  # Mercado vol√°til com qualidade
                weights = {
                    'temporal': 0.15,
                    'validation': 0.10,
                    'risk': 0.10,      # Menor peso (volatilidade = oportunidade)
                    'market': 0.20,    # Maior peso (mercado ativo)
                    'quality': 0.25,   # Alto peso (qualidade importante)
                    'confidence': 0.20
                }
            elif is_high_vol and not is_high_quality:  # Mercado vol√°til com baixa qualidade
                weights = {
                    'temporal': 0.10,
                    'validation': 0.20,  # Maior valida√ß√£o necess√°ria
                    'risk': 0.25,        # Muito maior peso no risco
                    'market': 0.15,
                    'quality': 0.20,
                    'confidence': 0.10    # Menor confian√ßa
                }
            elif not is_high_vol and is_high_quality:  # Mercado calmo com qualidade
                weights = {
                    'temporal': 0.20,    # Maior peso temporal
                    'validation': 0.15,
                    'risk': 0.05,        # Menor risco
                    'market': 0.10,
                    'quality': 0.35,     # Muito maior peso na qualidade
                    'confidence': 0.15
                }
            else:  # Mercado calmo com baixa qualidade
                weights = {
                    'temporal': 0.15,
                    'validation': 0.25,  # Muito maior valida√ß√£o
                    'risk': 0.20,
                    'market': 0.15,
                    'quality': 0.15,
                    'confidence': 0.10   # Baixa confian√ßa
                }
            
            # üöÄ C√ÅLCULO DIN√ÇMICO DO SCORE
            composite_score = torch.zeros_like(vol_factor)
            
            # Calcular score ponderado com penaliza√ß√µes
            for gate_name, gate_value in gates.items():
                if gate_name in weights:
                    weight = weights[gate_name]
                    
                    # Validar que gate_value √© tensor
                    if isinstance(gate_value, torch.Tensor):
                        gate_contribution = gate_value * weight
                    else:
                        gate_contribution = float(gate_value) * weight
                    
                    # üöÄ PENALIZA√á√ÉO PROGRESSIVA: Gates baixos s√£o penalizados mais
                    if isinstance(gate_value, torch.Tensor):
                        gate_mean = gate_value.mean().item()
                    else:
                        gate_mean = float(gate_value)
                    
                    # Penaliza√ß√£o para gates baixos
                    if gate_mean < 0.3:
                        gate_contribution *= 0.5  # 50% penaliza√ß√£o para gates muito baixos
                    elif gate_mean < 0.5:
                        gate_contribution *= 0.75  # 25% penaliza√ß√£o para gates baixos
                    elif gate_mean < 0.7:
                        gate_contribution *= 0.9   # 10% penaliza√ß√£o para gates medianos
                    
                    composite_score += gate_contribution
            
            # üöÄ BONIFICA√á√ÉO DIN√ÇMICA baseada em m√∫ltiplos fatores
            # Bonifica√ß√£o apenas para setups realmente extremos E com qualidade
            extreme_quality_bonus = torch.where(
                (extreme_factor < 0.7) & (quality_factor > 0.8),
                torch.full_like(composite_score, 0.10),  # +10% para extremo com qualidade
                torch.zeros_like(composite_score)
            )
            
            # Bonifica√ß√£o para alta adaptabilidade
            adaptability_bonus = torch.where(
                adaptability_factor > 1.5,
                torch.full_like(composite_score, 0.05),  # +5% para alta adaptabilidade
                torch.zeros_like(composite_score)
            )
            
            # üöÄ PENALIZA√á√ÉO para condi√ß√µes ruins
            # Penaliza√ß√£o para baixa qualidade
            quality_penalty = torch.where(
                quality_factor < 0.4,
                torch.full_like(composite_score, -0.15),  # -15% para qualidade muito baixa
                torch.where(
                    quality_factor < 0.6,
                    torch.full_like(composite_score, -0.05),  # -5% para qualidade baixa
                    torch.zeros_like(composite_score)
                )
            )
            
            # Score final com bonifica√ß√µes e penaliza√ß√µes
            final_score = composite_score + extreme_quality_bonus + adaptability_bonus + quality_penalty
            
            # üîí GARANTIR RANGE V√ÅLIDO com normaliza√ß√£o
            return torch.clamp(final_score, 0.0, 1.0)
            
        except Exception as e:
            # Fallback: score neutro
            return torch.full_like(market_context['volatility_factor'], 0.5)
    
    def forward(self, base_features, intelligent_components):
        """
        Forward pass simplificado mas inteligente
        
        Args:
            base_features: Features base da V5 [batch_size, 128]
            intelligent_components: Dict com componentes inteligentes da V4/V5
        """
        
        # üöÄ 1. CONSOLIDATED ATTENTION (√önico attention para todas as an√°lises)
        attn_input = base_features.unsqueeze(1)
        attn_features, attn_weights = self.consolidated_attention(attn_input, attn_input, attn_input)
        attn_features = attn_features.squeeze(1)
        attn_features = self.attention_norm(attn_features)
        
        # üöÄ 2. EXTRACT INTELLIGENT COMPONENTS (Mant√©m extra√ß√£o completa)
        horizon_emb = intelligent_components.get('horizon_embedding', torch.zeros(base_features.shape[0], 8).to(base_features.device))
        mtf_fusion = intelligent_components.get('timeframe_fusion', torch.zeros(base_features.shape[0], 128).to(base_features.device))
        risk_emb = intelligent_components.get('risk_embedding', torch.zeros(base_features.shape[0], 8).to(base_features.device))
        regime_emb = intelligent_components.get('regime_embedding', torch.zeros(base_features.shape[0], 8).to(base_features.device))
        pattern_memory = intelligent_components.get('pattern_memory', torch.zeros(base_features.shape[0], 192).to(base_features.device))
        lookahead = intelligent_components.get('lookahead', torch.zeros(base_features.shape[0], 1).to(base_features.device))
        
        # üöÄ 3. TEMPORAL & RISK ANALYSIS (Consolidado)
        temporal_risk_input = torch.cat([attn_features, horizon_emb, risk_emb, regime_emb], dim=-1)
        temporal_risk_scores = self.temporal_risk_analyzer(temporal_risk_input)
        temporal_score = temporal_risk_scores[:, 0:1]  # Score temporal
        risk_score = temporal_risk_scores[:, 1:2]      # Score de risco
        regime_score = temporal_risk_scores[:, 2:3]    # Score de regime
        
        # üöÄ 4. MARKET INTELLIGENCE PROCESSING (Consolidado)
        market_input = torch.cat([attn_features, mtf_fusion, pattern_memory, lookahead], dim=-1)
        market_scores = self.market_intelligence(market_input)
        mtf_score = market_scores[:, 0:1]        # Multi-timeframe score
        pattern_score = market_scores[:, 1:2]    # Pattern memory score
        lookahead_score = market_scores[:, 2:3]  # Lookahead score
        quality_raw = market_scores[:, 3:4]      # Quality base score
        
        # üöÄ 5. QUALITY & CONFIDENCE ESTIMATION (Consolidado)
        quality_conf_scores = self.quality_confidence(attn_features)
        quality_score = quality_conf_scores[:, 0:1]  # Quality refinado
        confidence_score = quality_conf_scores[:, 1:2]  # Confidence score
        
        # üöÄ 6. MARKET FATIGUE ANALYSIS (Simplificado)
        # üîß CORRE√á√ÉO CR√çTICA: Garantir que unified_memory est√° no device correto
        unified_memory_device = self.unified_memory.to(base_features.device)
        avg_memory = unified_memory_device.mean(dim=0)
        fatigue_score = 1.0 - self.fatigue_detector(avg_memory)  # Inverte: alta fatigue = baixo score
        fatigue_score = fatigue_score.expand(base_features.shape[0], 1)  # Broadcast para batch
        
        # üöÄ 7. COLLECT ALL SCORES (9 scores totais)
        all_scores = torch.cat([
            temporal_score,    # 1
            risk_score,        # 2
            regime_score,      # 3
            mtf_score,         # 4
            pattern_score,     # 5
            lookahead_score,   # 6
            quality_score,     # 7
            confidence_score,  # 8
            fatigue_score      # 9
        ], dim=-1)
        
        # üöÄ 8. WEIGHTED COMBINATION (Substitui multiplica√ß√£o de gates)
        # Aplica softmax aos pesos para normalizar
        normalized_weights = torch.softmax(self.score_weights, dim=0)
        weighted_score = torch.sum(all_scores * normalized_weights.unsqueeze(0), dim=-1, keepdim=True)
        
        # üöÄ VOLATILIDADE = NOSSA MAIOR ALIADA: Thresholds adaptativos por contexto
        market_context = self._extract_market_context(base_features, all_scores)
        main_threshold = self._adaptive_threshold(self.adaptive_threshold_main, market_context, 'main')
        risk_threshold = self._adaptive_threshold(self.adaptive_threshold_risk, market_context, 'risk')
        regime_threshold = self._adaptive_threshold(self.adaptive_threshold_regime, market_context, 'regime')
        
        # üöÄ GATES INTELIGENTES: Bonifica√ß√µes por volatilidade e setups extremos
        volatility_bonus = self._calculate_volatility_bonus(market_context)
        setup_bonus = self._calculate_setup_bonus(market_context)
        
        # Aplicar bonifica√ß√µes aos scores antes dos gates
        temporal_score_adj = temporal_score + volatility_bonus
        risk_score_adj = risk_score + volatility_bonus + setup_bonus
        quality_score_adj = quality_score + setup_bonus
        confidence_score_adj = confidence_score + volatility_bonus + setup_bonus
        
        # üöÄ ABORDAGEM H√çBRIDA V6: Sigmoid nos gates individuais + hard threshold final
        # Permite gradientes suaves para melhor converg√™ncia, mas mant√©m filtro real no final
        temporal_gate = torch.sigmoid((temporal_score_adj - regime_threshold) * 10)
        risk_gate = torch.sigmoid((risk_score_adj - risk_threshold) * 10)
        quality_gate = torch.sigmoid((quality_score_adj - main_threshold) * 10)
        confidence_gate = torch.sigmoid((confidence_score_adj - main_threshold) * 10)
        
        # Gates secund√°rios com sigmoid (gradientes suaves)
        validation_gate = torch.sigmoid((confidence_score_adj - main_threshold * 0.8) * 10)
        market_gate = torch.sigmoid((regime_score - regime_threshold * 0.9) * 10)
        
        # üéØ SISTEMA COMPOSITE INTELIGENTE: Pontua√ß√£o ponderada ao inv√©s de multiplica√ß√£o
        composite_score = self._calculate_composite_gate_score({
            'temporal': temporal_gate,
            'validation': validation_gate,
            'risk': risk_gate,
            'market': market_gate,
            'quality': quality_gate,
            'confidence': confidence_gate
        }, market_context)
        
        # Threshold adaptativo para composite (0.3-0.7 baseado no contexto)
        adaptability_factor = market_context.get('adaptability_factor', torch.tensor(0.8))
        if isinstance(adaptability_factor, torch.Tensor):
            adaptability_factor = adaptability_factor.mean().item()
        composite_threshold = 0.5 * adaptability_factor  # üéØ EQUILIBRADO: 0.5 com adaptability (0.4 final)
        # üöÄ GATE FINAL HARD THRESHOLD (√∫nico bin√°rio da V5 h√≠brida)
        # Aqui sim aplicamos hard threshold para filtro real, como na V6
        final_gate = (composite_score > composite_threshold).float()
        
        # üéØ DEBUG PERSONALIZADO: M√©tricas aos 750 passos e fim do epis√≥dio
        # Detectar se temos acesso ao step atual do epis√≥dio (via contexto global ou environment)
        current_step = getattr(self, '_current_episode_step', 0)
        episode_length = getattr(self, '_episode_length', 1000)  # Default 1000 steps
        
        # Incrementar step interno (fallback se n√£o vier do environment)
        if not hasattr(self, '_internal_step'):
            self._internal_step = 0
        self._internal_step += 1
        
        # SIMPLIFICAR: Logar a cada 750 forward passes ao inv√©s de por epis√≥dio
        # (Por epis√≥dio √© complexo de detectar corretamente)
        if not hasattr(self, '_global_step'):
            self._global_step = 0
        self._global_step += 1
        
        # Log a cada 750 forward passes (mais simples e confi√°vel)
        should_log = (self._global_step % 750 == 0)
        
        if should_log:
            # Determinar momento do log
            log_moment = f"STEP-{self._global_step}"
            
            # DEBUG PROFUNDO: Extrair valores intermedi√°rios
            vol_factor_val = 1.0
            volatility_raw_val = -1.0
            volatility_estimate_val = -1.0
            
            try:
                if 'volatility_factor' in market_context:
                    vol_factor_val = market_context['volatility_factor'].mean().item()
                    # DEBUG: Valores intermedi√°rios da volatilidade
                    if 'raw_volatility' in market_context:
                        volatility_estimate_val = market_context['raw_volatility'].mean().item()
                    # Tentar calcular o valor raw original
                    volatility_raw_val = base_features[:, 1].mean().item() if base_features.shape[1] > 1 else -2.0
                else:
                    vol_factor_val = -999.0  # Flag de debug
            except Exception as e:
                vol_factor_val = -888.0  # Flag de erro
                
            main_thresh_val = risk_thresh_val = regime_thresh_val = 0.5
            try:
                main_thresh_val = main_threshold.mean().item() if isinstance(main_threshold, torch.Tensor) else float(main_threshold)
                risk_thresh_val = risk_threshold.mean().item() if isinstance(risk_threshold, torch.Tensor) else float(risk_threshold)
                regime_thresh_val = regime_threshold.mean().item() if isinstance(regime_threshold, torch.Tensor) else float(regime_threshold)
            except Exception as e:
                main_thresh_val, risk_thresh_val, regime_thresh_val = -777.0, -777.0, -777.0  # Flag de erro
                
            composite_val = final_gate_val = 0.5
            # DEBUG dos gates individuais
            temporal_val = validation_val = risk_val = market_val = quality_val = confidence_val = -1.0
            try:
                temporal_val = temporal_gate.mean().item()
                validation_val = validation_gate.mean().item() 
                risk_val = risk_gate.mean().item()
                market_val = market_gate.mean().item()
                quality_val = quality_gate.mean().item()
                confidence_val = confidence_gate.mean().item()
                composite_val = composite_score.mean().item()
                final_gate_val = final_gate.mean().item()
            except Exception as e:
                composite_val, final_gate_val = -666.0, -666.0  # Flag de erro
            
            # Debug dos valores adaptive threshold originais
            try:
                adaptive_main_val = self.adaptive_threshold_main.item()
                adaptive_risk_val = self.adaptive_threshold_risk.item()
                adaptive_regime_val = self.adaptive_threshold_regime.item()
            except:
                adaptive_main_val = adaptive_risk_val = adaptive_regime_val = -555.0
            
            print(f"\n{'='*60}")
            print(f"METRICAS V5 - {log_moment}")
            print(f"{'='*60}")
            print(f"Volatilidade: {vol_factor_val:.3f} | Composite Score: {composite_val:.3f} | Final Gate: {final_gate_val:.3f}")
            print(f"Gates: temporal:{temporal_val:.3f} validation:{validation_val:.3f} risk:{risk_val:.3f}")
            print(f"       market:{market_val:.3f} quality:{quality_val:.3f} confidence:{confidence_val:.3f}")
            print(f"Thresholds: Main:{main_thresh_val:.3f} Risk:{risk_thresh_val:.3f} Regime:{regime_thresh_val:.3f}")
            print(f"{'='*60}\n")
        
        # üöÄ 10. FINAL DECISION NETWORK (Simplificado)
        final_input = torch.cat([
            attn_features,     # Features processadas
            all_scores,        # 9 scores individuais
            weighted_score     # 1 score combinado
        ], dim=-1)
        
        entry_decision = self.final_decision_network(final_input)
        
        # üöÄ 11. FINAL GATE APENAS PARA RETORNO (Sem multiplica√ß√£o - evita penaliza√ß√£o dupla)
        # entry_decision j√° √© processado pelos gates individuais na l√≥gica de filtros
        # final_gate √© apenas informativo para debugging e filtros externos
        
        return {
            'entry_decision': entry_decision,
            'scores': {
                'temporal': temporal_score,
                'risk': risk_score,
                'regime': regime_score,
                'mtf': mtf_score,
                'pattern': pattern_score,
                'lookahead': lookahead_score,
                'quality': quality_score,
                'confidence': confidence_score,
                'fatigue': fatigue_score,
                'weighted_composite': weighted_score,
                # üöÄ NOVOS SCORES AJUSTADOS
                'temporal_adj': temporal_score_adj,
                'risk_adj': risk_score_adj,
                'quality_adj': quality_score_adj,
                'confidence_adj': confidence_score_adj,
                'composite_score': composite_score
            },
            'gates': {
                'temporal': temporal_gate,
                'validation': validation_gate,
                'risk': risk_gate,
                'market': market_gate,
                'quality': quality_gate,
                'confidence': confidence_gate,
                'final': final_gate
            },
            'market_context': market_context,
            'bonuses': {
                'volatility': volatility_bonus,
                'setup': setup_bonus
            },
            'thresholds': {
                'main': main_threshold,
                'risk': risk_threshold,
                'regime': regime_threshold,
                'composite': composite_threshold
            },
            'attention_weights': attn_weights,
            'score_weights': normalized_weights  # Para debug
        }


class TwoHeadV5Intelligent48h(RecurrentActorCriticPolicy):
    """
    üöÄ TWOHEAD V5 INTELLIGENT 48H - Policy com Entry Head SIMPLIFICADA
    
    ARQUITETURA BASE (herdada da V4):
    - 2 LSTM Layers (hierarquia temporal)
    - 1 GRU Stabilizer (noise reduction)
    - 4 Attention Heads (SIMPLIFICADO - era 8)
    - Pattern Recognition (micro/macro)
    - Two Heads especializadas (Entry/Management)
    
    MELHORIAS V4 (herdadas):
    - Temporal Horizon Awareness
    - Multi-Timeframe Fusion
    - Advanced Pattern Memory
    - Dynamic Risk Adaptation
    - Market Regime Intelligence
    - Predictive Lookahead
    
    MELHORIAS V5 (SIMPLIFICADAS):
    - Entry Head SIMPLIFICADA (3 componentes consolidados)
    - 9 Scores de qualidade (otimizados)
    - Weighted Combination (substitui multiplica√ß√£o de gates)
    - Unified Memory Buffer (1 buffer ao inv√©s de 4)
    - Market Fatigue Detector (simplificado)
    - Thresholds adaptativos (mantidos)
    
    SIMPLIFICA√á√ïES IMPLEMENTADAS:
    - 20 attention heads ‚Üí 4 attention heads consolidados
    - 13 redes neurais ‚Üí 3 componentes principais
    - 4 buffers de mem√≥ria ‚Üí 1 buffer unificado
    - Multiplica√ß√£o de gates ‚Üí Weighted combination
    - Mant√©m TODA a intelig√™ncia core
    """
    
    def __init__(
        self,
        observation_space,
        action_space,
        lr_schedule,
        net_arch: Optional[List[Union[int, Dict[str, List[int]]]]] = None,
        activation_fn: Type[nn.Module] = nn.ReLU,
        ortho_init: bool = True,
        use_sde: bool = False,
        log_std_init: float = 0.0,
        use_expln: bool = False,
        squash_output: bool = False,
        features_extractor_class: Type[BaseFeaturesExtractor] = TradingTransformerFeatureExtractor,
        features_extractor_kwargs: Optional[Dict[str, Any]] = None,
        normalize_images: bool = True,
        optimizer_class: Type[torch.optim.Optimizer] = torch.optim.Adam,
        optimizer_kwargs: Optional[Dict[str, Any]] = None,
        # üöÄ PAR√ÇMETROS BASE (herdados da V4)
        lstm_hidden_size: int = 128,
        n_lstm_layers: int = 2,
        attention_heads: int = 8,
        gru_enabled: bool = True,
        pattern_recognition: bool = True,
        # üöÄ MELHORIAS CONSERVADORAS (herdadas da V4)
        adaptive_lr: bool = True,
        gradient_clipping: bool = True,
        feature_weighting: bool = True,
        dynamic_attention: bool = True,
        memory_bank_size: int = 100,
        # üöÄ MELHORIAS INTELIGENTES V4 (herdadas)
        enable_temporal_horizon: bool = True,
        enable_multi_timeframe: bool = True,
        enable_advanced_memory: bool = True,
        enable_dynamic_risk: bool = True,
        enable_regime_intelligence: bool = True,
        enable_lookahead: bool = True,
        # üöÄ MELHORIAS V5 (novas)
        enable_ultra_specialized_entry: bool = True,
        **kwargs
    ):
        """V5Intelligent48h com Entry Head Ultra-Especializada"""
        
        # üöÄ MANTER CONFIGURA√á√ïES COMPROVADAS
        if features_extractor_kwargs is None:
            features_extractor_kwargs = {
                'features_dim': 128,  # SIMPLIFICADO
                'seq_len': 10         # COMPROVADO
            }
        
        if net_arch is None:
            net_arch = [dict(pi=[192, 96], vf=[192, 96])]  # SIMPLIFICADO
        
        # üöÄ PAR√ÇMETROS BASE (herdados da V4)
        self.lstm_hidden_size = lstm_hidden_size
        self.n_lstm_layers = n_lstm_layers
        self.attention_heads = attention_heads
        self.gru_enabled = gru_enabled
        self.pattern_recognition = pattern_recognition
        
        # üöÄ MELHORIAS CONSERVADORAS (herdadas da V4)
        self.adaptive_lr = adaptive_lr
        self.gradient_clipping = gradient_clipping
        self.feature_weighting = feature_weighting
        self.dynamic_attention = dynamic_attention
        self.memory_bank_size = memory_bank_size
        
        # üöÄ MELHORIAS INTELIGENTES V4 (herdadas)
        self.enable_temporal_horizon = enable_temporal_horizon
        self.enable_multi_timeframe = enable_multi_timeframe
        self.enable_advanced_memory = enable_advanced_memory
        self.enable_dynamic_risk = enable_dynamic_risk
        self.enable_regime_intelligence = enable_regime_intelligence
        self.enable_lookahead = enable_lookahead
        
        # üöÄ MELHORIAS V5 (novas) - GARANTIA ABSOLUTA
        # FOR√áAR SEMPRE True para Entry Head Ultra-Especializada - ANTI-SABOTAGEM
        self.enable_ultra_specialized_entry = True  # SEMPRE True, ignorando par√¢metro
        if not enable_ultra_specialized_entry:
            print(f"üö® ANTI-SABOTAGEM: enable_ultra_specialized_entry foi passado como {enable_ultra_specialized_entry}, mas FOR√áADO para True")
        
        # Detectar action space
        self.action_dim = action_space.shape[0]
        
        # Inicializar classe pai
        super().__init__(
            observation_space=observation_space,
            action_space=action_space,
            lr_schedule=lr_schedule,
            net_arch=net_arch,
            activation_fn=activation_fn,
            ortho_init=ortho_init,
            use_sde=use_sde,
            log_std_init=log_std_init,
            use_expln=use_expln,
            squash_output=squash_output,
            features_extractor_class=features_extractor_class,
            features_extractor_kwargs=features_extractor_kwargs,
            normalize_images=normalize_images,
            optimizer_class=optimizer_class,
            optimizer_kwargs=optimizer_kwargs,
            lstm_hidden_size=self.lstm_hidden_size,
            n_lstm_layers=self.n_lstm_layers,
            **kwargs
        )
        
        # Inicializar componentes
        self._init_hybrid_components()
        self._init_enhancements()
        self._init_intelligent_48h_components()
        self._initialize_balanced_weights()
        
        # üé™ STRATEGIC FUSION LAYER V5 - Inicializa√ß√£o adaptativa
        self._init_strategic_fusion_layer()
        
        # Logs
        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"üöÄ TwoHeadV5Intelligent48h: {total_params:,} par√¢metros")
        print(f"üöÄ Base: 2-LSTM + 1-GRU + 4-Head Attention (Simplificado)")
        print(f"üöÄ Melhorias V4: Temporal Horizon, Multi-Timeframe, Advanced Memory, Dynamic Risk, Regime Intelligence, Lookahead")
        print(f"üöÄ Melhorias V5: Entry Head SIMPLIFICADA, 3 Componentes Consolidados, 9 Scores, Weighted Combination, Unified Memory")

    def _init_hybrid_components(self):
        """Inicializa componentes h√≠bridos com ENTRY HEAD SIMPLIFICADA"""
        features_dim = self.features_extractor.features_dim
        
        # üöÄ GRU ESTABILIZADOR (COMPROVADO)
        if self.gru_enabled:
            self.gru_stabilizer = nn.GRU(
                input_size=self.lstm_hidden_size,
                hidden_size=self.lstm_hidden_size,
                num_layers=1,
                batch_first=True,
                dropout=0.0
            )
            self.gru_norm = nn.LayerNorm(self.lstm_hidden_size)
        
        # üöÄ ATTENTION (COMPROVADO - 8 heads)
        self.temporal_attention = nn.MultiheadAttention(
            embed_dim=self.lstm_hidden_size,
            num_heads=self.attention_heads,
            dropout=0.10,  # COMPROVADO
            batch_first=True
        )
        
        # üöÄ PATTERN RECOGNITION (COMPROVADO)
        if self.pattern_recognition:
            self.micro_pattern_detector = nn.Sequential(
                nn.Linear(self.lstm_hidden_size, 64),
                nn.ReLU(),
                nn.Dropout(0.10),  # COMPROVADO
                nn.Linear(64, 32),
                nn.Sigmoid()
            )
            
            self.macro_pattern_detector = nn.Sequential(
                nn.Linear(self.lstm_hidden_size, 64),
                nn.ReLU(),
                nn.Dropout(0.10),  # COMPROVADO
                nn.Linear(64, 32),
                nn.Sigmoid()
            )
            pattern_features = 64
        else:
            pattern_features = 0
        
        # üöÄ FEATURE FUSION (COMPROVADO)
        fusion_input_size = self.lstm_hidden_size
        if self.gru_enabled:
            fusion_input_size += self.lstm_hidden_size
        fusion_input_size += pattern_features
        
        self.feature_fusion = nn.Sequential(
            nn.Linear(fusion_input_size, 320),  # COMPROVADO
            nn.LayerNorm(320),
            nn.ReLU(),
            nn.Dropout(0.15),  # COMPROVADO
            nn.Linear(320, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.10),  # COMPROVADO
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU()
        )
        
        # üöÄ ENTRY HEAD SIMPLIFICADA (V5) - GARANTIA ABSOLUTA ANTI-SABOTAGEM
        # SEMPRE criar Entry Head Simplificada, independente de qualquer flag
        print(f"üöÄ INICIALIZANDO Entry Head SIMPLIFICADA - GARANTIA TOTAL")
        print(f"   enable_ultra_specialized_entry: {self.enable_ultra_specialized_entry}")
        
        # FOR√áAR cria√ß√£o da Entry Head Simplificada
        self.entry_head = SimplifiedUltraEntryHead48h(
            base_features_dim=128,
            lstm_hidden_size=self.lstm_hidden_size
        )
        
        # Verifica√ß√£o de seguran√ßa
        if not isinstance(self.entry_head, SimplifiedUltraEntryHead48h):
            raise RuntimeError("üö® FALHA CR√çTICA: Entry Head Simplificada N√ÉO foi criada corretamente!")
        
        print(f"‚úÖ Entry Head Simplificada CRIADA com sucesso: {type(self.entry_head)}")
        
        # Log das simplifica√ß√µes implementadas
        print(f"üìä SIMPLIFICA√á√ïES IMPLEMENTADAS:")
        print(f"   ‚Ä¢ 20 attention heads ‚Üí 4 attention heads consolidados")
        print(f"   ‚Ä¢ 13 redes neurais ‚Üí 3 componentes principais")
        print(f"   ‚Ä¢ 4 buffers de mem√≥ria ‚Üí 1 buffer unificado")
        print(f"   ‚Ä¢ Multiplica√ß√£o de gates ‚Üí Weighted combination")
        print(f"   ‚Ä¢ Mant√©m TODA a intelig√™ncia core")
        
        # GARANTIR que enable_ultra_specialized_entry seja True
        if not self.enable_ultra_specialized_entry:
            print(f"üö® CORRE√á√ÉO: enable_ultra_specialized_entry era {self.enable_ultra_specialized_entry}, for√ßando para True")
            self.enable_ultra_specialized_entry = True
        
        # üöÄ MANAGEMENT HEAD (mant√©m da V4)
        self.management_head = nn.Sequential(
            nn.Linear(128, 96),
            nn.LayerNorm(96),
            nn.ReLU(),
            nn.Dropout(0.12),  # COMPROVADO
            nn.Linear(96, 64),
            nn.LayerNorm(64),
            nn.ReLU()
        )
        
        # Residual connection
        self.residual_connection = nn.Linear(128, 128)
    
    def _init_enhancements(self):
        """Inicializa MELHORIAS CONSERVADORAS da V4"""
        
        # üöÄ MELHORIA 1: Feature Importance Weighting
        if self.feature_weighting:
            self.feature_importance = nn.Parameter(
                torch.ones(self.lstm_hidden_size) * 0.5,  # Inicia neutro
                requires_grad=True
            )
        
        # üöÄ MELHORIA 2: Dynamic Attention Temperature
        if self.dynamic_attention:
            self.attention_temperature = nn.Parameter(
                torch.tensor(1.0),  # Inicia neutro
                requires_grad=True
            )
        
        # üöÄ MELHORIA 3: Memory Bank para padr√µes recorrentes
        if self.memory_bank_size > 0:
            self.register_buffer(
                'pattern_memory', 
                torch.zeros(self.memory_bank_size, 64)  # 64 = micro + macro patterns
            )
            self.register_buffer('memory_ptr', torch.zeros(1, dtype=torch.long))
        
        # üöÄ MELHORIA 4: Gradient statistics tracking
        if self.gradient_clipping:
            self.register_buffer('grad_norm_ema', torch.tensor(1.0))
            self.grad_ema_decay = 0.99
    
    def _init_intelligent_48h_components(self):
        """Inicializa MELHORIAS INTELIGENTES PARA 48H da V4"""
        
        # üöÄ MELHORIA 1: Temporal Horizon Awareness
        if self.enable_temporal_horizon:
            self.horizon_embedding = nn.Linear(1, 8)  # 1 valor (steps ou horas) -> 8d embedding
        
        # üöÄ MELHORIA 2: Multi-Timeframe Fusion
        if self.enable_multi_timeframe:
            self.timeframe_fusion = nn.Sequential(
                nn.Linear(self.features_extractor.features_dim * 3, 128),  # 3 timeframes (5m, 15m, 4h)
                nn.ReLU(),
                nn.LayerNorm(128)
            )
        
        # üöÄ MELHORIA 3: Advanced Pattern Memory System (multi-horizonte)
        if self.enable_advanced_memory:
            # Mem√≥ria separada para cada horizonte temporal
            self.memory_1h = nn.Parameter(torch.zeros(self.memory_bank_size, 64), requires_grad=False)
            self.memory_4h = nn.Parameter(torch.zeros(self.memory_bank_size, 64), requires_grad=False)
            self.memory_48h = nn.Parameter(torch.zeros(self.memory_bank_size, 64), requires_grad=False)
            self.memory_ptr_1h = nn.Parameter(torch.zeros(1, dtype=torch.long), requires_grad=False)
            self.memory_ptr_4h = nn.Parameter(torch.zeros(1, dtype=torch.long), requires_grad=False)
            self.memory_ptr_48h = nn.Parameter(torch.zeros(1, dtype=torch.long), requires_grad=False)
            
            # Attention cruzada entre padr√µes atuais e mem√≥rias
            self.cross_attention_1h = nn.MultiheadAttention(embed_dim=64, num_heads=4, batch_first=True)
            self.cross_attention_4h = nn.MultiheadAttention(embed_dim=64, num_heads=4, batch_first=True)
            self.cross_attention_48h = nn.MultiheadAttention(embed_dim=64, num_heads=4, batch_first=True)
        
        # üöÄ MELHORIA 4: Dynamic Risk Adaptation
        if self.enable_dynamic_risk:
            self.risk_embedding = nn.Linear(4, 8)  # [drawdown, vol, concentra√ß√£o, streak] -> 8d
            self.risk_gate = nn.Sequential(
                nn.Linear(8, 16),
                nn.ReLU(),
                nn.Linear(16, 1),
                nn.Sigmoid()
            )
        
        # üöÄ MELHORIA 5: Market Regime Intelligence
        if self.enable_regime_intelligence:
            self.regime_detector = nn.Sequential(
                nn.Linear(self.features_extractor.features_dim, 16),
                nn.ReLU(),
                nn.Linear(16, 4),  # 4 regimes: tend√™ncia, lateral, vol alto, vol baixo
                nn.Softmax(dim=-1)
            )
            self.regime_embedding = nn.Linear(4, 8)
        
        # üöÄ MELHORIA 6: Predictive Lookahead System
        if self.enable_lookahead:
            self.lookahead_head = nn.Sequential(
                nn.Linear(self.lstm_hidden_size, 32),
                nn.ReLU(),
                nn.Linear(32, 1)  # Previs√£o de retorno futuro
            )
    
    def _initialize_balanced_weights(self):
        """Inicializa√ß√£o COMPROVADA da V4"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.5)  # COMPROVADO
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.LayerNorm):
                nn.init.constant_(module.bias, 0)
                nn.init.constant_(module.weight, 1.0)
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.xavier_uniform_(param.data, gain=0.2)
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(param.data, gain=0.2)
                    elif 'bias' in name:
                        param.data.fill_(0)
                        n = param.size(0)
                        param.data[n//4:n//2].fill_(1)  # Forget gate bias = 1
            elif isinstance(module, nn.GRU):
                for name, param in module.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.xavier_uniform_(param.data, gain=0.2)
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(param.data, gain=0.2)
                    elif 'bias' in name:
                        param.data.fill_(0)
    
    def _apply_intelligent_48h_processing(self, input_features: torch.Tensor, extra_info: Optional[Dict[str, torch.Tensor]] = None) -> torch.Tensor:
        """Aplica processamento inteligente V5 com Entry Head Ultra-Especializada"""
        
        # 1. Proje√ß√£o inicial (se necess√°rio)
        if input_features.shape[-1] != self.lstm_hidden_size:
            if not hasattr(self, 'input_projector'):
                self.input_projector = nn.Linear(
                    input_features.shape[-1], 
                    self.lstm_hidden_size
                ).to(input_features.device)
            projected_features = self.input_projector(input_features)
        else:
            projected_features = input_features
        
        # üöÄ MELHORIA: Feature Importance Weighting
        if self.feature_weighting:
            importance_weights = torch.sigmoid(self.feature_importance)
            projected_features = projected_features * importance_weights.unsqueeze(0)
        
        # 2. GRU estabilizador (COMPROVADO)
        if self.gru_enabled:
            gru_input = projected_features.unsqueeze(1)
            gru_out, _ = self.gru_stabilizer(gru_input)
            gru_out = self.gru_norm(gru_out.squeeze(1))
        else:
            gru_out = projected_features
        
        # 3. Attention com temperature din√¢mica
        attn_input = projected_features.unsqueeze(1)
        
        # üöÄ MELHORIA: Dynamic Attention Temperature
        if self.dynamic_attention:
            temperature = torch.clamp(self.attention_temperature, 0.1, 2.0)
            attn_out, attn_weights = self.temporal_attention(
                attn_input, attn_input, attn_input
            )
            attn_out = attn_out / temperature
        else:
            attn_out, attn_weights = self.temporal_attention(
                attn_input, attn_input, attn_input
            )
        
        attn_out = attn_out.squeeze(1)
        
        # 4. Pattern Recognition (COMPROVADO)
        feature_list = [attn_out]
        
        if self.gru_enabled:
            feature_list.append(gru_out)
        
        if self.pattern_recognition:
            micro_patterns = self.micro_pattern_detector(attn_out)
            macro_patterns = self.macro_pattern_detector(attn_out)
            
            # üöÄ MELHORIA: Advanced Pattern Memory (multi-horizonte)
            if self.enable_advanced_memory and self.training:
                current_patterns = torch.cat([micro_patterns, macro_patterns], dim=-1)
                
                # Atualizar mem√≥rias multi-horizonte
                self._update_multi_horizon_memory(current_patterns)
                
                # Usar attention cruzada com mem√≥rias
                enhanced_patterns = self._apply_multi_horizon_attention(current_patterns)
                
                # Separar de volta
                pattern_dim = micro_patterns.shape[-1]
                micro_patterns = enhanced_patterns[:, :pattern_dim]
                macro_patterns = enhanced_patterns[:, pattern_dim:]
            
            feature_list.extend([micro_patterns, macro_patterns])
        
        # 5. Feature Fusion (COMPROVADO)
        fused_features = torch.cat(feature_list, dim=-1)
        fused_features = self.feature_fusion(fused_features)
        
        # üöÄ V5: PREPARAR COMPONENTES INTELIGENTES PARA ENTRY HEAD ULTRA-ESPECIALIZADA
        intelligent_components = {}
        
        # Temporal Horizon
        if self.enable_temporal_horizon and extra_info and 'horizon' in extra_info:
            intelligent_components['horizon_embedding'] = self.horizon_embedding(extra_info['horizon'])
        
        # Multi-Timeframe
        if self.enable_multi_timeframe and extra_info and 'multi_timeframe' in extra_info:
            intelligent_components['timeframe_fusion'] = self.timeframe_fusion(extra_info['multi_timeframe'])
        
        # Dynamic Risk
        if self.enable_dynamic_risk and extra_info and 'risk_state' in extra_info:
            intelligent_components['risk_embedding'] = self.risk_embedding(extra_info['risk_state'])
        
        # Market Regime
        if self.enable_regime_intelligence and extra_info and 'regime' in extra_info:
            intelligent_components['regime_embedding'] = self.regime_embedding(extra_info['regime'])
        
        # Pattern Memory
        if self.enable_advanced_memory and self.training:
            # Concatenar mem√≥rias multi-horizonte
            pattern_memory_concat = torch.cat([
                enhanced_patterns,
                self.memory_1h.mean(dim=0).unsqueeze(0).expand(enhanced_patterns.shape[0], -1),
                self.memory_4h.mean(dim=0).unsqueeze(0).expand(enhanced_patterns.shape[0], -1),
                self.memory_48h.mean(dim=0).unsqueeze(0).expand(enhanced_patterns.shape[0], -1)
            ], dim=-1)
            
            intelligent_components['pattern_memory'] = pattern_memory_concat
        
        # Predictive Lookahead
        if self.enable_lookahead:
            intelligent_components['lookahead'] = self.lookahead_head(projected_features)
        
        # üöÄ V5: APLICAR ENTRY HEAD SIMPLIFICADA - GARANTIA TOTAL
        if self.enable_ultra_specialized_entry and isinstance(self.entry_head, SimplifiedUltraEntryHead48h):
            # üéØ GARANTIA ABSOLUTA: Entry Head Simplificada SEMPRE ativa
            print(f"üöÄ V5 ENTRY HEAD SIMPLIFICADA ATIVA - Step {getattr(self, '_debug_step_counter', 0)}")
            entry_output = self.entry_head(fused_features, intelligent_components)
            entry_features = entry_output['entry_decision']
            
            # Debug para confirmar funcionamento
            if not hasattr(self, '_debug_step_counter'):
                self._debug_step_counter = 0
            self._debug_step_counter += 1
            
            if self._debug_step_counter % 1000 == 0:
                print(f"üéØ V5 CONFIRMA√á√ÉO: Entry Head Simplificada processou {self._debug_step_counter} steps")
                print(f"   Scores ativos: {list(entry_output.get('scores', {}).keys())}")
                print(f"   Gates ativos: {list(entry_output.get('gates', {}).keys())}")
                print(f"   Score weights: {entry_output.get('score_weights', 'N/A')}")
            
            # Garantir que entry_features seja 2D e tenha dimens√£o correta
            while entry_features.dim() > 2:
                entry_features = entry_features.squeeze(1)
            # Garantir que seja exatamente 64 dimens√µes para combinar com management_head
            if entry_features.shape[-1] != 64:
                if not hasattr(self, 'entry_projector'):
                    self.entry_projector = nn.Linear(
                        entry_features.shape[-1], 64
                    ).to(entry_features.device)
                entry_features = self.entry_projector(entry_features)
        else:
            # üö® ALERTA: Entry Head Simplificada N√ÉO EST√Å ATIVA!
            print(f"üö® SABOTAGEM DETECTADA: Entry Head Simplificada DESABILITADA!")
            print(f"   enable_ultra_specialized_entry: {self.enable_ultra_specialized_entry}")
            print(f"   entry_head type: {type(self.entry_head)}")
            print(f"   üîß FOR√áANDO ATIVA√á√ÉO...")
            
            # FOR√áAR ativa√ß√£o da Entry Head Simplificada
            if not self.enable_ultra_specialized_entry:
                self.enable_ultra_specialized_entry = True
                print(f"   ‚úÖ enable_ultra_specialized_entry FOR√áADO para True")
            
            # Se entry_head n√£o √© SimplifiedUltraEntryHead48h, recriar
            if not isinstance(self.entry_head, SimplifiedUltraEntryHead48h):
                print(f"   üîß RECRIANDO Entry Head Simplificada...")
                self.entry_head = SimplifiedUltraEntryHead48h(
                    base_features_dim=128,
                    lstm_hidden_size=self.lstm_hidden_size
                ).to(fused_features.device)
                print(f"   ‚úÖ Entry Head Simplificada RECRIADA")
            
            # Tentar novamente com Entry Head corrigida
            try:
                entry_output = self.entry_head(fused_features, intelligent_components)
                entry_features = entry_output['entry_decision']
                print(f"   ‚úÖ Entry Head Simplificada FUNCIONANDO ap√≥s corre√ß√£o")
            except Exception as e:
                print(f"   ‚ùå ERRO na Entry Head ap√≥s corre√ß√£o: {e}")
                # Fallback para entry head simples apenas em caso de erro cr√≠tico
                entry_features = self.entry_head(fused_features) if hasattr(self.entry_head, '__call__') else fused_features
        
        # 6. Management head (mant√©m processamento normal)
        management_features = self.management_head(fused_features)
        
        # 7. Residual Connection (COMPROVADO)
        residual = self.residual_connection(fused_features)
        
        # 8. Garantir dimens√µes compat√≠veis - FOR√áAR 2D
        while entry_features.dim() > 2:
            entry_features = entry_features.squeeze(1)
        while management_features.dim() > 2:
            management_features = management_features.squeeze(1)
        
        # 9. Garantir que ambos tenham exatamente 64 dimens√µes
        if entry_features.shape[-1] != 64:
            if not hasattr(self, 'entry_projector'):
                self.entry_projector = nn.Linear(
                    entry_features.shape[-1], 64
                ).to(entry_features.device)
            entry_features = self.entry_projector(entry_features)
        
        if management_features.shape[-1] != 64:
            if not hasattr(self, 'management_projector'):
                self.management_projector = nn.Linear(
                    management_features.shape[-1], 64
                ).to(management_features.device)
            management_features = self.management_projector(management_features)
        
        # 10. Debug - garantir que t√™m mesma forma
        if entry_features.shape != management_features.shape:
            print(f"üîß DEBUG: entry_features.shape={entry_features.shape}, management_features.shape={management_features.shape}")
            # For√ßar mesma forma
            batch_size = min(entry_features.shape[0], management_features.shape[0])
            entry_features = entry_features[:batch_size].reshape(batch_size, -1)
            management_features = management_features[:batch_size].reshape(batch_size, -1)
            
            # Garantir mesma √∫ltima dimens√£o
            if entry_features.shape[-1] != management_features.shape[-1]:
                target_dim = 64
                if entry_features.shape[-1] != target_dim:
                    if not hasattr(self, 'entry_projector'):
                        self.entry_projector = nn.Linear(
                            entry_features.shape[-1], target_dim
                        ).to(entry_features.device)
                    entry_features = self.entry_projector(entry_features)
                
                if management_features.shape[-1] != target_dim:
                    if not hasattr(self, 'management_projector'):
                        self.management_projector = nn.Linear(
                            management_features.shape[-1], target_dim
                        ).to(management_features.device)
                    management_features = self.management_projector(management_features)
        
        # 11. üé™ STRATEGIC FUSION LAYER V5 - Coordena√ß√£o Inteligente
        if (hasattr(self, 'strategic_fusion_enabled') and 
            self.strategic_fusion_enabled and 
            hasattr(self, 'strategic_fusion') and 
            self.strategic_fusion is not None):
            
            # Obter market context da Entry Head Ultra-Especializada
            try:
                if 'market_analysis' in entry_output:
                    market_context = entry_output['market_analysis']  # 4-dim market context
                else:
                    # Fallback: criar market context b√°sico dos gates
                    gates = entry_output.get('gates', {})
                    market_context = torch.stack([
                        gates.get('market', torch.tensor(0.5)),
                        gates.get('temporal', torch.tensor(0.5)), 
                        gates.get('risk', torch.tensor(0.5)),
                        gates.get('quality', torch.tensor(0.5))
                    ], dim=-1)
                    if market_context.dim() == 1:
                        market_context = market_context.unsqueeze(0).repeat(entry_features.shape[0], 1)
                
                # Strategic Fusion Layer - Coordena√ß√£o inteligente
                fusion_output = self.strategic_fusion(
                    entry_features, management_features, market_context
                )
                
                # Combinar com pesos da fusion layer
                combined_features = torch.cat([entry_features, management_features], dim=-1)
                
                # Aplicar coordena√ß√£o inteligente
                entry_weight = fusion_output['entry_weight'] 
                mgmt_weight = fusion_output['mgmt_weight']
                confidence = fusion_output['confidence']
                
                # Aplicar pesos estrat√©gicos
                combined_features = combined_features * (entry_weight + mgmt_weight) * confidence
                combined_features = combined_features + residual
                
                print(f"[V5 FUSION] Strategic coordination applied - Entry: {entry_weight.mean():.3f}, Mgmt: {mgmt_weight.mean():.3f}, Conf: {confidence.mean():.3f}")
                
            except Exception as e:
                print(f"[V5 WARNING] Strategic Fusion falhou, usando fallback: {e}")
                # Fallback para combina√ß√£o simples
                combined_features = torch.cat([entry_features, management_features], dim=-1)
                combined_features = combined_features + residual
        else:
            # Combina√ß√£o simples (compatibilidade com modelos sem fusion)
            combined_features = torch.cat([entry_features, management_features], dim=-1)
            combined_features = combined_features + residual
        
        # 12. Projetar para dimens√£o correta
        final_features = self._project_to_parent_dimension(combined_features)
        
        return final_features
    
    # üé™ STRATEGIC FUSION LAYER V5 - M√©todos de Learning
    def update_fusion_learning(self, decision_outcome, performance_metrics, market_state):
        """Atualiza aprendizado da Strategic Fusion Layer V5"""
        if (hasattr(self, 'strategic_fusion_enabled') and 
            self.strategic_fusion_enabled and 
            hasattr(self, 'strategic_fusion') and 
            self.strategic_fusion is not None):
            try:
                self.strategic_fusion.update_learning(decision_outcome, performance_metrics, market_state)
            except Exception as e:
                print(f"[V5 FUSION WARNING] Learning update failed: {e}")

    def get_fusion_diagnostics(self):
        """Retorna diagn√≥sticos da Strategic Fusion Layer V5"""
        if (hasattr(self, 'strategic_fusion_enabled') and 
            self.strategic_fusion_enabled and 
            hasattr(self, 'strategic_fusion') and 
            self.strategic_fusion is not None):
            try:
                return {
                    'fusion_status': 'active_v5',
                    'version': 'V5_Ultra_Specialized_Entry_Plus_Fusion',
                    'conflict_resolution_stats': self.strategic_fusion.conflict_resolver.get_stats(),
                    'temporal_coordination_stats': self.strategic_fusion.temporal_coordinator.get_stats(),
                    'adaptive_learning_stats': self.strategic_fusion.adaptive_learner.get_stats()
                }
            except Exception as e:
                return {'fusion_status': 'error', 'error': str(e)}
        return {'fusion_status': 'disabled', 'reason': 'model_v5_without_fusion_layer'}
    
    def _update_multi_horizon_memory(self, patterns: torch.Tensor):
        """Atualiza mem√≥rias multi-horizonte"""
        batch_size = patterns.shape[0]
        
        for i in range(batch_size):
            # Atualizar mem√≥ria 1h
            ptr_1h = int(self.memory_ptr_1h.item())
            self.memory_1h[ptr_1h] = patterns[i].detach()
            self.memory_ptr_1h[0] = (ptr_1h + 1) % self.memory_bank_size
            
            # Atualizar mem√≥ria 4h (a cada 4 updates)
            if ptr_1h % 4 == 0:
                ptr_4h = int(self.memory_ptr_4h.item())
                self.memory_4h[ptr_4h] = patterns[i].detach()
                self.memory_ptr_4h[0] = (ptr_4h + 1) % self.memory_bank_size
            
            # Atualizar mem√≥ria 48h (a cada 48 updates)
            if ptr_1h % 48 == 0:
                ptr_48h = int(self.memory_ptr_48h.item())
                self.memory_48h[ptr_48h] = patterns[i].detach()
                self.memory_ptr_48h[0] = (ptr_48h + 1) % self.memory_bank_size
    
    def _apply_multi_horizon_attention(self, current_patterns: torch.Tensor) -> torch.Tensor:
        """Aplica attention cruzada com mem√≥rias multi-horizonte"""
        enhanced_patterns = current_patterns
        
        # Attention com mem√≥ria 1h
        if hasattr(self, 'cross_attention_1h'):
            attn_1h, _ = self.cross_attention_1h(
                current_patterns.unsqueeze(1),
                self.memory_1h.unsqueeze(0).expand(current_patterns.shape[0], -1, -1),
                self.memory_1h.unsqueeze(0).expand(current_patterns.shape[0], -1, -1)
            )
            enhanced_patterns = enhanced_patterns + 0.1 * attn_1h.squeeze(1)
        
        # Attention com mem√≥ria 4h
        if hasattr(self, 'cross_attention_4h'):
            attn_4h, _ = self.cross_attention_4h(
                current_patterns.unsqueeze(1),
                self.memory_4h.unsqueeze(0).expand(current_patterns.shape[0], -1, -1),
                self.memory_4h.unsqueeze(0).expand(current_patterns.shape[0], -1, -1)
            )
            enhanced_patterns = enhanced_patterns + 0.1 * attn_4h.squeeze(1)
        
        # Attention com mem√≥ria 48h
        if hasattr(self, 'cross_attention_48h'):
            attn_48h, _ = self.cross_attention_48h(
                current_patterns.unsqueeze(1),
                self.memory_48h.unsqueeze(0).expand(current_patterns.shape[0], -1, -1),
                self.memory_48h.unsqueeze(0).expand(current_patterns.shape[0], -1, -1)
            )
            enhanced_patterns = enhanced_patterns + 0.1 * attn_48h.squeeze(1)
        
        return enhanced_patterns
    
    def _project_to_parent_dimension(self, combined_features: torch.Tensor) -> torch.Tensor:
        """Projeta para dimens√£o que a classe pai espera"""
        if combined_features.shape[-1] == self.lstm_hidden_size:
            return combined_features
        else:
            if not hasattr(self, 'dimension_projector'):
                self.dimension_projector = nn.Linear(
                    combined_features.shape[-1], 
                    self.lstm_hidden_size
                ).to(combined_features.device)
            return self.dimension_projector(combined_features)
    
    def _get_latent_from_obs(
        self,
        obs: PyTorchObs,
        lstm_states: RNNStates,
        episode_starts: torch.Tensor
    ) -> torch.Tensor:
        """M√©todo interno compat√≠vel com RecurrentPPO"""
        features = self.extract_features(obs)
        
        # Usar processamento inteligente V5
        return self._apply_intelligent_48h_processing(features)
    
    def update_entry_trade_memory(self, trade_result, trade_duration, entry_confidence, volatility, volume):
        """Atualiza mem√≥ria de trades para entry head ultra-especializada"""
        if self.enable_ultra_specialized_entry and hasattr(self.entry_head, 'update_trade_memory'):
            self.entry_head.update_trade_memory(
                trade_result, trade_duration, entry_confidence, volatility, volume
            )
    
    def predict_deterministic(self, obs: PyTorchObs, deterministic: bool = True):
        """M√©todo determin√≠stico personalizado"""
        was_training = self.training
        self.eval()
        
        try:
            self.reset_all_internal_states()
            self.force_deterministic_state()
            
            if isinstance(obs, np.ndarray):
                obs = torch.from_numpy(obs).float()
            
            if hasattr(self, 'device'):
                obs = obs.to(self.device)
            
            batch_size = obs.shape[0] if len(obs.shape) > 1 else 1
            device = obs.device
            
            with torch.no_grad():
                from stable_baselines3.common.utils import obs_as_tensor
                obs_tensor = obs_as_tensor(obs, device)
                episode_starts = torch.ones(batch_size, dtype=torch.bool, device=device)
                
                actions, values, log_probs, lstm_states = self.forward(
                    obs_tensor, 
                    lstm_states=None,
                    episode_starts=episode_starts,
                    deterministic=deterministic
                )
                
                return actions.cpu().numpy(), None
                
        finally:
            if was_training:
                self.train()
            else:
                self.eval()
    
    def _init_strategic_fusion_layer(self):
        """Inicializa Strategic Fusion Layer V5 com compatibilidade total"""
        if not STRATEGIC_FUSION_AVAILABLE:
            print("[V5 INFO] Strategic Fusion Layer n√£o dispon√≠vel - continuando sem fusion")
            self.strategic_fusion = None
            self.strategic_fusion_enabled = False
            self._fusion_initialization_attempted = True
            return
            
        try:
            # Inicializar Strategic Fusion Layer adaptativa
            self.strategic_fusion = StrategicFusionLayer(
                entry_dim=64,  # Sa√≠da da Entry Head Ultra-Especializada 
                management_dim=64,  # Sa√≠da da Management Head
                market_dim=4  # Market context do Entry Head
            )
            self.strategic_fusion_enabled = True
            self._fusion_initialization_attempted = True
            print("[V5 SUCCESS] üé™ Strategic Fusion Layer ATIVADA na V5!")
            print("[V5 INFO] Entry Head Ultra-Especializada + Strategic Fusion = SUPER POTENTE!")
            
        except Exception as e:
            # Fallback seguro
            self.strategic_fusion = None 
            self.strategic_fusion_enabled = False
            self._fusion_initialization_attempted = True
            print(f"[V5 WARNING] Strategic Fusion Layer falhou na inicializa√ß√£o: {e}")
    
    def set_deterministic_mode(self, deterministic: bool = True):
        """For√ßa modo determin√≠stico"""
        if deterministic:
            self.eval()
            self._disable_all_dropouts()
            
            for param in self.parameters():
                param.requires_grad = False
                
            if hasattr(self, 'pattern_memory') and hasattr(self, 'memory_ptr'):
                self.pattern_memory.zero_()
                self.memory_ptr.zero_()
            
            if hasattr(self, 'temporal_attention'):
                self.temporal_attention.dropout = 0.0
                
            print("üîß V5 Modo DETERMIN√çSTICO ativado")
        else:
            self.train()
            
            for param in self.parameters():
                param.requires_grad = True
                
            print("üîß V5 Modo TREINAMENTO ativado")
    
    def _disable_all_dropouts(self):
        """Desabilita todos os dropouts da rede"""
        dropout_count = 0
        
        for module in self.modules():
            if isinstance(module, nn.Dropout):
                module.p = 0.0
                module.eval()
                dropout_count += 1
        
        print(f"üîß V5: {dropout_count} camadas de Dropout desabilitadas")
    
    def force_deterministic_state(self):
        """For√ßa estado completamente determin√≠stico"""
        torch.manual_seed(42)
        torch.cuda.manual_seed(42)
        torch.cuda.manual_seed_all(42)
        
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
        self.eval()
        self._disable_all_dropouts()
        
        if hasattr(self, 'pattern_memory') and hasattr(self, 'memory_ptr'):
            self.pattern_memory.zero_()
            self.memory_ptr.zero_()
        
        if hasattr(self, 'temporal_attention'):
            self.temporal_attention.dropout = 0.0
        
        for param in self.parameters():
            param.requires_grad = False
        
        print("üîß V5 ESTADO DETERMIN√çSTICO TOTAL ativado")
    
    def reset_memory_bank(self):
        """Reseta Memory Bank para estado inicial"""
        if hasattr(self, 'pattern_memory') and hasattr(self, 'memory_ptr'):
            self.pattern_memory.zero_()
            self.memory_ptr.zero_()
        
        if hasattr(self, 'memory_1h') and hasattr(self, 'memory_ptr_1h'):
            self.memory_1h.zero_()
            self.memory_ptr_1h.zero_()
        
        if hasattr(self, 'memory_4h') and hasattr(self, 'memory_ptr_4h'):
            self.memory_4h.zero_()
            self.memory_ptr_4h.zero_()
        
        if hasattr(self, 'memory_48h') and hasattr(self, 'memory_ptr_48h'):
            self.memory_48h.zero_()
            self.memory_ptr_48h.zero_()
        
        # Reset entry head unified memory
        if self.enable_ultra_specialized_entry and hasattr(self.entry_head, 'unified_memory'):
            self.entry_head.unified_memory.zero_()
            self.entry_head.memory_ptr.zero_()
        
        print("üîß V5 Memory Bank resetado")
    
    def reset_all_internal_states(self):
        """Reseta TODOS os estados internos"""
        self.reset_memory_bank()
        
        for module in self.modules():
            if isinstance(module, (nn.LSTM, nn.GRU)):
                module.reset_parameters()
        
        for name, buffer in self.named_buffers():
            if 'running' in name or 'num_batches' in name:
                buffer.zero_()
        
        self.zero_grad()
        
        print("üîß V5 TODOS os estados internos resetados")



    
    def setup_gradient_monitoring(self, check_frequency: int = 500, log_dir: str = "gradient_logs"):
        """üîç Configurar monitoramento de gradientes"""
        try:
            from fix_twoheadv5_issues import GradientHealthMonitor
            self.gradient_monitor = GradientHealthMonitor(self, check_frequency, log_dir)
            return True
        except Exception as e:
            print(f"‚ùå Erro ao configurar monitoramento: {e}")
            return False
    
    def check_and_fix_gradients(self, current_step: int):
        """üîß Verificar e corrigir gradientes"""
        if hasattr(self, 'gradient_monitor'):
            return self.gradient_monitor.check_and_fix_gradients(current_step)
        return {}
    
    def get_gradient_health_summary(self):
        """üìä Obter resumo da sa√∫de dos gradientes"""
        if hasattr(self, 'gradient_monitor'):
            return self.gradient_monitor.get_gradient_health_summary()
        return {'status': 'not_monitored'}
    
    def save_gradient_report(self):
        """üíæ Salvar relat√≥rio de gradientes"""
        if hasattr(self, 'gradient_monitor'):
            return self.gradient_monitor.save_gradient_report()
        return ""
    
    def apply_improved_initialization(self):
        """üéØ Aplicar inicializa√ß√µes melhoradas"""
        try:
            from fix_twoheadv5_issues import ImprovedInitializer
            return ImprovedInitializer.initialize_model(self)
        except Exception as e:
            print(f"‚ùå Erro ao aplicar inicializa√ß√µes: {e}")
            return 0
def get_intelligent_v5_kwargs() -> Dict[str, Any]:
    """Retorna kwargs otimizados para TwoHeadV5Intelligent48h (SIMPLIFICADO)"""
    return {
        'features_extractor_class': TradingTransformerFeatureExtractor,
        'features_extractor_kwargs': {'features_dim': 128, 'seq_len': 10},
        'net_arch': [dict(pi=[192, 96], vf=[192, 96])],
        'lstm_hidden_size': 128,
        'n_lstm_layers': 2,
        'attention_heads': 8,
        'gru_enabled': True,
        'pattern_recognition': True,
        'enable_ultra_specialized_entry': True
    } 