"""
ğŸ§ª TESTE COMPREHENSIVO DE GRADIENTES V9

PROBLEMA: Gradient norm 0.0000 em teste simples
SOLUÃ‡ÃƒO: Teste mais realÃ­stico com loss function real
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from trading_framework.extractors.transformer_v9_daytrading import TradingTransformerV9
import gym

def test_v9_comprehensive_gradients():
    """Teste comprehensivo de gradientes V9 com cenÃ¡rio realÃ­stico"""
    
    print("ğŸ§ª TESTE COMPREHENSIVO GRADIENTES V9...")
    print("ğŸ¯ Simulando treinamento real com loss function")
    
    # Setup
    obs_space = gym.spaces.Box(low=-1, high=1, shape=(450,), dtype=np.float32)
    v9_transformer = TradingTransformerV9(observation_space=obs_space, features_dim=256)
    v9_transformer.train()
    
    # Optimizer para simular treinamento real
    optimizer = torch.optim.Adam(v9_transformer.parameters(), lr=0.001)
    
    print("ğŸ“Š Iniciando loop de treinamento simulado...")
    
    gradient_norms = []
    weight_changes = []
    
    for epoch in range(5):
        print(f"\nğŸ”„ Epoch {epoch+1}/5")
        
        # Batch realÃ­stico
        batch_size = 16
        obs = torch.randn(batch_size, 450, requires_grad=False)  # Input nÃ£o precisa gradients
        
        # Target aleatÃ³rio para simular loss real
        target = torch.randn(batch_size, 256)
        
        # Forward pass
        optimizer.zero_grad()
        output = v9_transformer(obs)
        
        # Loss function realÃ­stica
        loss = F.mse_loss(output, target)
        
        # Backward pass
        loss.backward()
        
        # Analisar gradientes ANTES do optimizer step
        input_proj_grad = v9_transformer.input_projection.weight.grad
        
        if input_proj_grad is not None:
            grad_norm = input_proj_grad.norm().item()
            grad_zeros = (input_proj_grad.abs() < 1e-8).float().mean().item() * 100
            grad_max = input_proj_grad.abs().max().item()
            grad_mean = input_proj_grad.abs().mean().item()
            
            gradient_norms.append(grad_norm)
            
            print(f"   Loss: {loss.item():.4f}")
            print(f"   Gradient norm: {grad_norm:.6f}")
            print(f"   Gradient zeros: {grad_zeros:.1f}%")
            print(f"   Gradient max: {grad_max:.6f}")
            print(f"   Gradient mean: {grad_mean:.6f}")
            
            # Weight antes do update
            weight_before = v9_transformer.input_projection.weight.data.clone()
            
            # Optimizer step
            optimizer.step()
            
            # Weight depois do update
            weight_after = v9_transformer.input_projection.weight.data
            weight_change = (weight_after - weight_before).norm().item()
            weight_changes.append(weight_change)
            
            print(f"   Weight change: {weight_change:.6f}")
            
            # Verificar health
            zeros_after = (weight_after.abs() < 1e-8).float().mean().item() * 100
            print(f"   Zeros apÃ³s update: {zeros_after:.1f}%")
            
        else:
            print("   âŒ NENHUM GRADIENT ENCONTRADO!")
            gradient_norms.append(0.0)
            weight_changes.append(0.0)
    
    # AnÃ¡lise final
    print("\n" + "="*50)
    print("ğŸ“ˆ ANÃLISE FINAL DE GRADIENTES")
    print("="*50)
    
    if len(gradient_norms) > 0:
        avg_grad_norm = np.mean(gradient_norms)
        max_grad_norm = np.max(gradient_norms)
        min_grad_norm = np.min(gradient_norms)
        
        avg_weight_change = np.mean(weight_changes)
        total_weight_change = np.sum(weight_changes)
        
        print(f"Gradient norm mÃ©dio: {avg_grad_norm:.6f}")
        print(f"Gradient norm mÃ¡ximo: {max_grad_norm:.6f}")
        print(f"Gradient norm mÃ­nimo: {min_grad_norm:.6f}")
        print(f"Weight change mÃ©dio: {avg_weight_change:.6f}")
        print(f"Weight change total: {total_weight_change:.6f}")
        
        # CritÃ©rios de sucesso
        success_criteria = []
        
        # 1. Gradient norm > threshold
        grad_healthy = avg_grad_norm > 1e-6
        success_criteria.append(grad_healthy)
        print(f"âœ… Gradients saudÃ¡veis: {'âœ…' if grad_healthy else 'âŒ'} (avg: {avg_grad_norm:.6f})")
        
        # 2. Weight changes happening
        weights_moving = avg_weight_change > 1e-8
        success_criteria.append(weights_moving)
        print(f"âœ… Weights mudando: {'âœ…' if weights_moving else 'âŒ'} (avg: {avg_weight_change:.6f})")
        
        # 3. No gradient explosion
        no_explosion = max_grad_norm < 10.0
        success_criteria.append(no_explosion)
        print(f"âœ… Sem explosÃ£o: {'âœ…' if no_explosion else 'âŒ'} (max: {max_grad_norm:.6f})")
        
        # 4. Consistent gradients
        consistent = min_grad_norm > 0
        success_criteria.append(consistent)
        print(f"âœ… Gradients consistentes: {'âœ…' if consistent else 'âŒ'} (min: {min_grad_norm:.6f})")
        
        success_rate = sum(success_criteria) / len(success_criteria)
        print(f"\nğŸ¯ SUCCESS RATE: {sum(success_criteria)}/{len(success_criteria)} ({success_rate*100:.0f}%)")
        
        if success_rate >= 0.75:
            print("ğŸ‰ GRADIENTS V9 FUNCIONANDO CORRETAMENTE!")
            return True
        else:
            print("âŒ GRADIENTS V9 AINDA COM PROBLEMAS!")
            return False
    else:
        print("âŒ NENHUM GRADIENT PROCESSADO!")
        return False

def test_v9_initialization_details():
    """Teste detalhado da inicializaÃ§Ã£o V9"""
    print("\nğŸ” TESTE DETALHADO INICIALIZAÃ‡ÃƒO V9...")
    
    obs_space = gym.spaces.Box(low=-1, high=1, shape=(450,), dtype=np.float32)
    v9_transformer = TradingTransformerV9(observation_space=obs_space, features_dim=256)
    
    # Analisar cada layer
    for name, module in v9_transformer.named_modules():
        if isinstance(module, nn.Linear):
            weights = module.weight.data
            print(f"{name:30s}: shape={str(weights.shape):15s} mean={weights.mean().item():8.4f} std={weights.std().item():.4f}")
            
            # Check special initialization
            if 'input_projection' in name:
                expected_std = np.sqrt(2.0 / (weights.shape[0] + weights.shape[1])) * 0.3  # gain=0.3
                actual_std = weights.std().item()
                print(f"{'':30s}   Expected std (gain=0.3): {expected_std:.4f}, Actual: {actual_std:.4f}")

if __name__ == "__main__":
    print("ğŸ§ª TESTE COMPREHENSIVO V9 GRADIENTS + INICIALIZAÃ‡ÃƒO")
    print("="*60)
    
    # Teste de inicializaÃ§Ã£o
    test_v9_initialization_details()
    
    # Teste comprehensivo de gradientes
    gradient_success = test_v9_comprehensive_gradients()
    
    print("\n" + "ğŸ"*20)
    if gradient_success:
        print("ğŸ‰ V9 GRADIENTS VALIDADOS COM SUCESSO!")
        print("âœ… Input projection funcionando corretamente")
        print("ğŸš€ Pronto para treinamento sem morte de neurÃ´nios!")
    else:
        print("âŒ V9 GRADIENTS AINDA PRECISAM DE AJUSTES!")
        print("ğŸ”§ Revisar implementaÃ§Ã£o dos fixes")
    print("ğŸ"*20)