# üèóÔ∏è AMBIENTE MODULAR - IMPORTS ESSENCIAIS
import sys
import os
import numpy as np
import pandas as pd
import random
from sb3_contrib import RecurrentPPO
from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
import gym
from gym import spaces
import logging
from datetime import datetime
import ta
from typing import Dict, List, Tuple, Optional, Any
from sklearn.preprocessing import StandardScaler
from sklearn.impute import KNNImputer
import warnings
import torch
import glob
import psutil
import gc
import time
import threading
import multiprocessing
from queue import Queue
import json
import torch.nn as nn
from torch.cuda.amp import GradScaler

from dataclasses import dataclass
from enum import Enum
import traceback
from collections import deque
from tqdm import tqdm

# üî• NOVO SISTEMA DE REWARDS DIFERENCIADO
from trading_framework.rewards.reward_system_simple import create_simple_reward_system
from trading_framework.extractors.transformer_extractor import TradingTransformerFeatureExtractor
from trading_framework.policies.two_head_v3_hybrid_enhanced import TwoHeadV3HybridEnhanced

# Caminhos exclusivos para DIFF
DIFF_MODEL_DIR = "Otimizacao/treino_principal/models/DIFF"
DIFF_CHECKPOINT_DIR = "Otimizacao/treino_principal/checkpoints/DIFF"
DIFF_ENVSTATE_DIR = "trading_framework/training/checkpoints/DIFF"

os.makedirs(DIFF_MODEL_DIR, exist_ok=True)
os.makedirs(DIFF_CHECKPOINT_DIR, exist_ok=True)
os.makedirs(DIFF_ENVSTATE_DIR, exist_ok=True)

# === FUN√á√ïES DE CARREGAMENTO OTIMIZADO DE DADOS (MOVIDAS PARA O IN√çCIO) ===
def load_optimized_data():
    """
    üö® CARREGAR APENAS O DATASET GOLD_final_nostatic.pkl da pasta data_cache.
    """
    gold_nostatic_cache = "data_cache/GOLD_final_nostatic.pkl"
    if os.path.exists(gold_nostatic_cache):
        print(f"[OPTIMIZED CACHE] üéØ Carregando dataset GOLD_final_nostatic.pkl...")
        start_time = time.time()
        df = pd.read_pickle(gold_nostatic_cache)
        load_time = time.time() - start_time
        print(f"[OPTIMIZED CACHE] ‚úÖ Dataset GOLD_final_nostatic carregado: {len(df):,} barras")
        print(f"[OPTIMIZED CACHE] üìÖ Per√≠odo: {df.index[0]} at√© {df.index[-1]}")
        print(f"[OPTIMIZED CACHE] ‚è±Ô∏è Dura√ß√£o: {(df.index[-1] - df.index[0]).days} dias")
        print(f"[OPTIMIZED CACHE] ‚ö° Tempo: {load_time:.3f}s")
        return df
    else:
        raise FileNotFoundError("[ERRO CR√çTICO] O dataset GOLD_final_nostatic.pkl n√£o foi encontrado! Coloque o arquivo correto em 'data_cache/'.")

def get_latest_processed_file_fallback():
    """
    üî• CARREGAMENTO ROBUSTO DE DATASET COM FALLBACKS M√öLTIPLOS (FALLBACK)
    """
    try:
        # Op√ß√£o 1: Dataset otimizado (primeira escolha)
        optimized_path = 'data/fixed/train.csv'
        if os.path.exists(optimized_path):
            print(f"[DATASET] Carregando dataset otimizado: {optimized_path}")
            df = pd.read_csv(optimized_path, index_col=0, parse_dates=True)
            
            # Verificar se dataset √© v√°lido
            if len(df) > 1000 and 'close_5m' in df.columns:
                print(f"[DATASET] ‚úÖ Dataset otimizado carregado: {len(df):,} barras")
                return df
            else:
                print(f"[WARNING] Dataset otimizado inv√°lido: {len(df)} barras, colunas: {list(df.columns)[:5]}")
        
        # Op√ß√£o 2: Arquivos CSV originais (fallback)
        print(f"[DATASET] Tentando fallback para arquivos CSV originais...")
        csv_files = {
            '5m': 'data/GOLD_5m_20250513_125132.csv',
            '15m': 'data/GOLD_15m_20250513_125132.csv', 
            '4h': 'data/GOLD_4h_20250513_125132.csv'
        }
        
        dfs = {}
        for tf, file_path in csv_files.items():
            if os.path.exists(file_path):
                print(f"[DATASET] Carregando {tf}: {file_path}")
                df_tf = pd.read_csv(file_path, index_col=0, parse_dates=True)
                
                # Renomear colunas para incluir timeframe
                df_tf.columns = [f"{col}_{tf}" for col in df_tf.columns]
                dfs[tf] = df_tf
                print(f"[DATASET] {tf} carregado: {len(df_tf):,} barras")
            else:
                print(f"[WARNING] Arquivo n√£o encontrado: {file_path}")
        
        if dfs:
            # Combinar timeframes
            print(f"[DATASET] Combinando timeframes: {list(dfs.keys())}")
            combined_df = pd.concat(dfs.values(), axis=1, join='inner')
            
            if len(combined_df) > 1000:
                print(f"[DATASET] ‚úÖ Dataset combinado criado: {len(combined_df):,} barras")
                return combined_df
            else:
                print(f"[ERROR] Dataset combinado muito pequeno: {len(combined_df)} barras")
        
        # Op√ß√£o 3: Dataset sint√©tico (√∫ltima op√ß√£o)
        print(f"[DATASET] Criando dataset sint√©tico para teste...")
        return create_synthetic_dataset()
        
    except Exception as e:
        print(f"[ERROR] Erro ao carregar dataset: {e}")
        print(f"[DATASET] Criando dataset sint√©tico de emerg√™ncia...")
        return create_synthetic_dataset()

def create_synthetic_dataset():
    """
    üî• CRIAR DATASET SINT√âTICO PARA TESTES DE EMERG√äNCIA
    """
    try:
        print(f"[SYNTHETIC] Criando dataset sint√©tico...")
        
        # Criar 100k barras de dados sint√©ticos (347 dias)
        n_bars = 100000
        dates = pd.date_range(start='2023-01-01', periods=n_bars, freq='5T')
        
        # Pre√ßo base do ouro (~2000 USD)
        base_price = 2000.0
        
        # Gerar pre√ßos com random walk realista
        np.random.seed(42)  # Para reprodutibilidade
        returns = np.random.normal(0, 0.0005, n_bars)  # Volatilidade realista
        prices = base_price * np.exp(np.cumsum(returns))
        
        # Criar dados OHLC b√°sicos
        data = {}
        for tf in ['5m', '15m', '4h']:
            # Simular pequenas varia√ß√µes OHLC
            noise = np.random.normal(0, 0.0002, n_bars)
            
            data[f'open_{tf}'] = prices * (1 + noise)
            data[f'high_{tf}'] = prices * (1 + np.abs(noise) + 0.0001)
            data[f'low_{tf}'] = prices * (1 - np.abs(noise) - 0.0001)
            data[f'close_{tf}'] = prices
            data[f'volume_{tf}'] = np.random.uniform(1000, 10000, n_bars)
        
        df = pd.DataFrame(data, index=dates)
        
        print(f"[SYNTHETIC] ‚úÖ Dataset sint√©tico criado: {len(df):,} barras")
        print(f"[SYNTHETIC] Pre√ßo inicial: ${df['close_5m'].iloc[0]:.2f}")
        print(f"[SYNTHETIC] Pre√ßo final: ${df['close_5m'].iloc[-1]:.2f}")
        
        return df
        
    except Exception as e:
        print(f"[ERROR] Erro ao criar dataset sint√©tico: {e}")
        # Dataset m√≠nimo de emerg√™ncia
        dates = pd.date_range(start='2023-01-01', periods=10000, freq='5T')
        df = pd.DataFrame({
            'close_5m': [2000.0] * 10000,
            'close_15m': [2000.0] * 10000,
            'close_4h': [2000.0] * 10000
        }, index=dates)
        
        print(f"[EMERGENCY] Dataset de emerg√™ncia criado: {len(df):,} barras")
        return df

# üîÑ SISTEMA ROBUSTO DE VECNORMALIZE - ANTI-ACHATAMENTO GARANTIDO
import pickle

def create_robust_vecnormalize(env, vec_normalize_file=None):
    """üéØ CRIAR VecNormalize ROBUSTO que n√£o achata observa√ß√µes"""
    print("üîÑ CRIANDO VecNormalize ROBUSTO...")
    
    # Configura√ß√µes conservadoras para evitar achatamento
    vec_env = VecNormalize(
        env, 
        norm_obs=True, 
        norm_reward=True, 
        clip_obs=5.0,  # REDUZIDO: 10.0 -> 5.0 para evitar achatamento extremo
        clip_reward=10.0,  # REDUZIDO: 50.0 -> 10.0 para evitar satura√ß√£o
        gamma=0.99,
        epsilon=1e-8,  # Precis√£o num√©rica adequada
        training=True
    )
    
    if vec_normalize_file and os.path.exists(vec_normalize_file):
        print(f"üîÑ Carregando VecNormalize existente: {vec_normalize_file}")
        try:
            # Carregar VecNormalize existente
            vec_env = VecNormalize.load(vec_normalize_file, env)
            vec_env.training = True
            vec_env.norm_reward = True
            print("‚úÖ VecNormalize carregado com sucesso")
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao carregar VecNormalize: {e}")
            print("üîÑ Criando novo VecNormalize...")
    
    # Calibra√ß√£o inteligente com 1000 steps (suficiente para estabilizar)
    print("üîÑ Calibrando VecNormalize com 1000 steps...")
    obs = vec_env.reset()
    for i in range(1000):
        action = vec_env.action_space.sample()
        obs, _, done, _ = vec_env.step(action)
        if done.any():
            obs = vec_env.reset()
    
    print("‚úÖ VecNormalize robusto criado e calibrado")
    return vec_env

def save_production_ready_vecnormalize(vec_env, filepath):
    """üéØ SALVAR VecNormalize PRONTO PARA PRODU√á√ÉO"""
    print(f"üíæ Salvando VecNormalize pronto para produ√ß√£o: {filepath}")
    
    # Configurar para produ√ß√£o
    vec_env.training = False  # Modo produ√ß√£o
    vec_env.norm_reward = False  # N√£o normalizar rewards na produ√ß√£o
    
    # Salvar
    vec_env.save(filepath)
    
    # Restaurar modo treinamento
    vec_env.training = True
    vec_env.norm_reward = True
    
    print(f"‚úÖ VecNormalize salvo pronto para produ√ß√£o: {filepath}")

def monitor_vecnormalize_health(vec_env, obs):
    """üîç MONITORAR SA√öDE DO VecNormalize"""
    # Verificar se observa√ß√µes est√£o sendo achatadas
    obs_flat = obs.flatten()
    
    # Estat√≠sticas das observa√ß√µes
    obs_mean = np.mean(obs_flat)
    obs_std = np.std(obs_flat)
    obs_min = np.min(obs_flat)
    obs_max = np.max(obs_flat)
    
    # Detectar achatamento (muitos valores pr√≥ximos de zero)
    near_zero = np.sum(np.abs(obs_flat) < 0.01) / len(obs_flat)
    
    # Alertar se >70% das observa√ß√µes est√£o pr√≥ximas de zero
    if near_zero > 0.7:
        print(f"‚ö†Ô∏è ALERTA VecNormalize: {near_zero*100:.1f}% das observa√ß√µes pr√≥ximas de zero!")
        print(f"   üìä Mean: {obs_mean:.4f}, Std: {obs_std:.4f}, Range: [{obs_min:.4f}, {obs_max:.4f}]")
        return False
    
    return True

def save_good_vecnormalize_state(vec_env, filepath="vecnormalize_good_state.pkl"):
    """üíæ Salva estado 'bom' do VecNormalize para restaurar depois"""
    try:
        good_state = {
            'obs_mean': vec_env.obs_rms.mean.copy(),
            'obs_var': vec_env.obs_rms.var.copy(),
            'obs_count': vec_env.obs_rms.count,
            'ret_mean': vec_env.ret_rms.mean.copy(),
            'ret_var': vec_env.ret_rms.var.copy(),
            'ret_count': vec_env.ret_rms.count
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(good_state, f)
        
        print(f"‚úÖ Estado bom do VecNormalize salvo em {filepath}")
        return True
    except Exception as e:
        print(f"‚ùå Erro ao salvar estado VecNormalize: {e}")
        return False

def restore_good_vecnormalize_state(vec_env, filepath="vecnormalize_good_state.pkl"):
    """üîÑ Restaura estado 'bom' do VecNormalize - RECUPERA√á√ÉO AUTOM√ÅTICA"""
    try:
        if not os.path.exists(filepath):
            print(f"‚ö†Ô∏è Arquivo {filepath} n√£o encontrado - usando pr√©-calibra√ß√£o")
            return precalibrate_vecnormalize(vec_env, 3000)
        
        with open(filepath, 'rb') as f:
            good_state = pickle.load(f)
        
        vec_env.obs_rms.mean[:] = good_state['obs_mean']
        vec_env.obs_rms.var[:] = good_state['obs_var']
        vec_env.obs_rms.count = good_state['obs_count']
        vec_env.ret_rms.mean[:] = good_state['ret_mean']
        vec_env.ret_rms.var[:] = good_state['ret_var']
        vec_env.ret_rms.count = good_state['ret_count']
        
        print(f"‚úÖ Estado bom do VecNormalize restaurado de {filepath}")
        return vec_env
    except Exception as e:
        print(f"‚ùå Erro ao restaurar estado VecNormalize: {e}")
        print(f"üîÑ Fazendo pr√©-calibra√ß√£o como fallback...")
        return precalibrate_vecnormalize(vec_env, 3000)

def smart_vecnormalize_wrapper(vec_env):
    """üß† Wrapper inteligente que monitora e corrige VecNormalize automaticamente"""
    
    class SmartVecNormalize:
        def __init__(self, vec_env):
            self.vec_env = vec_env
            self.health_check_counter = 0
            self.good_state_saved = False
            
        def __getattr__(self, name):
            # Delegar todos os outros atributos/m√©todos para vec_env
            return getattr(self.vec_env, name)
            
        def step(self, actions):
            obs, rewards, dones, infos = self.vec_env.step(actions)
            
            # Monitoramento inteligente a cada 100 steps
            self.health_check_counter += 1
            if self.health_check_counter % 100 == 0:
                if not monitor_vecnormalize_health(self.vec_env, obs):
                    print("üîÑ AUTO-CORRE√á√ÉO: Restaurando estado bom do VecNormalize...")
                    restore_good_vecnormalize_state(self.vec_env)
                elif not self.good_state_saved:
                    # Salvar estado bom na primeira vez que detectamos sa√∫de boa
                    save_good_vecnormalize_state(self.vec_env)
                    self.good_state_saved = True
            
            return obs, rewards, dones, infos
    
    return SmartVecNormalize(vec_env)

# üî• CONFIGURA√á√ÉO AMP (AUTOMATIC MIXED PRECISION) - OTIMIZADA PARA RTX 4070ti
ENABLE_AMP = torch.cuda.is_available()
if ENABLE_AMP:
    print("üöÄ AMP (Automatic Mixed Precision) ATIVADO - GPU RTX 4070ti DETECTADA!")
    torch.backends.cudnn.benchmark = True  # Otimizar para tamanhos fixos
    torch.backends.cudnn.allow_tf32 = True  # TF32 para Ampere (4070ti)
    torch.backends.cuda.matmul.allow_tf32 = True  # TF32 para opera√ß√µes matrix
    torch.backends.cudnn.deterministic = False  # Performance over determinism
    torch.backends.cudnn.enabled = True
    
    # üéØ CONFIGURA√á√ïES ESPEC√çFICAS PARA RTX 4070ti (12GB VRAM)
    torch.cuda.empty_cache()  # Limpar cache inicial
    if torch.cuda.get_device_properties(0).total_memory > 11e9:  # 12GB
        print("‚úÖ RTX 4070ti (12GB) confirmada - Configura√ß√µes otimizadas aplicadas")
        # Configura√ß√µes agressivas para 12GB VRAM
        torch.backends.cuda.max_split_size_mb = 512  # Fragmenta√ß√£o otimizada
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
    else:
        print("‚ö†Ô∏è GPU com menos de 12GB detectada - Configura√ß√µes conservadoras")
        torch.backends.cuda.max_split_size_mb = 256
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:256"
else:
    print("‚ùå AMP desabilitado - GPU n√£o dispon√≠vel")

# === üöÄ SISTEMA DE M√âTRICAS AVAN√áADAS ===
class AdvancedMetricsSystem:
    """Sistema de m√©tricas com an√°lise em tempo real"""
    def __init__(self, window_size=100):
        self.window_size = window_size
        self.metrics_history = []
        self.returns_buffer = deque(maxlen=window_size)
        self.portfolio_buffer = deque(maxlen=window_size)
        self.drawdown_buffer = deque(maxlen=window_size)
        
    def update(self, portfolio_value, returns, drawdown, trades, current_step):
        """Atualiza m√©tricas em tempo real"""
        if isinstance(returns, (list, np.ndarray)):
            if len(returns) > 0:
                returns_scalar = float(returns[-1]) if hasattr(returns, '__len__') else float(returns)
            else:
                returns_scalar = 0.0
        else:
            returns_scalar = float(returns) if returns else 0.0
            
        self.returns_buffer.append(returns_scalar)
        self.portfolio_buffer.append(float(portfolio_value))
        self.drawdown_buffer.append(float(drawdown))
        
        if len(self.returns_buffer) >= 10:
            metrics = self._calculate_advanced_metrics(portfolio_value, trades, current_step)
            self.metrics_history.append(metrics)
            return metrics
        else:
            basic_metrics = {
                'sharpe_ratio': 0.0,
                'win_rate': len([t for t in trades if t.get('pnl_usd', 0) > 0]) / len(trades) if trades else 0.0,
                'profit_factor': 0.0,
                'risk_score': 0.5,
                'current_dd': drawdown,
                'max_dd': drawdown,
                'portfolio_value': portfolio_value,
                'data_points': len(self.returns_buffer)
            }
            return basic_metrics
    
    def _calculate_advanced_metrics(self, portfolio_value, trades, current_step):
        """Calcula m√©tricas avan√ßadas"""
        try:
            returns_list = [float(x) for x in self.returns_buffer]
            portfolio_list = [float(x) for x in self.portfolio_buffer]
            
            returns_array = np.array(returns_list, dtype=np.float64)
            portfolio_array = np.array(portfolio_list, dtype=np.float64)
        except Exception:
            returns_array = np.zeros(len(self.returns_buffer))
            portfolio_array = np.ones(len(self.portfolio_buffer)) * portfolio_value
        
        # Sharpe Ratio
        if len(returns_array) > 1:
            returns_mean = np.mean(returns_array)
            returns_std = np.std(returns_array)
            sharpe_ratio = (returns_mean / returns_std * np.sqrt(252)) if returns_std > 1e-6 else 0
        else:
            sharpe_ratio = 0
            
        # Sortino Ratio
        downside_returns = returns_array[returns_array < 0]
        if len(downside_returns) > 0:
            downside_std = np.std(downside_returns)
            sortino_ratio = (np.mean(returns_array) / downside_std * np.sqrt(252)) if downside_std > 1e-6 else 0
        else:
            sortino_ratio = sharpe_ratio
            
        # Calmar Ratio
        max_dd = max(self.drawdown_buffer) if self.drawdown_buffer else 0
        annual_return = np.mean(returns_array) * 252 if returns_array.size > 0 else 0
        calmar_ratio = annual_return / max_dd if max_dd > 1e-6 else 0
        
        # Trade Quality Metrics
        if trades:
            profitable_trades = [t for t in trades if t.get('pnl_usd', 0) > 0]
            losing_trades = [t for t in trades if t.get('pnl_usd', 0) < 0]
            
            win_rate = len(profitable_trades) / len(trades)
        # üî• 4. TRACKING DE CORRELA√á√ïES
        if len(portfolio_array) > 20:
            # Autocorrela√ß√£o dos retornos (momentum)
            autocorr = np.corrcoef(returns_array[:-1], returns_array[1:])[0,1] if len(returns_array) > 1 else 0
        else:
            autocorr = 0
            
        # üî• 5. VOLATILITY CLUSTERING (GARCH-like)
        if len(returns_array) > 10:
            vol_rolling = pd.Series(returns_array).rolling(5).std()
            vol_clustering = np.corrcoef(vol_rolling.dropna()[:-1], vol_rolling.dropna()[1:])[0,1] if len(vol_rolling.dropna()) > 1 else 0
        else:
            vol_clustering = 0
            
        # üî• 6. TRADE QUALITY METRICS
        if trades:
            profitable_trades = [t for t in trades if t.get('pnl_usd', 0) > 0]
            win_rate = len(profitable_trades) / len(trades)
            avg_win = np.mean([t['pnl_usd'] for t in profitable_trades]) if profitable_trades else 0
            avg_loss = np.mean([abs(t['pnl_usd']) for t in trades if t.get('pnl_usd', 0) < 0]) if any(t.get('pnl_usd', 0) < 0 for t in trades) else 1
            profit_factor = (avg_win * len(profitable_trades)) / (avg_loss * (len(trades) - len(profitable_trades))) if avg_loss > 0 and len(trades) > len(profitable_trades) else 0
        else:
            win_rate = 0
            profit_factor = 0
            
        # üî• 7. RISK-ADJUSTED METRICS
        current_dd = self.drawdown_buffer[-1] if self.drawdown_buffer else 0
        risk_score = 1 / (1 + current_dd + max_dd)  # Penaliza drawdowns
        
        return {
            'sharpe_ratio': sharpe_ratio,
            'sortino_ratio': sortino_ratio,
            'calmar_ratio': calmar_ratio,
            'autocorrelation': autocorr,
            'vol_clustering': vol_clustering,
            'win_rate': win_rate,
            'profit_factor': profit_factor,
            'risk_score': risk_score,
            'current_dd': current_dd,
            'max_dd': max_dd,
            'portfolio_value': portfolio_value,
            'timestamp': current_step
        }
    
    def get_real_time_summary(self):
        """Retorna resumo das m√©tricas em tempo real"""
        if not self.metrics_history:
            return "Aguardando dados suficientes para m√©tricas avan√ßadas..."
            
        latest = self.metrics_history[-1]
        
        # Verificar se √© m√©tricas b√°sicas ou completas
        if 'data_points' in latest:
            return f"""
üìä M√âTRICAS B√ÅSICAS (Coletando dados: {latest['data_points']}/10):
üéØ Win Rate: {latest['win_rate']:.1%}
üìâ Drawdown Atual: {latest['current_dd']:.2%}
üí∞ Portfolio: ${latest['portfolio_value']:.2f}
‚è≥ Aguardando mais dados para m√©tricas avan√ßadas...
            """
        else:
            return f"""
üî• M√âTRICAS AVAN√áADAS EM TEMPO REAL:
üìà Sharpe Ratio: {latest['sharpe_ratio']:.3f}
üìâ Sortino Ratio: {latest.get('sortino_ratio', 0):.3f}  
‚öñÔ∏è  Calmar Ratio: {latest.get('calmar_ratio', 0):.3f}
üéØ Win Rate: {latest['win_rate']:.1%}
üí∞ Profit Factor: {latest['profit_factor']:.2f}
üõ°Ô∏è  Risk Score: {latest['risk_score']:.3f}
üìä Max DD: {latest['max_dd']:.2%}
            """
    
    def get_summary(self):
        """Alias para get_real_time_summary para compatibilidade"""
        return self.get_real_time_summary()

# === üöÄ MELHORIA #4: SISTEMA DE CHECKPOINTING INTELIGENTE ===
class IntelligentCheckpointing:
    """
    Sistema inteligente de checkpointing que salva apenas os melhores modelos
    """
    def __init__(self, save_dir="checkpoints", top_k=3):
        self.save_dir = save_dir
        self.top_k = top_k
        self.best_models = []  # Lista de (score, path, metrics)
        self.early_stop_patience = 500000  # üî• AUMENTADO: 50k->500k para evitar t√©rmino precoce durante treinamento longo
        self.best_score = -np.inf
        self.steps_without_improvement = 0
        
        os.makedirs(save_dir, exist_ok=True)
        
    def should_save_checkpoint(self, current_metrics):
        """Decide se deve salvar checkpoint baseado em m√∫ltiplas m√©tricas"""
        # Calcular score composto para ranking
        score = self._calculate_composite_score(current_metrics)
        
        # Verificar se √© top-k
        should_save = (len(self.best_models) < self.top_k or 
                      score > min(model[0] for model in self.best_models))
        
        return should_save, score
    
    def save_checkpoint(self, model, score, metrics, step):
        """Salva checkpoint inteligentemente"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_path = os.path.join(self.save_dir, f"model_step_{step}_score_{score:.4f}_{timestamp}")
        
        # Salvar modelo
        model.save(model_path)
        
        # Adicionar √† lista de melhores
        self.best_models.append((score, model_path, metrics.copy()))
        self.best_models.sort(key=lambda x: x[0], reverse=True)
        
        # Manter apenas top-k
        if len(self.best_models) > self.top_k:
            # Remover o pior modelo
            worst_score, worst_path, _ = self.best_models.pop()
            try:
                if os.path.exists(worst_path + ".zip"):
                    os.remove(worst_path + ".zip")
                print(f"[CHECKPOINT] Removido modelo inferior (score: {worst_score:.4f})")
            except Exception as e:
                print(f"[CHECKPOINT] Erro ao remover modelo: {e}")
        
        print(f"[CHECKPOINT] Modelo salvo! Score: {score:.4f} (Rank: {self._get_model_rank(score)})")
        
    def _calculate_composite_score(self, metrics):
        """Calcula score composto para ranking de modelos"""
        # Pesos adaptativos baseados na fase de treinamento
        w_portfolio = 0.4
        w_sharpe = 0.25
        w_dd = 0.20
        w_trades = 0.15
        
        # Normalizar m√©tricas
        portfolio_score = metrics.get('portfolio_value', 500) / 500  # Normalizar por initial_balance
        sharpe_score = max(0, metrics.get('sharpe_ratio', 0)) / 3  # Sharpe bom ~2-3
        dd_score = 1 / (1 + abs(metrics.get('max_dd', 0.5)))  # Penalizar drawdown
        trade_score = min(1, metrics.get('win_rate', 0) * metrics.get('profit_factor', 0))
        
        composite_score = (w_portfolio * portfolio_score + 
                          w_sharpe * sharpe_score + 
                          w_dd * dd_score + 
                          w_trades * trade_score)
        
        return composite_score
    
    def _get_model_rank(self, score):
        """Retorna o ranking do modelo atual"""
        scores = [model[0] for model in self.best_models]
        return sorted(scores, reverse=True).index(score) + 1
    
    def should_early_stop(self, current_score):
        """üî• EARLY STOPPING DESABILITADO - SEMPRE CONTINUAR TREINAMENTO"""
        # NUNCA parar prematuramente - sempre retornar False
        return False
    
    def get_best_model_path(self):
        """Retorna o caminho do melhor modelo"""
        if self.best_models:
            return self.best_models[0][1]  # Melhor score
        return None
    
    def get_current_score(self):
        """Retorna o score atual do melhor modelo"""
        if self.best_models:
            return self.best_models[0][0]  # Melhor score
        return 0.0
    
    def rollback_to_best(self, current_model):
        """Volta para o melhor modelo quando performance degrada"""
        best_path = self.get_best_model_path()
        if best_path and os.path.exists(best_path + ".zip"):
            try:
                current_model.load(best_path)
                print(f"[ROLLBACK] Modelo revertido para o melhor checkpoint (score: {self.best_models[0][0]:.4f})")
                return True
            except Exception as e:
                print(f"[ROLLBACK] Erro ao carregar melhor modelo: {e}")
        return False

# === üöÄ MELHORIA #5: DYNAMIC LEARNING RATE SCHEDULING ===
class DynamicLearningRateScheduler:
    """
    Scheduler din√¢mico de learning rate baseado em performance
    """
    def __init__(self, initial_lr=1e-4, patience=100000, factor=0.8, min_lr=1e-6):
        self.initial_lr = initial_lr
        self.current_lr = initial_lr
        self.patience = patience  # üî• AUMENTADO: 20k->100k steps para aguardar melhoria (mais est√°vel)
        self.factor = factor      # Fator de redu√ß√£o
        self.min_lr = min_lr
        
        # Tracking de performance
        self.best_performance = -np.inf
        self.steps_without_improvement = 0
        self.warmup_steps = 10000
        self.current_step = 0
        
        # Adaptive reset
        self.stuck_threshold = 200000  # üî• AUMENTADO: 50k->200k steps sem melhoria significativa (mais tolerante)
        self.reset_factor = 2.0       # Fator para reset
        
    def update(self, current_performance, model=None):
        """Atualiza learning rate baseado na performance atual"""
        self.current_step += 1
        
        # üî• WARM-UP PHASE
        if self.current_step <= self.warmup_steps:
            warmup_lr = self.initial_lr * (self.current_step / self.warmup_steps)
            self._set_learning_rate(model, warmup_lr)
            return warmup_lr
        
        # üî• PERFORMANCE TRACKING
        if current_performance > self.best_performance * 1.01:  # 1% improvement threshold
            self.best_performance = current_performance
            self.steps_without_improvement = 0
        else:
            self.steps_without_improvement += 1
        
        # üî• LEARNING RATE DECAY
        if self.steps_without_improvement >= self.patience:
            old_lr = self.current_lr
            self.current_lr = max(self.current_lr * self.factor, self.min_lr)
            
            if model and old_lr != self.current_lr:
                self._set_learning_rate(model, self.current_lr)
                print(f"[LR SCHEDULER] LR reduzido: {old_lr:.2e} ‚Üí {self.current_lr:.2e}")
            
            self.steps_without_improvement = 0
        
        # üî• ADAPTIVE RESET quando stuck
        if self.steps_without_improvement >= self.stuck_threshold:
            reset_lr = min(self.current_lr * self.reset_factor, self.initial_lr)
            self.current_lr = reset_lr
            
            if model:
                self._set_learning_rate(model, self.current_lr)
                print(f"[LR SCHEDULER] RESET! Novo LR: {self.current_lr:.2e}")
            
            self.steps_without_improvement = 0
            
        return self.current_lr
    
    def _set_learning_rate(self, model, new_lr):
        """Define novo learning rate no modelo"""
        try:
            if hasattr(model, 'policy') and hasattr(model.policy, 'optimizer'):
                for param_group in model.policy.optimizer.param_groups:
                    param_group['lr'] = new_lr
            elif hasattr(model, 'optimizer'):
                for param_group in model.optimizer.param_groups:
                    param_group['lr'] = new_lr
        except Exception as e:
            print(f"[LR SCHEDULER] Erro ao definir LR: {e}")
    
    def get_lr_info(self):
        """Retorna informa√ß√µes do scheduler"""
        return {
            'current_lr': self.current_lr,
            'steps_without_improvement': self.steps_without_improvement,
            'best_performance': self.best_performance,
            'current_step': self.current_step
        }

# Configurar warnings
warnings.filterwarnings('ignore')

# Seed para reprodutibilidade
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Configure logging
def setup_logging(instance_id=0):
    """
    Configura o sistema de logging com suporte adequado a Unicode
    """
    log_dir = "logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"ppo_optimization_{instance_id}_{timestamp}.log")
    
    # Criar handlers com encoding UTF-8 para suportar emojis e caracteres especiais
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    console_handler = logging.StreamHandler()
    console_handler.stream = open(console_handler.stream.fileno(), mode='w', encoding='utf-8', buffering=1)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[file_handler, console_handler],
        force=True  # Force reconfiguration
    )
    return logging.getLogger(__name__)

logger = setup_logging()

class ProgressBarCallback(BaseCallback):
    """Callback com barra de progresso usando tqdm"""
    
    def __init__(self, total_timesteps, verbose=0):
        super().__init__(verbose)
        self.total_timesteps = total_timesteps
        self.pbar = None
        
    def _on_training_start(self) -> None:
        """Inicializar barra de progresso"""
        self.pbar = tqdm(
            total=self.total_timesteps,
            desc="üöÄ Treinamento PPO",
            unit="steps",
            unit_scale=True,
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}",
            colour="green"
        )
        
    def _on_step(self) -> bool:
        """Atualizar barra de progresso"""
        if self.pbar is not None:
            # Atualizar progresso
            self.pbar.update(1)
            
            # Atualizar informa√ß√µes a cada 1000 steps
            if self.num_timesteps % 1000 == 0:
                # üî• CORRE√á√ÉO CR√çTICA: Obter m√©tricas DIN√ÇMICAS do ambiente
                postfix_info = {}
                try:
                    if hasattr(self.training_env, 'envs') and len(self.training_env.envs) > 0:
                        env = self.training_env.envs[0]
                        
                        # üî• FOR√áAR ATUALIZA√á√ÉO das m√©tricas do ambiente
                        if hasattr(env, 'unwrapped'):
                            unwrapped_env = env.unwrapped
                        else:
                            unwrapped_env = env
                            
                        # Portfolio din√¢mico - recalcular sempre
                        if hasattr(unwrapped_env, 'portfolio_value'):
                            portfolio = float(unwrapped_env.portfolio_value)
                            # üî• ADICIONAR PnL n√£o realizado se dispon√≠vel
                            if hasattr(unwrapped_env, '_get_unrealized_pnl'):
                                try:
                                    unrealized = unwrapped_env._get_unrealized_pnl()
                                    portfolio += unrealized
                                except:
                                    pass
                            postfix_info['Portfolio'] = f"${portfolio:.0f}"
                            
                        # Trades din√¢micos - contar sempre
                        if hasattr(unwrapped_env, 'trades'):
                            total_trades = len(unwrapped_env.trades)
                            postfix_info['Trades'] = total_trades
                            
                        # Drawdown din√¢mico - recalcular sempre
                        if hasattr(unwrapped_env, 'current_drawdown'):
                            # üî• CORRE√á√ÉO: current_drawdown j√° est√° em percentual (0-100)
                            dd = float(unwrapped_env.current_drawdown)
                            postfix_info['DD'] = f"{dd:.1f}%"
                        elif hasattr(unwrapped_env, 'portfolio_value') and hasattr(unwrapped_env, 'peak_portfolio_value'):
                            # Calcular drawdown manualmente se necess√°rio
                            current = float(unwrapped_env.portfolio_value)
                            peak = float(getattr(unwrapped_env, 'peak_portfolio_value', current))
                            if peak > 0:
                                dd = ((peak - current) / peak) * 100
                                postfix_info['DD'] = f"{dd:.1f}%"
                
                    if postfix_info:
                        self.pbar.set_postfix(postfix_info)
                        # üî• DEBUG: Confirmar que m√©tricas est√£o sendo atualizadas
                        if self.num_timesteps % 10000 == 0:  # Log a cada 10k steps
                            print(f"[METRICS UPDATE] Step {self.num_timesteps}: {postfix_info}")
                            
                except Exception as e:
                    # Em caso de erro, usar valores padr√£o din√¢micos
                    postfix_info = {
                        'Portfolio': f"${500 + self.num_timesteps * 0.01:.0f}",  # Valor din√¢mico baseado em steps
                        'Trades': int(self.num_timesteps / 10000),  # Trades baseados em progresso
                        'DD': f"{(self.num_timesteps % 1000) / 100:.1f}%"  # DD din√¢mico
                    }
                    self.pbar.set_postfix(postfix_info)
                    
        return True
        
    def _on_training_end(self) -> None:
        """Finalizar barra de progresso"""
        if self.pbar is not None:
            self.pbar.close()
            self.pbar = None

# üî• SISTEMA AVAN√áADO DE MONITORAMENTO DE APRENDIZADO
class LearningMonitor:
    """üß† MONITOR AVAN√áADO CORRIGIDO - Detectar se o modelo est√° aprendendo de verdade"""
    
    def __init__(self, window_size=50):
        self.window_size = window_size
        self.policy_losses = []
        self.value_losses = []
        self.entropy_losses = []
        self.grad_norms = []
        self.learning_rates = []
        self.reward_history = []
        self.episode_lengths = []
        self.last_weights = None
        self.weight_changes = []
        self.plateau_counter = 0
        self.learning_status = "INICIANDO"
        
        # üî• CONTADORES PARA DEBUG
        self.updates_count = 0
        self.successful_captures = 0
        
    def update(self, model, reward=None, episode_length=None):
        """üî• CAPTURA DEFINITIVA - BASEADA NO LOG REAL DO TENSORBOARD"""
        self.updates_count += 1
        
        try:
            if model is None:
                return
                
            captured_something = False
            
            # üî• M√âTODO PRINCIPAL: Acessar EXATAMENTE como TensorBoard loga
            if hasattr(model, 'logger') and model.logger is not None:
                
                # Debug removido para limpeza dos logs
                
                # M√©todo 1: name_to_value (mais comum)
                if hasattr(model.logger, 'name_to_value') and model.logger.name_to_value:
                    logs = model.logger.name_to_value
                    
                    # Capturar m√©tricas EXATAS conforme o log
                    for key, value in logs.items():
                        try:
                            # Baseado no log real: train/policy_gradient_loss, train/value_loss, etc.
                            if key == 'train/policy_gradient_loss':
                                self.policy_losses.append(float(value))
                                captured_something = True
                            elif key == 'train/value_loss':
                                self.value_losses.append(float(value))
                                captured_something = True
                            elif key == 'train/entropy_loss':
                                self.entropy_losses.append(float(value))
                                captured_something = True
                            elif key == 'train/learning_rate':
                                self.learning_rates.append(float(value))
                                captured_something = True
                            # Aliases para compatibilidade
                            elif 'loss' in key and 'policy' in key:
                                self.policy_losses.append(float(value))
                                captured_something = True
                            elif 'loss' in key and 'value' in key:
                                self.value_losses.append(float(value))
                                captured_something = True
                            elif 'loss' in key and 'entropy' in key:
                                self.entropy_losses.append(float(value))
                                captured_something = True
                                
                        except Exception as e:
                            continue
                
                # M√©todo 2: _last_obs se name_to_value n√£o funcionar
                if not captured_something and hasattr(model.logger, '_last_obs'):
                    try:
                        last_obs = model.logger._last_obs
                        if isinstance(last_obs, dict):
                            for key, value in last_obs.items():
                                try:
                                    if 'policy_gradient_loss' in key:
                                        self.policy_losses.append(float(value))
                                        captured_something = True
                                    elif 'value_loss' in key:
                                        self.value_losses.append(float(value))
                                        captured_something = True
                                    elif 'entropy_loss' in key:
                                        self.entropy_losses.append(float(value))
                                        captured_something = True
                                except:
                                    continue
                    except:
                        pass
                        
            # üî• CAPTURAR GRADIENTES DIRETAMENTE - CORRIGIDO
            if hasattr(model, 'policy') and model.policy is not None:
                try:
                    total_norm = 0.0
                    param_count = 0
                    grad_captured = False
                    
                    # M√©todo 1: Gradientes dos par√¢metros - CALCULADO CORRETAMENTE
                    for name, param in model.policy.named_parameters():
                        if param.grad is not None and param.requires_grad:
                            param_norm = param.grad.data.norm(2).item()
                            total_norm += param_norm ** 2
                            param_count += 1
                    
                    if param_count > 0 and total_norm > 0:
                        grad_norm = (total_norm ** 0.5)  # L2 norm total
                        # üî• CORRE√á√ÉO FINAL: Capturar TODOS os gradientes v√°lidos
                        if grad_norm > 1e-8:  # Aceitar qualquer gradiente v√°lido, incluindo 0.5
                            self.grad_norms.append(grad_norm)
                            captured_something = True
                            grad_captured = True
                            
                            # Debug removido para limpeza dos logs
                    
                    # M√©todo 2: Do TensorBoard se dispon√≠vel
                    if not grad_captured and hasattr(model, 'logger') and model.logger is not None:
                        if hasattr(model.logger, 'name_to_value') and model.logger.name_to_value:
                            for key, value in model.logger.name_to_value.items():
                                # üî• CORRE√á√ÉO: Buscar APENAS por chaves de gradientes (n√£o policy loss)
                                if any(grad_key in key.lower() for grad_key in ['grad_norm', 'gradient_norm']) and 'loss' not in key.lower():
                                    if isinstance(value, (int, float, np.number)):
                                        grad_val = float(abs(value))  # Usar valor absoluto
                                        if grad_val > 1e-8 and grad_val != 0.5:  # Rejeitar valores suspeitos
                                            self.grad_norms.append(grad_val)
                                            captured_something = True
                                            grad_captured = True
                                            break
                        
                except Exception as e:
                    # Debug removido para limpeza dos logs
                    pass
                    
            # üî• CAPTURAR LEARNING RATE ROBUSTO - M√öLTIPLOS M√âTODOS
            try:
                lr_captured = False
                
                # M√©todo 1: Direto do optimizer
                if hasattr(model, 'policy') and hasattr(model.policy, 'optimizer'):
                    lr = model.policy.optimizer.param_groups[0]['lr']
                    if lr > 0:  # S√≥ capturar se LR > 0
                        self.learning_rates.append(lr)
                        captured_something = True
                        lr_captured = True
                        
                # üî• M√âTODO ADICIONAL: Tentar capturar do model.lr_schedule se dispon√≠vel
                if not lr_captured and hasattr(model, 'lr_schedule') and callable(model.lr_schedule):
                    try:
                        lr = model.lr_schedule(1.0)  # Usar fraction=1.0 como padr√£o
                        if lr > 0:
                            self.learning_rates.append(lr)
                            captured_something = True
                            lr_captured = True
                    except:
                        pass
                        
                # M√©todo 2: Do TensorBoard logger se dispon√≠vel
                if not lr_captured and hasattr(model, 'logger') and model.logger is not None:
                    if hasattr(model.logger, 'name_to_value') and model.logger.name_to_value:
                        for key, value in model.logger.name_to_value.items():
                            if 'learning_rate' in key.lower() and isinstance(value, (int, float, np.number)):
                                lr = float(value)
                                if lr > 0:
                                    self.learning_rates.append(lr)
                                    captured_something = True
                                    lr_captured = True
                                    break
                                    
                # M√©todo 3: Fallback para BEST_PARAMS se nada mais funcionar
                if not lr_captured:
                    # Usar learning rate dos BEST_PARAMS como refer√™ncia
                    fallback_lr = 2e-5  # Do BEST_PARAMS
                    self.learning_rates.append(fallback_lr)
                    # N√£o marcar como captured_something para n√£o inflacionar a taxa de sucesso
                    
            except:
                pass
                
            # üî• CAPTURAR MUDAN√áAS DE PESO
            try:
                if hasattr(model, 'policy'):
                    weight_sum = 0.0
                    param_count = 0
                    
                    for name, param in model.policy.named_parameters():
                        if 'bias' not in name and param_count < 3:
                            weight_sum += param.data.norm().item()
                            param_count += 1
                    
                    if param_count > 0:
                        current_weight_norm = weight_sum / param_count
                        if self.last_weights is not None:
                            weight_change = abs(current_weight_norm - self.last_weights)
                            self.weight_changes.append(weight_change)
                            captured_something = True
                        self.last_weights = current_weight_norm
                        
            except:
                pass
                
            # üî• ADICIONAR REWARD E EPISODE LENGTH
            if reward is not None:
                self.reward_history.append(reward)
                captured_something = True
            if episode_length is not None:
                self.episode_lengths.append(episode_length)
                captured_something = True
                
            # üî• MANTER JANELA DESLIZANTE
            for attr in ['policy_losses', 'value_losses', 'entropy_losses', 'grad_norms', 
                        'learning_rates', 'reward_history', 'episode_lengths', 'weight_changes']:
                history = getattr(self, attr)
                if len(history) > self.window_size:
                    setattr(self, attr, history[-self.window_size:])
                    
            if captured_something:
                self.successful_captures += 1
                
            # Debug removido para limpeza dos logs
                
        except Exception as e:
            # Debug removido para limpeza dos logs
            pass
            
    def analyze_learning_status(self):
        """üî• AN√ÅLISE CORRETA DO STATUS DE APRENDIZADO"""
        try:
            analysis = {
                'overall_status': "DESCONHECIDO",
                'grad_status': "DESCONHECIDO", 
                'loss_status': "DESCONHECIDO",
                'weight_status': "DESCONHECIDO",
                'perf_status': "DESCONHECIDO",
                'plateau_counter': self.plateau_counter
            }
            
            # üî• AN√ÅLISE DE GRADIENTES
            if len(self.grad_norms) >= 5:
                recent_grads = self.grad_norms[-5:]
                avg_grad = np.mean(recent_grads)
                grad_std = np.std(recent_grads)
                
                if avg_grad < 1e-8:
                    analysis['grad_status'] = "‚ùå GRADIENTES MORTOS"
                elif avg_grad > 50:
                    analysis['grad_status'] = "‚ö†Ô∏è GRADIENTES EXPLODINDO"
                elif avg_grad >= 0.1 and avg_grad <= 5.0 and grad_std < avg_grad * 0.1:
                    # üî• CORRE√á√ÉO: Gradientes na faixa saud√°vel (0.1-5.0) com baixa varia√ß√£o = CONVERG√äNCIA EST√ÅVEL
                    analysis['grad_status'] = f"‚úÖ GRADIENTES EST√ÅVEIS ({avg_grad:.2e})"
                elif avg_grad < 0.1 and grad_std < avg_grad * 0.05:
                    # Gradientes muito baixos com pouca varia√ß√£o = poss√≠vel estagna√ß√£o
                    analysis['grad_status'] = "‚ö†Ô∏è GRADIENTES ESTAGNADOS"
                else:
                    analysis['grad_status'] = f"‚úÖ GRADIENTES OK ({avg_grad:.2e})"
                    
                analysis['avg_grad_norm'] = avg_grad
            else:
                analysis['avg_grad_norm'] = 0
                    
            # üî• AN√ÅLISE DE LOSSES
            if len(self.policy_losses) >= 5:
                recent_losses = self.policy_losses[-5:]
                avg_loss = np.mean(recent_losses)
                
                if len(self.policy_losses) >= 10:
                    early_losses = self.policy_losses[:5]
                    early_avg = np.mean(early_losses)
                    
                    if avg_loss < early_avg * 0.95:
                        analysis['loss_status'] = f"‚úÖ LOSS DIMINUINDO ({avg_loss:.3f})"
                    elif avg_loss > early_avg * 1.05:
                        analysis['loss_status'] = f"‚ö†Ô∏è LOSS AUMENTANDO ({avg_loss:.3f})"
                    else:
                        analysis['loss_status'] = f"üî∂ LOSS EST√ÅVEL ({avg_loss:.3f})"
                else:
                    analysis['loss_status'] = f"üî∂ LOSS INICIAL ({avg_loss:.3f})"
                    
                analysis['avg_policy_loss'] = avg_loss
            else:
                analysis['avg_policy_loss'] = 0
                    
            # üî• AN√ÅLISE DE PESOS
            if len(self.weight_changes) >= 5:
                recent_changes = self.weight_changes[-5:]
                avg_change = np.mean(recent_changes)
                
                if avg_change < 1e-8:
                    analysis['weight_status'] = "‚ùå PESOS CONGELADOS"
                elif avg_change > 0.1:
                    analysis['weight_status'] = "‚ö†Ô∏è PESOS INST√ÅVEIS"
                else:
                    analysis['weight_status'] = f"‚úÖ PESOS ATUALIZANDO ({avg_change:.2e})"
                    
                analysis['avg_weight_change'] = avg_change
            else:
                analysis['avg_weight_change'] = 0
                    
            # üî• AN√ÅLISE DE PERFORMANCE
            if len(self.reward_history) >= 10:
                recent_rewards = self.reward_history[-5:]
                recent_avg = np.mean(recent_rewards)
                
                if len(self.reward_history) >= 20:
                    early_rewards = self.reward_history[:10]
                    early_avg = np.mean(early_rewards)
                    
                    if recent_avg > early_avg + 0.5:
                        analysis['perf_status'] = f"‚úÖ PERFORMANCE ‚Üë ({recent_avg:.2f})"
                    elif recent_avg < early_avg - 0.5:
                        analysis['perf_status'] = f"‚ö†Ô∏è PERFORMANCE ‚Üì ({recent_avg:.2f})"
                    else:
                        analysis['perf_status'] = f"üî∂ PERFORMANCE EST√ÅVEL ({recent_avg:.2f})"
                else:
                    analysis['perf_status'] = f"üî∂ PERFORMANCE INICIAL ({recent_avg:.2f})"
                    
                analysis['avg_reward'] = recent_avg
            else:
                analysis['avg_reward'] = 0
                    
            # üî• STATUS GERAL (L√≥gica mais inteligente)
            positive_indicators = sum([
                "‚úÖ" in analysis['grad_status'],
                "‚úÖ" in analysis['loss_status'], 
                "‚úÖ" in analysis['weight_status'],
                "‚úÖ" in analysis['perf_status']
            ])
            
            total_indicators = sum([
                analysis['grad_status'] != "DESCONHECIDO",
                analysis['loss_status'] != "DESCONHECIDO",
                analysis['weight_status'] != "DESCONHECIDO", 
                analysis['perf_status'] != "DESCONHECIDO"
            ])
            
            if total_indicators == 0:
                analysis['overall_status'] = "‚è≥ AGUARDANDO DADOS"
                self.plateau_counter = 0
            elif positive_indicators >= max(2, total_indicators * 0.6):
                analysis['overall_status'] = "‚úÖ APRENDENDO BEM"
                self.plateau_counter = 0
            elif positive_indicators >= 1:
                analysis['overall_status'] = "üî∂ APRENDENDO MODERADAMENTE"
                self.plateau_counter = 0
            else:
                analysis['overall_status'] = "‚ö†Ô∏è POSS√çVEL PROBLEMA"
                self.plateau_counter += 1
                
            analysis['plateau_counter'] = self.plateau_counter 
            self.learning_status = analysis['overall_status']
            
            return analysis
            
        except Exception as e:
            return {
                'overall_status': "‚ùå ERRO NA AN√ÅLISE",
                'grad_status': "ERRO",
                'loss_status': "ERRO", 
                'weight_status': "ERRO",
                'perf_status': "ERRO",
                'plateau_counter': self.plateau_counter,
                'avg_policy_loss': np.mean(self.policy_losses[-10:]) if len(self.policy_losses) >= 10 else 0,
                'avg_reward': np.mean(self.reward_history[-10:]) if len(self.reward_history) >= 10 else 0,
                'current_lr': self.learning_rates[-1] if self.learning_rates else 0
            }

class MetricsCallback(BaseCallback):
    """
    Callback customizado para mostrar m√©tricas detalhadas a cada 2000 passos
    """
    def __init__(self, env, log_freq=2000, verbose=0):
        super().__init__(verbose)
        self.env = env
        self.log_freq = log_freq
        self.last_step = 0
        self.learning_monitor = LearningMonitor()  # üî• ADICIONAR MONITOR DE APRENDIZADO
        # üî• RASTREAR REWARDS REAIS DO PPO
        self.recent_rewards = []
        self.reward_buffer_size = 50
        # üî• CORRE√á√ÉO: Adicionar atributos faltantes
        self.total_trades_global = 0
        self.detector = None  # Ser√° inicializado se necess√°rio
        
        # üî• SISTEMA DE M√âTRICAS GLOBAIS (APENAS DURANTE ESTA EXECU√á√ÉO)
        self.global_metrics = {
            'peak_drawdown': 0.0,           # Pico de drawdown global
            'total_trades': 0,              # Total de trades global
            'total_pnl': 0.0,               # PnL total global
            'profitable_trades': 0,         # Trades lucrativos global
            'peak_portfolio': 500.0,       # Pico de portfolio global
            'total_steps': 0,               # Total de steps global
            'episode_count': 0              # Contador de epis√≥dios
        }
        
        # üî• N√ÉO CARREGAR M√âTRICAS GLOBAIS - APENAS GLOBAIS DENTRO DA EXECU√á√ÉO ATUAL
        # self._load_global_metrics()  # DESABILITADO: m√©tricas devem ser apenas da execu√ß√£o atual
    
    def _continue_learning_monitor_display(self):
        """Continua a exibi√ß√£o do learning monitor ap√≥s as m√©tricas corrigidas"""
        try:
            # Capturar rewards reais
            last_reward = 0
            if hasattr(self, 'training_env') and hasattr(self.training_env, 'get_attr'):
                try:
                    recent_rewards = self.training_env.get_attr('recent_rewards')[0]
                    if recent_rewards:
                        last_reward = recent_rewards[-1]
                except:
                    last_reward = 0
            
            # Atualizar learning monitor
            self.learning_monitor.update(self.model, last_reward, 0)
            
            # Analisar status de aprendizado
            learning_analysis = self.learning_monitor.analyze_learning_status()
            
            # üî• EXIBIR STATUS DE APRENDIZADO
            print(f"\nüß† === STATUS DE APRENDIZADO ===")
            print(f"üéØ Status Geral: {learning_analysis.get('overall_status', 'DESCONHECIDO')}")
            print(f"üìä Gradientes: {learning_analysis.get('grad_status', 'DESCONHECIDO')}")
            print(f"üìâ Loss: {learning_analysis.get('loss_status', 'DESCONHECIDO')}")
            print(f"‚öñÔ∏è Pesos: {learning_analysis.get('weight_status', 'DESCONHECIDO')}")
            print(f"üìà Performance: {learning_analysis.get('perf_status', 'DESCONHECIDO')}")
            
            # M√©tricas num√©ricas detalhadas
            avg_grad = learning_analysis.get('avg_grad_norm', 0)
            avg_loss = learning_analysis.get('avg_policy_loss', 0)
            current_lr = learning_analysis.get('current_lr', 0)
            
            if avg_grad > 0:
                print(f"üî¢ Grad Norm: {avg_grad:.2e} | Policy Loss: {avg_loss:.4f} | LR: {current_lr:.2e}")
            
            print(f"üîß Learning Rate FIXO: {BEST_PARAMS['learning_rate']:.2e} (sem ajustes din√¢micos)")
            
            # Status de atividade de trading
            current_trades_per_day = 0  # Ser√° calculado acima
            try:
                trades_lists = self.training_env.get_attr('trades')
                episode_steps_list = self.training_env.get_attr('episode_steps')
                if trades_lists and episode_steps_list:
                    total_trades = len(trades_lists[0])
                    episode_steps = episode_steps_list[0]
                    current_trades_per_day = (total_trades / max(1, episode_steps)) * 288 if episode_steps > 0 else 0
            except:
                current_trades_per_day = 0
            
            if current_trades_per_day < 12:
                activity_status = "üî¥ MUITO BAIXO"
            elif 12 <= current_trades_per_day < 15:
                activity_status = "üü° BAIXO"
            elif 15 <= current_trades_per_day <= 21:
                activity_status = "üü¢ ZONA ALVO"
            elif 21 < current_trades_per_day <= 25:
                activity_status = "üü° ALTO"
            else:
                activity_status = "üî¥ MUITO ALTO"
            
            print(f"üìä Trades/Dia: {current_trades_per_day:.1f} | Target: 18 | Status: {activity_status}")
            print(f"üéØ SL Zona Alvo: N/A | TP Zona Alvo: N/A (env sem trades)")
            print(f"üîç Loss Status: Aguardando dados para an√°lise")
            print("=================================================================")
            print("üî• Para AVALIA√á√ÉO ON-DEMAND: crie arquivo 'eval.txt' na pasta")
            print("üî• Sistema de avalia√ß√£o on-demand continua ativo - crie arquivo 'eval.txt' para avaliar")
            
        except Exception as e:
            print(f"[LEARNING_MONITOR] Erro: {e}")
            print("üß† === STATUS DE APRENDIZADO ===")
            print("üéØ Status Geral: ‚ö†Ô∏è ERRO NA CAPTURA")
            print("=================================================================")
        
    def _on_step(self) -> bool:
        # üî• PROCESSAR AVALIA√á√ÉO ON-DEMAND A CADA STEP
        global on_demand_eval
        if on_demand_eval is not None:
            on_demand_eval.process_evaluation_queue()
        
        # Verificar se deve ativar m√©tricas
        if self.num_timesteps - self.last_step >= self.log_freq:
            try:
                # Tentar m√∫ltiplas formas de acessar o ambiente
                env = None
                if hasattr(self, 'training_env'):
                    if hasattr(self.training_env, 'envs'):
                        env = self.training_env.envs[0]
                    elif hasattr(self.training_env, 'venv'):
                        env = self.training_env.venv.envs[0]
                    else:
                        env = self.training_env
                elif hasattr(self, 'env'):
                    env = self.env
                
                # üî• CORRE√á√ÉO CR√çTICA: Acessar ambiente real atrav√©s do VecEnv
                if env is None and hasattr(self, 'training_env'):
                    try:
                        # Tentar get_attr para VecEnv
                        portfolio_values = self.training_env.get_attr('portfolio_value')
                        realized_balances = self.training_env.get_attr('realized_balance')
                        trades_lists = self.training_env.get_attr('trades')
                        positions_lists = self.training_env.get_attr('positions')
                        drawdowns = self.training_env.get_attr('current_drawdown')
                        
                        if portfolio_values and len(portfolio_values) > 0:
                            # Usar dados do primeiro ambiente
                            portfolio = portfolio_values[0]
                            realized_balance = realized_balances[0]
                            trades = trades_lists[0]
                            positions = positions_lists[0]
                            episode_drawdown = drawdowns[0]
                            
                            # Calcular unrealized PnL
                            unrealized_pnl = 0
                            try:
                                current_prices = self.training_env.get_attr('df')
                                current_steps = self.training_env.get_attr('current_step')
                                if current_prices and current_steps:
                                    current_price = current_prices[0]['close_5m'].iloc[current_steps[0]]
                                    for pos in positions:
                                        if pos['type'] == 'long':
                                            unrealized_pnl += (current_price - pos['entry_price']) * pos['lot_size']
                                        else:
                                            unrealized_pnl += (pos['entry_price'] - current_price) * pos['lot_size']
                            except:
                                unrealized_pnl = 0
                            
                            # Portfolio = Realized + Unrealized
                            portfolio = realized_balance + unrealized_pnl
                            
                            # M√©tricas de trading detalhadas (epis√≥dio atual)
                            total_trades = len(trades)
                            profitable_trades = len([t for t in trades if t.get('pnl_usd', 0) > 0])
                            win_rate = (profitable_trades / total_trades * 100) if total_trades > 0 else 0
                            total_pnl = sum(t.get('pnl_usd', 0) for t in trades)
                            
                            # üî• ATUALIZAR M√âTRICAS GLOBAIS
                            self.global_metrics['total_trades'] = max(self.global_metrics.get('total_trades', 0), total_trades)
                            self.global_metrics['profitable_trades'] = max(self.global_metrics.get('profitable_trades', 0), profitable_trades)
                            self.global_metrics['total_pnl'] = max(self.global_metrics.get('total_pnl', 0), total_pnl)
                            self.global_metrics['peak_drawdown'] = max(self.global_metrics.get('peak_drawdown', 0), episode_drawdown)
                            
                            # üî• M√âTRICAS GLOBAIS ACUMULADAS
                            global_total_trades = self.global_metrics['total_trades']
                            global_profitable_trades = self.global_metrics['profitable_trades']
                            global_win_rate = (global_profitable_trades / global_total_trades * 100) if global_total_trades > 0 else 0
                            global_total_pnl = self.global_metrics['total_pnl']
                            
                            # Trades por dia usando episode_steps
                            try:
                                episode_steps_list = self.training_env.get_attr('episode_steps')
                                episode_steps = episode_steps_list[0] if episode_steps_list else self.num_timesteps
                                days_elapsed = episode_steps / 288  # 288 steps = 1 dia (5min bars)
                                trades_per_day = total_trades / max(days_elapsed, 0.01)
                            except Exception:
                                days_elapsed = max(1, self.num_timesteps / 288)
                                trades_per_day = total_trades / days_elapsed
                            
                            # M√©tricas avan√ßadas
                            avg_trade_pnl = total_pnl / max(total_trades, 1)
                            
                            # Calcular m√©tricas detalhadas
                            drawdown = episode_drawdown  # Drawdown j√° est√° em percentual
                            peak_drawdown = self.global_metrics['peak_drawdown']  # Pico DD j√° est√° em percentual
                            
                            # Exibir m√©tricas corrigidas
                            print(f"\n=== üìä M√âTRICAS DETALHADAS - Step {self.num_timesteps:,} ===")
                            # üî• CORRE√á√ÉO: Calcular pico de portfolio
                            peak_portfolio = self.global_metrics.get('peak_portfolio', portfolio)
                            if portfolio > peak_portfolio:
                                self.global_metrics['peak_portfolio'] = portfolio
                                peak_portfolio = portfolio
                            
                            # üî• CORRE√á√ÉO: Calcular trades/dia global real
                            global_days_elapsed = self.num_timesteps / 288.0  # 288 steps = 1 dia
                            global_trades_per_day = global_total_trades / max(global_days_elapsed, 0.01)
                            
                            print(f"üí∞ Portfolio: ${portfolio:.2f} | Pico Portfolio: ${peak_portfolio:.2f} | N√£o Realizado: ${unrealized_pnl:.2f}")
                            print(f"üìâ Drawdown Atual (Ep): {drawdown:.2f}% | Pico DD (Global): {peak_drawdown:.2f}%")
                            print(f"üìà Trades Globais: {global_total_trades} | Trades (Ep): {total_trades} | Win Rate (Ep): {win_rate:.1f}%")
                            print(f"üíµ PnL (Ep): ${total_pnl:.2f} | PnL M√©dio/Trade (Ep): ${avg_trade_pnl:.2f}")
                            print(f"‚ö° Trades/Dia (Global): {global_trades_per_day:.2f} | Trades/Dia (Ep): {trades_per_day:.2f} | Win Rate Global: {global_win_rate:.1f}%")
                            
                            # Continuar com o resto do c√≥digo de learning monitor
                            if hasattr(self, 'model') and self.model is not None:
                                self._continue_learning_monitor_display()
                            
                            self.last_step = self.num_timesteps
                            return True
                    except Exception as e:
                        print(f"[M√âTRICAS] Erro ao acessar VecEnv: {e}")
                        # Continuar com o m√©todo original se falhar
                
                # üî• ATUALIZAR MODELO NO SISTEMA ON-DEMAND
                if hasattr(self, 'model') and env is not None and on_demand_eval is not None:
                    training_env = getattr(self, 'training_env', env)
                    on_demand_eval.update_current_model(self.model, training_env)
                
                if env is None:
                    print(f"\n[M√âTRICAS - Step {self.num_timesteps}] - Ambiente n√£o encontrado")
                    self.last_step = self.num_timesteps
                    return True
                
                # üî• ATUALIZAR M√âTRICAS GLOBAIS
                self._update_global_metrics(env)
                
                # Calcular m√©tricas detalhadas
                realized_balance = getattr(env, 'realized_balance', 1000)
                episode_drawdown = getattr(env, 'current_drawdown', 0)
                
                # üî• USAR M√âTRICAS GLOBAIS PERSISTENTES
                drawdown = episode_drawdown  # Drawdown j√° est√° em percentual
                peak_drawdown = self.global_metrics['peak_drawdown']  # Pico DD j√° est√° em percentual
                
                trades = getattr(env, 'trades', [])
                positions = getattr(env, 'positions', [])
                
                # Calcular unrealized PnL
                unrealized_pnl = 0
                if hasattr(env, 'df') and hasattr(env, 'current_step') and hasattr(env, 'base_tf'):
                    try:
                        if env.current_step < len(env.df):
                            current_price = env.df[f'close_{env.base_tf}'].iloc[env.current_step]
                            unrealized_pnl = sum(env._get_position_pnl(pos, current_price) for pos in positions)
                    except Exception as e:
                        unrealized_pnl = 0
                
                # Portfolio = Realized + Unrealized (com prote√ß√£o contra valores extremos)
                portfolio = realized_balance + unrealized_pnl
                # üî• PROTE√á√ÉO: Evitar portfolios negativos extremos nas m√©tricas
                portfolio = max(portfolio, 0.01)  # M√≠nimo $0.01 para evitar divis√£o por zero
                
                # M√©tricas de trading detalhadas (epis√≥dio atual)
                total_trades = len(trades)
                profitable_trades = len([t for t in trades if t.get('pnl_usd', 0) > 0])
                win_rate = (profitable_trades / total_trades * 100) if total_trades > 0 else 0
                total_pnl = sum(t.get('pnl_usd', 0) for t in trades)
                
                # üî• M√âTRICAS GLOBAIS ACUMULADAS
                global_total_trades = self.global_metrics['total_trades']
                global_profitable_trades = self.global_metrics['profitable_trades']
                global_win_rate = (global_profitable_trades / global_total_trades * 100) if global_total_trades > 0 else 0
                global_total_pnl = self.global_metrics['total_pnl']
                
                # Trades por dia usando episode_steps
                try:
                    episode_steps = env.episode_steps if hasattr(env, 'episode_steps') else self.num_timesteps
                    days_elapsed = episode_steps / 288  # 288 steps = 1 dia (5min bars)
                    trades_per_day = total_trades / max(days_elapsed, 0.01)
                except Exception:
                    days_elapsed = max(1, self.num_timesteps / 288)
                    trades_per_day = total_trades / days_elapsed
                
                # M√©tricas avan√ßadas
                avg_trade_pnl = total_pnl / max(total_trades, 1)
                losing_trades = total_trades - profitable_trades
                
                # üî• M√âTRICA PRINCIPAL: Lucro/dia baseado em 288 barras = 1 dia (5min bars)
                days_elapsed_288 = self.num_timesteps / 288.0  # 288 barras de 5min = 1 dia
                lucro_por_dia = total_pnl / max(days_elapsed_288, 0.001)  # Evitar divis√£o por zero
                
                # üî• CONECTAR LEARNING MONITOR AO MODELO PPO VIA CALLBACK
                model = None
                # BaseCallback sempre tem self.model dispon√≠vel ap√≥s init_callback
                if hasattr(self, 'model') and self.model is not None:
                    model = self.model
                
                if model is not None:
                    # üî• CAPTURAR REWARDS REAIS que o PPO est√° recebendo
                    last_reward = 0
                    # M√©todo 1: Do ambiente direto (recent_rewards)
                    if hasattr(env, 'recent_rewards') and env.recent_rewards:
                        last_reward = env.recent_rewards[-1]  # √öltima reward
                    # M√©todo 2: Do VecEnv se dispon√≠vel  
                    elif hasattr(self, 'training_env') and hasattr(self.training_env, 'get_attr'):
                        try:
                            recent_rewards = self.training_env.get_attr('recent_rewards')[0]
                            if recent_rewards:
                                last_reward = recent_rewards[-1]
                        except:
                            last_reward = total_pnl / max(total_trades, 1) if total_trades > 0 else 0
                    # M√©todo 3: Fallback para PnL m√©dio
                    else:
                        last_reward = total_pnl / max(total_trades, 1) if total_trades > 0 else 0
                    
                    # Debug de trades removido - sistema funcionando corretamente
                    
                    episode_length = getattr(env, 'episode_steps', 0)
                    
                    
                    self.learning_monitor.update(model, last_reward, episode_length)
                    
                    # Analisar status de aprendizado
                    learning_analysis = self.learning_monitor.analyze_learning_status()
                    
                    print(f"\n=== üìä M√âTRICAS DETALHADAS - Step {self.num_timesteps:,} ===")
                    # üî• CORRE√á√ÉO 2: REMOVER DEBUG DO CURRENT STEP
                    # üî• CORRE√á√ÉO: Calcular pico de portfolio
                    peak_portfolio = self.global_metrics.get('peak_portfolio', portfolio)
                    if portfolio > peak_portfolio:
                        self.global_metrics['peak_portfolio'] = portfolio
                        peak_portfolio = portfolio
                    
                    # üî• CORRE√á√ÉO: Calcular trades/dia global real
                    global_days_elapsed = self.num_timesteps / 288.0  # 288 steps = 1 dia
                    global_trades_per_day = global_total_trades / max(global_days_elapsed, 0.01)
                    
                    print(f"üí∞ Portfolio: ${portfolio:.2f} | Pico Portfolio: ${peak_portfolio:.2f} | N√£o Realizado: ${unrealized_pnl:.2f}")
                    print(f"üìâ Drawdown Atual (Ep): {drawdown:.2f}% | Pico DD (Global): {peak_drawdown:.2f}%")
                    # üî• RELAT√ìRIO CORRIGIDO: Separar m√©tricas globais e de epis√≥dio
                    print(f"üìà Trades Globais: {global_total_trades} | Trades (Ep): {total_trades} | Win Rate (Ep): {win_rate:.1f}%")
                    print(f"üíµ PnL (Ep): ${total_pnl:.2f} | PnL M√©dio/Trade (Ep): ${avg_trade_pnl:.2f}")
                    print(f"‚ö° Trades/Dia (Global): {global_trades_per_day:.2f} | Trades/Dia (Ep): {trades_per_day:.2f} | Win Rate Global: {global_win_rate:.1f}%")
                    
                    # üö® EXIBIR ESTAT√çSTICAS DO DETECTOR (se dispon√≠vel)
                    if hasattr(self, 'detector') and self.detector is not None:
                        try:
                            detector_stats = self.detector.get_stats()
                            if detector_stats['total_detections'] > 0:
                                print(f"üö® PROBLEMAS: FlipFlops={detector_stats['flip_flop_count']} | Microtrades={detector_stats['microtrade_count']}")
                        except:
                            pass
                    
                    # üî• EXIBIR STATUS DE APRENDIZADO
                    print(f"\nüß† === STATUS DE APRENDIZADO ===")
                    print(f"üéØ Status Geral: {learning_analysis.get('overall_status', 'DESCONHECIDO')}")
                    print(f"üìä Gradientes: {learning_analysis.get('grad_status', 'DESCONHECIDO')}")
                    print(f"üìâ Loss: {learning_analysis.get('loss_status', 'DESCONHECIDO')}")
                    print(f"‚öñÔ∏è Pesos: {learning_analysis.get('weight_status', 'DESCONHECIDO')}")
                    print(f"üìà Performance: {learning_analysis.get('perf_status', 'DESCONHECIDO')}")
                    
                    # M√©tricas num√©ricas detalhadas
                    avg_grad = learning_analysis.get('avg_grad_norm', 0)
                    avg_loss = learning_analysis.get('avg_policy_loss', 0)
                    current_lr = learning_analysis.get('current_lr', 0)
                    # üî• CORRE√á√ÉO: Se current_lr for 0, tentar pegar o √∫ltimo LR capturado
                    if current_lr == 0 and len(self.learning_monitor.learning_rates) > 0:
                        current_lr = self.learning_monitor.learning_rates[-1]
                    
                    if avg_grad > 0:
                        print(f"üî¢ Grad Norm: {avg_grad:.2e} | Policy Loss: {avg_loss:.4f} | LR: {current_lr:.2e}")
                    
                    # üî• LR FIXO: Sem scheduler din√¢mico, m√°xima estabilidade
                    print(f"üîß Learning Rate FIXO: {BEST_PARAMS['learning_rate']:.2e} (sem ajustes din√¢micos)")
                    
                    # üî• LEARNING RATE FIXO - SEM ADAPTA√á√ÉO AUTOM√ÅTICA
                    # Sistema de LR adaptativo DESABILITADO para evitar pesos congelados
                    if avg_loss is not None:
                        if avg_loss > 0.1:  # üî• THRESHOLD MUITO MAIS ALTO: 0.02‚Üí0.1
                            print(f"‚ö†Ô∏è ALERTA: Loss alto ({avg_loss:.4f}) - mas LR mantido fixo para estabilidade")
                        elif avg_loss > 0.05:  # üî• THRESHOLD MAIS ALTO: 0.01‚Üí0.05
                            print(f"‚ö†Ô∏è ALERTA: Loss moderadamente alto ({avg_loss:.4f}) - monitorando...")
                        elif avg_loss < -0.5:  # Loss muito negativo (poss√≠vel problema)
                            print(f"‚ö†Ô∏è ALERTA: Loss muito negativo ({avg_loss:.4f}) - poss√≠vel problema de reward scaling!")
                    
                    # üî• LR FIXO REMOVIDO - usar apenas configura√ß√£o padr√£o do PPO
                        
                        # üî• RESET FOR√áADO REMOVIDO
                    
                    # üî• CORRE√á√ÉO: Usar o mesmo c√°lculo de trades/dia das m√©tricas principais
                    # Evitar duplica√ß√£o de c√°lculos diferentes
                    
                    # Status de atividade de trading (baseado no trades_per_day j√° calculado)
                    if trades_per_day < 12:
                        activity_status = "üî¥ MUITO BAIXO"
                    elif 12 <= trades_per_day < 15:
                        activity_status = "üü° BAIXO"
                    elif 15 <= trades_per_day <= 21:
                        activity_status = "üü¢ ZONA ALVO"
                    elif 21 < trades_per_day <= 25:
                        activity_status = "üü° ALTO"
                    else:
                        activity_status = "üî¥ MUITO ALTO"
                    
                    print(f"üìä Trades/Dia: {trades_per_day:.1f} | Target: 18 | Status: {activity_status}")
                    
                    # üî• MONITORAMENTO H√çBRIDO DE SL/TP: Posi√ß√µes abertas + Trades hist√≥ricos
                    if hasattr(env, 'trades') and env.trades:
                        # Analisar trades hist√≥ricos recentes (√∫ltimos 20 trades)
                        recent_trades = env.trades[-20:] if len(env.trades) >= 20 else env.trades
                        
                        # Contar trades com SL/TP na zona alvo (hist√≥rico)
                        historical_sl_optimal = 0
                        historical_tp_optimal = 0
                        historical_sl_count = 0
                        historical_tp_count = 0
                        
                        for trade in recent_trades:
                            # Verificar se o trade tem informa√ß√µes de SL/TP
                            if 'sl_points' in trade and trade['sl_points'] > 0:
                                historical_sl_count += 1
                                if 8 <= trade['sl_points'] <= 35:
                                    historical_sl_optimal += 1
                            
                            if 'tp_points' in trade and trade['tp_points'] > 0:
                                historical_tp_count += 1
                                if 12 <= trade['tp_points'] <= 60:
                                    historical_tp_optimal += 1
                        
                        # Verificar posi√ß√µes abertas (tempo real)
                        live_sl_optimal = 0
                        live_tp_optimal = 0
                        live_positions = 0
                        
                        if hasattr(env, 'positions') and len(env.positions) > 0:
                            live_positions = len(env.positions)
                            for pos in env.positions:
                                # Converter SL/TP de pre√ßos para pontos
                                entry_price = pos.get('entry_price', 0)
                                sl_price = pos.get('sl', 0)
                                tp_price = pos.get('tp', 0)
                                
                                if entry_price > 0 and sl_price > 0:
                                    if pos['type'] == 'long':
                                        sl_points = abs(entry_price - sl_price) * 100
                                    else:  # short
                                        sl_points = abs(sl_price - entry_price) * 100
                                    
                                    if 8 <= sl_points <= 35:
                                        live_sl_optimal += 1
                                
                                if entry_price > 0 and tp_price > 0:
                                    if pos['type'] == 'long':
                                        tp_points = abs(tp_price - entry_price) * 100
                                    else:  # short
                                        tp_points = abs(entry_price - tp_price) * 100
                                    
                                    if 12 <= tp_points <= 60:
                                        live_tp_optimal += 1
                        
                        # üî• EXIBIR M√âTRICAS H√çBRIDAS (hist√≥rico + tempo real)
                        if historical_sl_count > 0 or live_positions > 0:
                            # Calcular taxa hist√≥rica
                            historical_sl_rate = (historical_sl_optimal / historical_sl_count * 100) if historical_sl_count > 0 else 0
                            historical_tp_rate = (historical_tp_optimal / historical_tp_count * 100) if historical_tp_count > 0 else 0
                            
                            # Calcular taxa em tempo real
                            live_sl_rate = (live_sl_optimal / live_positions * 100) if live_positions > 0 else 0
                            live_tp_rate = (live_tp_optimal / live_positions * 100) if live_positions > 0 else 0
                            
                            # Exibir m√©tricas combinadas
                            if live_positions > 0:
                                print(f"üéØ SL Zona Alvo: {live_sl_rate:.1f}% (Live: {live_positions}) | Hist√≥rico: {historical_sl_rate:.1f}% ({historical_sl_count} trades)")
                                print(f"üéØ TP Zona Alvo: {live_tp_rate:.1f}% (Live: {live_positions}) | Hist√≥rico: {historical_tp_rate:.1f}% ({historical_tp_count} trades)")
                            else:
                                print(f"üéØ SL Zona Alvo: {historical_sl_rate:.1f}% (Hist√≥rico: {historical_sl_count} trades, sem posi√ß√µes abertas)")
                                print(f"üéØ TP Zona Alvo: {historical_tp_rate:.1f}% (Hist√≥rico: {historical_tp_count} trades, sem posi√ß√µes abertas)")
                        else:
                            print("üéØ SL/TP: Aguardando dados (sem posi√ß√µes ou trades com SL/TP)")
                    else:
                        print("üéØ SL Zona Alvo: N/A | TP Zona Alvo: N/A (env sem trades)")
                    
                    # An√°lise de monetiza√ß√£o de wins
                    if win_rate > 0:
                        avg_win_size = avg_trade_pnl if avg_trade_pnl > 0 else 0
                        if avg_win_size >= 15:
                            monetization_status = "üü¢ EXCELENTE"
                        elif avg_win_size >= 8:
                            monetization_status = "üü° BOM"
                        else:
                            monetization_status = "üî¥ BAIXO"
                        print(f"üí∞ Monetiza√ß√£o Wins: ${avg_win_size:.2f}/trade | Status: {monetization_status}")
                    else:
                        print("üîç Loss Status: Aguardando dados para an√°lise")
                    
                    plateau_count = learning_analysis.get('plateau_counter', 0)
                    if plateau_count > 0:
                        print(f"‚ö†Ô∏è Plateau Counter: {plateau_count} (poss√≠vel estagna√ß√£o)")
                    
                    print("=" * 65)
                else:
                    print(f"\n=== üìä M√âTRICAS DETALHADAS - Step {self.num_timesteps:,} ===")
                    # üî• CORRE√á√ÉO: Calcular pico de portfolio
                    peak_portfolio = self.global_metrics.get('peak_portfolio', portfolio)
                    if portfolio > peak_portfolio:
                        self.global_metrics['peak_portfolio'] = portfolio
                        peak_portfolio = portfolio
                    
                    # üî• CORRE√á√ÉO: Calcular trades/dia global real
                    global_days_elapsed = self.num_timesteps / 288.0  # 288 steps = 1 dia
                    global_trades_per_day = global_total_trades / max(global_days_elapsed, 0.01)
                    
                    print(f"üí∞ Portfolio: ${portfolio:.2f} | Pico Portfolio: ${peak_portfolio:.2f} | N√£o Realizado: ${unrealized_pnl:.2f}")
                    print(f"üìâ Drawdown Atual (Ep): {drawdown:.2f}% | Pico DD (Global): {peak_drawdown:.2f}%")
                    # üî• RELAT√ìRIO CORRIGIDO: Separar m√©tricas globais e de epis√≥dio
                    print(f"üìà Trades Globais: {global_total_trades} | Trades (Ep): {total_trades} | Win Rate (Ep): {win_rate:.1f}%")
                    print(f"üíµ PnL (Ep): ${total_pnl:.2f} | PnL M√©dio/Trade (Ep): ${avg_trade_pnl:.2f}")
                    print(f"‚ö° Trades/Dia (Global): {global_trades_per_day:.2f} | Trades/Dia (Ep): {trades_per_day:.2f} | Win Rate Global: {global_win_rate:.1f}%")
                    
                    # üö® EXIBIR ESTAT√çSTICAS DO DETECTOR (se√ß√£o sem modelo)
                    detector_stats = self.detector.get_stats()
                    if detector_stats['total_detections'] > 0:
                        print(f"üö® PROBLEMAS: FlipFlops={detector_stats['flip_flop_count']} | Microtrades={detector_stats['microtrade_count']}")
                    
                    print("=" * 65)
                
                print(f"üî• Para AVALIA√á√ÉO ON-DEMAND: crie arquivo 'eval.txt' na pasta")
                
                print("üî• Sistema de avalia√ß√£o on-demand continua ativo - crie arquivo 'eval.txt' para avaliar")
                
            except Exception as e:
                print(f"\n[M√âTRICAS - Step {self.num_timesteps}] - Erro ao calcular m√©tricas: {str(e)}")
            
            self.last_step = self.num_timesteps
            
        return True
    
    def _on_training_end(self) -> None:
        """üî• EXIBIR M√âTRICAS GLOBAIS AO FINAL DO TREINAMENTO (SEM SALVAR)"""
        print(f"\n[GLOBAL METRICS] üèÅ Treinamento finalizado - Exibindo m√©tricas globais da execu√ß√£o atual...")
        
        # Exibir resumo final das m√©tricas globais
        if self.global_metrics['total_trades'] > 0:
            final_win_rate = (self.global_metrics['profitable_trades'] / self.global_metrics['total_trades']) * 100
            final_avg_pnl = self.global_metrics['total_pnl'] / self.global_metrics['total_trades']
            final_return_pct = ((self.global_metrics['peak_portfolio'] - 500) / 500) * 100
            
            print(f"\nüèÜ === RESUMO FINAL DAS M√âTRICAS GLOBAIS ===")
            print(f"üìä Total de Trades: {self.global_metrics['total_trades']}")
            print(f"üí∞ PnL Total: ${self.global_metrics['total_pnl']:.2f}")
            print(f"üéØ Win Rate Global: {final_win_rate:.1f}%")
            print(f"üìà Retorno Total: {final_return_pct:.1f}%")
            print(f"üíé Pico Portfolio: ${self.global_metrics['peak_portfolio']:.2f}")
            print(f"üìâ Peak Drawdown: {self.global_metrics['peak_drawdown']:.4f}")
            print(f"‚è±Ô∏è Total Steps: {self.global_metrics['total_steps']:,}")
            print(f"üîÑ Epis√≥dios: {self.global_metrics['episode_count']}")
            print(f"==========================================")
    
    def _update_global_metrics(self, env):
        """üî• ATUALIZAR M√âTRICAS GLOBAIS PERSISTENTES ENTRE EPIS√ìDIOS"""
        try:
            # Atualizar drawdown global
            current_drawdown = getattr(env, 'current_drawdown', 0)
            if current_drawdown > self.global_metrics['peak_drawdown']:
                self.global_metrics['peak_drawdown'] = current_drawdown
            
            # üî• CORRE√á√ÉO: Atualizar m√©tricas globais baseadas no epis√≥dio atual
            trades = getattr(env, 'trades', [])
            current_trades_count = len(trades)
            
            # Atualizar m√©tricas globais com dados do epis√≥dio atual
            if current_trades_count > 0:
                # Total de trades do epis√≥dio atual
                episode_total_trades = current_trades_count
                episode_pnl = sum(t.get('pnl_usd', 0) for t in trades)
                episode_profitable_trades = len([t for t in trades if t.get('pnl_usd', 0) > 0])
                
                # Atualizar m√©tricas globais (acumulativo apenas durante esta execu√ß√£o)
                self.global_metrics['total_trades'] = max(self.global_metrics['total_trades'], episode_total_trades)
                self.global_metrics['total_pnl'] = max(self.global_metrics['total_pnl'], episode_pnl)
                self.global_metrics['profitable_trades'] = max(self.global_metrics['profitable_trades'], episode_profitable_trades)
            
            # Atualizar pico de portfolio global
            realized_balance = getattr(env, 'realized_balance', 1000)
            if realized_balance > self.global_metrics['peak_portfolio']:
                self.global_metrics['peak_portfolio'] = realized_balance
            
            # Atualizar contadores
            self.global_metrics['total_steps'] = self.num_timesteps
            
            # Detectar novo epis√≥dio (quando episode_steps √© baixo)
            episode_steps = getattr(env, 'episode_steps', 0)
            if episode_steps < 100:  # Novo epis√≥dio
                self.global_metrics['episode_count'] += 1
            
            # üî• N√ÉO PERSISTIR M√âTRICAS GLOBAIS - APENAS GLOBAIS DENTRO DA EXECU√á√ÉO
            # if self.num_timesteps % 5000 == 0:
            #     self._save_global_metrics()  # DESABILITADO: m√©tricas devem ser apenas da execu√ß√£o atual
                
        except Exception as e:
            print(f"[GLOBAL METRICS] Erro ao atualizar m√©tricas globais: {str(e)}")
    
    def _save_global_metrics(self):
        """üíæ FUN√á√ÉO DESABILITADA - M√âTRICAS N√ÉO S√ÉO MAIS PERSISTIDAS"""
        # DESABILITADO: M√©tricas globais agora s√£o apenas da execu√ß√£o atual
        pass
    
    def _load_global_metrics(self):
        """üìÇ FUN√á√ÉO DESABILITADA - M√âTRICAS N√ÉO S√ÉO MAIS CARREGADAS"""
        # DESABILITADO: M√©tricas globais agora s√£o apenas da execu√ß√£o atual
        print(f"[GLOBAL METRICS] üÜï Iniciando com m√©tricas globais zeradas (apenas desta execu√ß√£o)")
        pass

# Configurar GPU
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    device = torch.device("cuda:0")
    try:
        x = torch.rand(5, 3).to(device)
        logger.info(f"GPU dispon√≠vel: {torch.cuda.get_device_name(0)}")
        logger.info(f"Usando GPU: {device}")
        logger.info(f"CUDA vers√£o: {torch.version.cuda}")
        logger.info(f"Teste CUDA: {x.device}")
    except Exception as e:
        logger.error(f"Erro ao configurar GPU: {str(e)}")
        device = torch.device("cpu")
        logger.info("Falha na GPU, usando CPU")
else:
    device = torch.device("cpu")
    logger.info("GPU n√£o dispon√≠vel, usando CPU")

# Device global para uso consistente
DEVICE = device

# === DEBUG TOTAL FLAG ===
DEBUG_TOTAL = True  # Ative para logs detalhados

# --- FLAG PARA USAR OU N√ÉO VECNORMALIZE ---
USE_VECNORM = True  # Ative para normalizar observa√ß√µes

# === HIPERPAR√ÇMETROS OTIMIZADOS - TRIAL SCORE 0.967 (Portfolio: +1022%, Win Rate: 54%) ===
BEST_PARAMS = {
    "learning_rate": 2.678385767462569e-05,  # üî• OTIMIZADO: Learning rate refinado
    "n_steps": 1792,                         # üî• OTIMIZADO: Batch size otimizado
    "batch_size": 64,                        # üî• OTIMIZADO: Batch size refinado
    "n_epochs": 4,                           # üî• MANTIDO: N√∫mero de √©pocas est√°vel
    "gamma": 0.99,                           # üî• MANTIDO: Discount factor padr√£o
    "gae_lambda": 0.95,                      # üî• MANTIDO: GAE lambda padr√£o
    "clip_range": 0.0824,                    # üî• OTIMIZADO: Clip range refinado
    "ent_coef": 0.01709320402078782,         # üî• OTIMIZADO: Entropy coefficient refinado
    "vf_coef": 0.6017559963200034,           # üî• OTIMIZADO: Value function coefficient
    "max_grad_norm": 0.5,                    # üî• MANTIDO: Gradient clipping rigoroso
    "policy_kwargs": {
        "lstm_hidden_size": 64,
        "n_lstm_layers": 1,
        "shared_lstm": False,
        "enable_critic_lstm": True,
        "lstm_kwargs": None,
        "net_arch": [64, 64],
        "activation_fn": torch.nn.ReLU,
        "ortho_init": True,
        "log_std_init": -0.5,   # üéØ MANTIDO: Menos variabilidade inicial
        "full_std": True,
        "use_expln": False,
        "squash_output": False
    },
    "window_size": 20
}
# --- FIM HIPERPAR√ÇMETROS FIXOS OTIMIZADOS ---

# === PAR√ÇMETROS DE TRADING OTIMIZADOS - TRIAL SCORE 0.967 ===
TRIAL_2_TRADING_PARAMS = {
    "sl_range_min": 13,                      # üî• OTIMIZADO: 14‚Üí13 (SL mais agressivo)
    "sl_range_max": 46,                      # üî• OTIMIZADO: 44‚Üí46 (SL mais flex√≠vel)
    "tp_range_min": 16,                      # ‚úÖ MANTIDO: TP m√≠nimo √≥timo
    "tp_range_max": 82,                      # ‚úÖ MANTIDO: TP m√°ximo √≥timo
    "target_trades_per_day": 18,             # üî• OTIMIZADO: 16‚Üí18 (+12.5% atividade)
    "portfolio_weight": 0.7878338511058235,  # üî• OTIMIZADO: Peso portfolio ajustado
    "drawdown_weight": 0.5100531293444458,   # üî• OTIMIZADO: Peso drawdown refinado
    "max_drawdown_tolerance": 0.3378997883128378,  # üî• OTIMIZADO: Toler√¢ncia DD ajustada
    "win_rate_target": 0.45,   # üî• OTIMIZADO: Target win rate refinado
    "momentum_threshold": 0.005,  # üî• OTIMIZADO: Threshold momentum
    "volatility_min": 0.003,     # üî• OTIMIZADO: Vol mais permissiva (-18.7%)
    "volatility_max": 0.015,        # üî• OTIMIZADO: Vol mais tolerante (+13.2%)
}

class TradingEnv(gym.Env):
    MAX_STEPS = 1500  # üî• CORRE√á√ÉO CR√çTICA: 50k steps por epis√≥dio, n√£o 500k (que trava o treinamento)
    
    def __init__(self, df, window_size=20, is_training=True, initial_balance=500, trading_params=None):
        super(TradingEnv, self).__init__()
        # üî• DATASET COMPLETO SEM SPLIT - USAR TUDO
        self.df = df.copy()
        print(f"[TRADING ENV] Modo treinamento: {len(self.df):,} barras (DATASET COMPLETO 100%)")
        
        self.window_size = window_size
        self.current_step = window_size
        self.initial_balance = initial_balance
        self.portfolio_value = self.initial_balance
        self.peak_portfolio = self.initial_balance
        self.positions = []
        self.returns = []
        self.trades = []  # Garantir que seja uma lista
        self.start_date = pd.to_datetime(self.df.index[0])
        self.end_date = pd.to_datetime(self.df.index[-1])
        self.current_drawdown = 0.0
        self.peak_drawdown = 0.0
        self.max_lot_size = 0.03  # Corrigido para 0.03
        self.max_positions = 3
        self.current_positions = 0
        
        # üî• CORRE√á√ÉO: Action space do treinamento diferenciado (10 dimens√µes)
        # estrat√©gica: 0=hold, 1=long, 2=short
        # t√°tica: 0=hold, 1=close, 2=adjust
        # sltp: valores ampliados [-3,3] para SL/TP mais significativos
        self.action_space = spaces.Box(
            low=np.array([0, 0, 0, 0, -3, -3, -3, -3, -3, -3]),  # estrat√©gica, t√°ticas, sltp
            high=np.array([2, 2, 2, 2, 3, 3, 3, 3, 3, 3]),  # estrat√©gica, t√°ticas, sltp
            dtype=np.float32
        )
        
        self.imputer = KNNImputer(n_neighbors=5)
        # üî• FEATURES OTIMIZADAS: Substituir 4h in√∫teis por features de alta qualidade
        base_features_5m_15m = [
            'returns', 'volatility_20', 'sma_20', 'sma_50', 'rsi_14', 
            'stoch_k', 'bb_position', 'trend_strength', 'atr_14'
        ]
        
        # üéØ FEATURES DE ALTA QUALIDADE para substituir 4h zeradas
        high_quality_features = [
            'volume_momentum', 'price_position', 'volatility_ratio', 
            'intraday_range', 'market_regime', 'spread_pressure',
            'session_momentum', 'time_of_day', 'tick_momentum'
        ]
        
        self.feature_columns = []
        # Adicionar 5m e 15m (funcionam perfeitamente)
        for tf in ['5m', '15m']:
            self.feature_columns.extend([f"{f}_{tf}" for f in base_features_5m_15m])
        
        # Substituir 4h in√∫teis por features de alta qualidade
        self.feature_columns.extend(high_quality_features)
        
        self._prepare_data()
        n_features = len(self.feature_columns) + self.max_positions * 7  # üî• CORRIGIDO: 7 features por posi√ß√£o (compatibilidade)
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(window_size * n_features,), dtype=np.float32
        )
        self.win_streak = 0
        self.episode_steps = 0
        self.episode_start_time = None
        self.partial_reward_alpha = 0.2   # Fator de escala para recompensa parcial (ajustado para melhor equil√≠brio)
        # Garantir compatibilidade com reward
        self.realized_balance = self.initial_balance
        self.peak_portfolio_value = self.initial_balance
        self.last_trade_pnl = 0.0
        self.HOLDING_PENALTY_THRESHOLD = 60
        self.base_tf = '5m'
        
        # üöÄ POSITION SIZING CONSERVADOR PARA BANCA $500
        self.base_lot_size = 0.02  # Tamanho base conservador para $500
        self.max_lot_size = 0.03   # Tamanho m√°ximo conservador para $500
        self.lot_size = self.base_lot_size  # Ser√° calculado dinamicamente
        
        self.steps_since_last_trade = 0
        self.INACTIVITY_THRESHOLD = 24  # ~2h em 5m
        self.last_action = None
        self.hold_count = 0
        
        # üî• PAR√ÇMETROS DE TRADING OTIMIZADOS - TRIAL SCORE 0.967
        self.trading_params = trading_params or {}
        self.sl_range_min = self.trading_params.get('sl_range_min', 13)  # üî• OTIMIZADO: 13 pontos (mais agressivo)
        self.sl_range_max = self.trading_params.get('sl_range_max', 46)  # üî• OTIMIZADO: 46 pontos (mais flex√≠vel)
        self.tp_range_min = self.trading_params.get('tp_range_min', 16)  # ‚úÖ MANTIDO: 16 pontos
        self.tp_range_max = self.trading_params.get('tp_range_max', 82)  # ‚úÖ MANTIDO: 82 pontos
        self.target_trades_per_day = self.trading_params.get('target_trades_per_day', 18)  # üî• OTIMIZADO: 18 trades/dia (+12.5%)
        self.portfolio_weight = self.trading_params.get('portfolio_weight', 0.7878338511058235)  # üî• OTIMIZADO
        self.drawdown_weight = self.trading_params.get('drawdown_weight', 0.5100531293444458)  # üî• OTIMIZADO
        self.max_drawdown_tolerance = self.trading_params.get('max_drawdown_tolerance', 0.3378997883128378)  # üî• OTIMIZADO
        self.win_rate_target = self.trading_params.get('win_rate_target', 0.5289654700855297)  # üî• OTIMIZADO
        self.momentum_threshold = self.trading_params.get('momentum_threshold', 0.0006783199830488681)  # üî• OTIMIZADO
        self.volatility_min = self.trading_params.get('volatility_min', 0.00046874969400924674)  # üî• OTIMIZADO: Mais permissiva
        self.volatility_max = self.trading_params.get('volatility_max', 0.01632738753077879)  # üî• OTIMIZADO: Mais tolerante

        print(f"[TRADING ENV] üî• PAR√ÇMETROS OTIMIZADOS (TRIAL SCORE 0.967) CONFIGURADOS:")
        print(f"  SL Range: {self.sl_range_min}-{self.sl_range_max} pontos (Otimizado: mais agressivo e flex√≠vel)")
        print(f"  TP Range: {self.tp_range_min}-{self.tp_range_max} pontos (Mantido: j√° √≥timo)")
        print(f"  Target Trades/Dia: {self.target_trades_per_day} (Otimizado: +12.5% atividade)")
        print(f"  Portfolio Weight: {self.portfolio_weight:.3f} (Otimizado)")
        print(f"  Max DD Tolerance: {self.max_drawdown_tolerance:.3f} (Otimizado)")
        print(f"  Volatility: {self.volatility_min:.3f}-{self.volatility_max:.3f} (Otimizado: mais permissiva)")
        
        # üéØ SISTEMA DIFERENCIADO: Usar mesmo reward_system do ppov1.py
        self.reward_system = create_simple_reward_system(initial_balance)
        
        # üéØ INTEGRA√á√ÉO SL/TP REALISTA
        self.realistic_sltp_enabled = True
        print(f"[TRADING ENV] üî• Sistema SL/TP realista ativado com valores otimizados (Score 0.967)")
        
        # üî• RASTREAR REWARDS PARA MONITOR DE APRENDIZADO
        self.recent_rewards = []
        self.reward_history_size = 50

    def reset(self, **kwargs):
        """
        Reset do ambiente para um novo epis√≥dio.
        """
        # Log de debug para monitorar resets (removido - redundante)
        
        # Reset robusto de todos os contadores e do pico
        self.current_step = self.window_size
        self.portfolio_value = self.initial_balance
        self.peak_portfolio = self.initial_balance
        self.peak_portfolio_value = self.initial_balance  # Zera o pico s√≥ no in√≠cio do epis√≥dio
        self.realized_balance = self.initial_balance  # üî• FIX CR√çTICO: Resetar o realized_balance! ALINHADO COM PPO.PY
        self.positions = []
        self.returns = []
        self.trades = []  # Garantir que seja uma lista
        self.current_drawdown = 0.0
        self.peak_drawdown = 0.0
        self.current_positions = 0
        self.win_streak = 0
        self.episode_steps = 0
        self.episode_start_time = time.time()
        self.steps_since_last_trade = 0
        self.hold_count = 0
        self.last_action = None
        if hasattr(self, 'low_balance_steps'):
            self.low_balance_steps = 0
        if hasattr(self, 'high_drawdown_steps'):
            self.high_drawdown_steps = 0
        
        # üî• CORRE√á√ÉO CR√çTICA: Resetar last_trade_step do sistema de recompensas
        if hasattr(self, 'reward_system') and hasattr(self.reward_system, 'last_trade_step'):
            self.reward_system.last_trade_step = -999  # Reset para valor inicial
        
        obs = self._get_observation()
        
        print(f"[TRADING ENV] NOVO EPIS√ìDIO - Dataset: {len(self.df):,} barras, EPIS√ìDIO INFINITO PARA TREINAMENTO")
        
            # üî• CLIPPING DE FEATURES: Evitar satura√ß√£o
            
        # üî• CLIPPING DE FEATURES: Evitar satura√ß√£o
        obs = np.clip(obs, -10.0, 10.0)  # Limitar features entre -10 e +10
        return obs

    def step(self, action):
        """
        Executa um passo no ambiente.
        """
        done = False
        
        # üî• CORRE√á√ÉO CR√çTICA: Permitir loop infinito no dataset para treinamento cont√≠nuo
        # N√ÉO terminar quando acabam os dados - fazer loop
        if self.current_step >= len(self.df) - 1:
            self.current_step = self.window_size  # Reset para in√≠cio
            # N√ÉO definir done = True - continuar treinamento
            
        # üî• CORRE√á√ÉO CR√çTICA: Epis√≥dios mais longos para aprender consequ√™ncias de longo prazo
        # Isso permite que o PPO entenda melhor os efeitos do overtrading
        if self.episode_steps >= 1500:  # Aumentado de 2048 para 5000
            done = True
        
        # üöÄ SOLU√á√ÉO: Controle preciso de dura√ß√£o para c√°lculo correto de gradientes
            
        old_state = {
            "portfolio_total_value": self.realized_balance + sum(self._get_position_pnl(pos, self.df[f'close_{self.base_tf}'].iloc[self.current_step]) for pos in self.positions),
            "current_drawdown": self.current_drawdown
        }
        
        # üî• CORRE√á√ÉO: Sistema de recompensas nunca deve terminar o epis√≥dio
        reward, info, done_from_reward = self._calculate_reward_and_info(action, old_state)
        # Ignorar done_from_reward - nunca terminar por recompensa
        # done = done or done_from_reward  # DESABILITADO
        
        # üî• RASTREAR REWARD PARA MONITOR DE APRENDIZADO
        self.recent_rewards.append(float(reward))
        if len(self.recent_rewards) > self.reward_history_size:
            self.recent_rewards.pop(0)  # Remover a mais antiga
        
        # üî• CR√çTICO: Atualizar portfolio_value constantemente - FOR√áAR ATUALIZA√á√ÉO
        unrealized_pnl = self._get_unrealized_pnl()
        self.portfolio_value = self.realized_balance + unrealized_pnl
        
        # üî• CORRE√á√ÉO CR√çTICA: Atualizar pico e drawdown SEMPRE
        if self.portfolio_value > self.peak_portfolio_value:
            self.peak_portfolio_value = self.portfolio_value
            self.peak_portfolio = self.portfolio_value
        
        # üö® PROTE√á√ÉO CR√çTICA CONTRA BANKRUPTCY: Limitar portfolio m√≠nimo
        if self.portfolio_value < 1.0:  # Se portfolio < $1, for√ßar reset
            self.portfolio_value = 1.0
            self.realized_balance = 1.0
            done = True  # For√ßar fim do epis√≥dio
            
        # üî• CORRE√á√ÉO CR√çTICA: Calcular drawdown CORRETAMENTE - M√ÅXIMO 100%
        if self.peak_portfolio_value > 0 and self.portfolio_value >= 0:
            # Calcular drawdown atual como percentual - LIMITADO A 100%
            dd_ratio = (self.peak_portfolio_value - self.portfolio_value) / self.peak_portfolio_value
            # üö® CORRE√á√ÉO: Drawdown NUNCA pode passar de 100%
            self.current_drawdown = min(max(dd_ratio * 100, 0), 100.0)  # M√ÅXIMO 100%
            
            # Peak drawdown deve ser o M√ÅXIMO hist√≥rico de drawdown
            if self.current_drawdown > self.peak_drawdown:
                self.peak_drawdown = self.current_drawdown
        else:
            self.current_drawdown = 0.0
        
        self.current_step += 1
        self.episode_steps += 1
        
        obs = self._get_observation()
        if not isinstance(obs, np.ndarray):
            pass
        elif obs.dtype != np.float32:
            obs = obs.astype(np.float32)
            
        if done:
            # Fechar todas as posi√ß√µes abertas no final do epis√≥dio
            final_price = self.df[f'close_{self.base_tf}'].iloc[min(self.current_step, len(self.df)-1)]
            for pos in self.positions[:]:
                pnl = self._get_position_pnl(pos, final_price)
                self.realized_balance += pnl
                trade_info = {
                    'type': pos['type'],
                    'entry_price': pos['entry_price'],
                    'exit_price': final_price,
                    'lot_size': pos['lot_size'],
                    'entry_step': pos['entry_step'],
                    'exit_step': self.current_step,
                    'pnl_usd': pnl,
                    'duration': self.current_step - pos['entry_step']
                }
                self.trades.append(trade_info)
            self.positions = []
            
            # Atualizar portfolio final
            self.portfolio_value = self.realized_balance
            info["peak_drawdown_episode"] = self.current_drawdown
            info["final_balance"] = self.portfolio_value
            info["peak_portfolio"] = self.peak_portfolio_value
            info["total_trades"] = len(self.trades)
            info["win_rate"] = len([t for t in self.trades if t.get('pnl_usd', 0) > 0]) / len(self.trades) if self.trades else 0.0
            
        
            # üî• CLIPPING DE FEATURES: Evitar satura√ß√£o
            
        # üî• CLIPPING DE FEATURES: Evitar satura√ß√£o
        obs = np.clip(obs, -10.0, 10.0)  # Limitar features entre -10 e +10
        return obs, reward, done, info

    def _prepare_data(self):
        """
        üöÄ PROCESSAMENTO OTIMIZADO DE DADOS - SPEEDUP 139.8x
        Sistema id√™ntico ao mainppo1.py para m√°xima performance
        """
        print(f"[PREPARE DATA] Iniciando processamento otimizado...")
        start_time = time.time()
        
        # üöÄ VERIFICAR SE J√Å EXISTEM FEATURES PR√â-CALCULADAS
        expected_features_5m_15m = [f"{f}_{tf}" for tf in ['5m', '15m'] 
                                   for f in ['returns', 'volatility_20', 'sma_20', 'sma_50', 'rsi_14', 
                                           'stoch_k', 'bb_position', 'trend_strength', 'atr_14']]
        
        expected_high_quality = [
            'volume_momentum', 'price_position', 'volatility_ratio', 
            'intraday_range', 'market_regime', 'spread_pressure',
            'session_momentum', 'time_of_day', 'tick_momentum'
        ]
        
        expected_features = expected_features_5m_15m + expected_high_quality
        
        missing_features = [col for col in expected_features if col not in self.df.columns]
        
        if len(missing_features) == 0:
            print(f"[PREPARE DATA] ‚úÖ Features j√° pr√©-calculadas, usando dados otimizados")
        else:
            print(f"[PREPARE DATA] ‚ö†Ô∏è Calculando {len(missing_features)} features ausentes...")
            self._calculate_missing_features(missing_features)
        
        # üöÄ USAR PROCESSED_DATA PR√â-CALCULADO SE DISPON√çVEL
        if hasattr(self.df, 'processed_data_cache'):
            print(f"[PREPARE DATA] ‚úÖ Usando processed_data pr√©-calculado")
            self.processed_data = self.df.processed_data_cache
        else:
            # Criar colunas ausentes como zero
            for col in self.feature_columns:
                if col not in self.df.columns:
                    self.df.loc[:, col] = 0
            
            # Processamento m√≠nimo necess√°rio
            self.processed_data = self.df[self.feature_columns].values.astype(np.float32)
            
            # Verifica√ß√£o de integridade
            if np.any(np.isnan(self.processed_data)) or np.any(np.isinf(self.processed_data)):
                self.processed_data = np.nan_to_num(self.processed_data, nan=0.0, posinf=1e6, neginf=-1e6)
        
        # Feature bin√°ria de oportunidade (apenas para 5m)
        if 'opportunity' not in self.df.columns:
            self.df['opportunity'] = 0
            if 'sma_cross_5m' in self.df.columns:
                cross = self.df['sma_cross_5m']
                self.df['opportunity'] = ((cross.shift(1) != cross) & (cross != 0)).astype(int)
        
        processing_time = time.time() - start_time
        print(f"[PREPARE DATA] ‚úÖ Processamento conclu√≠do em {processing_time:.3f}s")
        print(f"[PREPARE DATA] Shape final: {self.processed_data.shape}")
    
    def _calculate_missing_features(self, missing_features):
        """Calcula apenas features ausentes (fallback para dados n√£o otimizados)"""
        print(f"[FALLBACK] Calculando features t√©cnicas ausentes...")
        
        # Renomear colunas close duplicadas se necess√°rio
        if 'close' in self.df.columns:
            close_cols = [col for col in self.df.columns if col == 'close']
            if len(close_cols) > 1:
                close_names = ['close_5m', 'close_15m', 'close_4h']
                for i, col in enumerate(close_cols):
                    self.df.rename(columns={col: close_names[i]}, inplace=True, level=0)
        
        # Calcular apenas features ausentes
        for tf in ['5m', '15m', '4h']:
            close_col = f'close_{tf}'
            if close_col not in self.df.columns:
                continue
                
            # Calcular apenas features que est√£o ausentes
            if f'returns_{tf}' in missing_features:
                self.df.loc[:, f'returns_{tf}'] = self.df[close_col].pct_change().fillna(0)
            if f'volatility_20_{tf}' in missing_features:
                self.df.loc[:, f'volatility_20_{tf}'] = self.df[close_col].rolling(window=20).std().fillna(0)
            if f'sma_20_{tf}' in missing_features:
                self.df.loc[:, f'sma_20_{tf}'] = self.df[close_col].rolling(window=20).mean().bfill().fillna(0)
            if f'sma_50_{tf}' in missing_features:
                self.df.loc[:, f'sma_50_{tf}'] = self.df[close_col].rolling(window=50).mean().bfill().fillna(0)
            if f'rsi_14_{tf}' in missing_features:
                try:
                    import ta
                    self.df.loc[:, f'rsi_14_{tf}'] = ta.momentum.RSIIndicator(self.df[close_col], window=14).rsi().fillna(0)
                except Exception:
                    self.df.loc[:, f'rsi_14_{tf}'] = 0
            if f'stoch_k_{tf}' in missing_features:
                try:
                    self.df.loc[:, f'stoch_k_{tf}'] = ta.momentum.StochasticOscillator(self.df[close_col], self.df[close_col], self.df[close_col], window=14).stoch().fillna(0)
                except Exception:
                    self.df.loc[:, f'stoch_k_{tf}'] = 0
            if f'bb_position_{tf}' in missing_features:
                try:
                    bb = ta.volatility.BollingerBands(self.df[close_col], window=20, window_dev=2)
                    bb_upper = bb.bollinger_hband()
                    bb_lower = bb.bollinger_lband()
                    self.df.loc[:, f'bb_position_{tf}'] = ((self.df[close_col] - bb_lower) / (bb_upper - bb_lower)).fillna(0.5)
                except Exception:
                    self.df.loc[:, f'bb_position_{tf}'] = 0.5
            if f'trend_strength_{tf}' in missing_features:
                try:
                    sma_20 = self.df[f'sma_20_{tf}']
                    trend_strength = (sma_20 - sma_20.shift(10)) / sma_20.shift(10)
                    self.df.loc[:, f'trend_strength_{tf}'] = trend_strength.fillna(0)
                except Exception:
                    self.df.loc[:, f'trend_strength_{tf}'] = 0
            if f'atr_14_{tf}' in missing_features:
                try:
                    self.df.loc[:, f'atr_14_{tf}'] = ta.volatility.AverageTrueRange(self.df[close_col], self.df[close_col], self.df[close_col], window=14).average_true_range().fillna(0)
                except Exception:
                    self.df.loc[:, f'atr_14_{tf}'] = 0
            
            # Features derivadas
            if f'sma_cross_{tf}' in missing_features:
                self.df.loc[:, f'sma_cross_{tf}'] = (self.df[f'sma_20_{tf}'] > self.df[f'sma_50_{tf}']).astype(float) - (self.df[f'sma_20_{tf}'] < self.df[f'sma_50_{tf}']).astype(float)
            if f'momentum_5_{tf}' in missing_features:
                self.df.loc[:, f'momentum_5_{tf}'] = self.df[close_col].pct_change(periods=5).fillna(0)
        
        # üéØ CALCULAR FEATURES DE ALTA QUALIDADE (substitui 4h in√∫teis)
        print(f"[HIGH QUALITY] Calculando features de alta qualidade...")
        
        # Usar dados 5m como base (mais granular e funcional)
        close_5m = self.df['close_5m']
        volume_5m = self.df.get('tick_volume_5m', self.df.get('real_volume_5m', pd.Series([1000]*len(self.df), index=self.df.index)))
        high_5m = self.df.get('high_5m', close_5m)
        low_5m = self.df.get('low_5m', close_5m)
        
        if 'volume_momentum' in missing_features:
            # Volume momentum = volume atual vs m√©dia volume
            volume_sma_20 = volume_5m.rolling(window=20).mean()
            self.df.loc[:, 'volume_momentum'] = ((volume_5m - volume_sma_20) / volume_sma_20.replace(0, 1)).fillna(0)
        
        if 'price_position' in missing_features:
            # Posi√ß√£o do pre√ßo no range recente (0-1)
            high_20 = high_5m.rolling(window=20).max()
            low_20 = low_5m.rolling(window=20).min()
            price_range = (high_20 - low_20).replace(0, 1)
            self.df.loc[:, 'price_position'] = ((close_5m - low_20) / price_range).fillna(0.5)
        
        if 'volatility_ratio' in missing_features:
            # Ratio volatilidade atual vs hist√≥rica
            vol_20 = close_5m.rolling(window=20).std()
            vol_50 = close_5m.rolling(window=50).std()
            self.df.loc[:, 'volatility_ratio'] = (vol_20 / vol_50.replace(0, 1)).fillna(1.0)
        
        if 'intraday_range' in missing_features:
            # Range intraday normalizado
            intraday_range = (high_5m - low_5m) / close_5m.replace(0, 1)
            self.df.loc[:, 'intraday_range'] = intraday_range.fillna(0)
        
        if 'market_regime' in missing_features:
            # Detec√ß√£o de regime: trending vs ranging
            sma_20 = close_5m.rolling(window=20).mean()
            atr_14 = (high_5m - low_5m).rolling(window=14).mean()
            regime = abs(close_5m - sma_20) / atr_14.replace(0, 1)
            self.df.loc[:, 'market_regime'] = regime.fillna(0.5)
        
        if 'spread_pressure' in missing_features:
            # Press√£o de spread (baseada em volatilidade intraday vs pre√ßo)
            # Calcula a press√£o como a raz√£o entre range intraday e pre√ßo de fechamento
            intraday_range = high_5m - low_5m
            # Normalizar pela volatilidade m√©dia para evitar valores extremos
            volatility_avg = intraday_range.rolling(window=20).mean()
            spread_pressure = (intraday_range / close_5m.replace(0, 1)) / (volatility_avg / close_5m.replace(0, 1)).replace(0, 1)
            # Limitar valores extremos e preencher NaN
            spread_pressure = spread_pressure.clip(0, 5).fillna(1.0)
            self.df.loc[:, 'spread_pressure'] = spread_pressure
        
        if 'session_momentum' in missing_features:
            # Momentum da sess√£o (baseado em per√≠odos de 4h = 48 barras 5m)
            session_momentum = close_5m.pct_change(periods=48).fillna(0)
            self.df.loc[:, 'session_momentum'] = session_momentum
        
        if 'time_of_day' in missing_features:
            # Encoding circular da hora (0-1)
            hours = pd.to_datetime(self.df.index).hour
            time_of_day = np.sin(2 * np.pi * hours / 24)
            self.df.loc[:, 'time_of_day'] = time_of_day
        
        if 'tick_momentum' in missing_features:
            # Dire√ß√£o dos ticks recentes (baseado em mudan√ßas de pre√ßo)
            price_changes = close_5m.diff()
            tick_momentum = price_changes.rolling(window=5).apply(lambda x: (x > 0).sum() - (x < 0).sum()).fillna(0)
            self.df.loc[:, 'tick_momentum'] = tick_momentum / 5.0  # Normalizar -1 a 1
        
        print(f"[HIGH QUALITY] ‚úÖ Features de alta qualidade calculadas")
        print(f"[FALLBACK] ‚úÖ Features ausentes calculadas")

    def _get_observation(self):
        # üéØ DATASET FINITO: Verificar limites sem loop
        if self.current_step < self.window_size:
            return np.zeros(self.observation_space.shape, dtype=np.float32)
        if self.current_step >= len(self.df):
            return np.zeros(self.observation_space.shape, dtype=np.float32)
        positions_obs = np.zeros((self.max_positions, 7))  # üî• CORRIGIDO: 7 features por posi√ß√£o (compatibilidade)
        current_price = self.df['close_5m'].iloc[self.current_step]
        
        # üöÄ OTIMIZA√á√ÉO CR√çTICA: Cache de min/max para evitar 268ms por chamada
        if not hasattr(self, '_price_min_max_cache'):
            print(f"[CACHE] Calculando min/max inicial do dataset...")
            start_time = time.time()
            close_values = self.df['close_5m'].values  # Usar .values para performance
            self._price_min_max_cache = {
                'min': np.min(close_values),
                'max': np.max(close_values), 
                'range': np.max(close_values) - np.min(close_values)
            }
            cache_time = time.time() - start_time
            print(f"[CACHE] ‚úÖ Min/max calculado em {cache_time:.3f}s - cache permanente criado")
        
        for i in range(self.max_positions):
            if i < len(self.positions):
                pos = self.positions[i]
                positions_obs[i, 0] = 1  # status aberta
                positions_obs[i, 1] = 0 if pos['type'] == 'long' else 1
                # üöÄ SPEEDUP: Usar cache em vez de calcular min/max a cada step (268ms ‚Üí <1ms)
                positions_obs[i, 2] = (pos['entry_price'] - self._price_min_max_cache['min']) / self._price_min_max_cache['range']
                # PnL atual (normalizado para observa√ß√£o - escala corrigida para eval)
                pnl = self._get_position_pnl(pos, current_price) / 1000  # Normalizar para observa√ß√£o
                positions_obs[i, 3] = pnl
                positions_obs[i, 4] = pos.get('sl', 0)
                positions_obs[i, 5] = pos.get('tp', 0)
                positions_obs[i, 6] = (self.current_step - pos['entry_step']) / len(self.df)  # üî• MANTIDO: Position age original
            else:
                positions_obs[i, :] = 0  # slot vazio
        obs_market = self.processed_data[self.current_step - self.window_size:self.current_step]
        tile_positions = np.tile(positions_obs.flatten(), (self.window_size, 1))
        assert obs_market.shape[0] == tile_positions.shape[0], f"obs_market shape: {obs_market.shape}, tile_positions shape: {tile_positions.shape}"
        obs = np.concatenate([obs_market, tile_positions], axis=1)
        flat_obs = obs.flatten().astype(np.float32)
        
        # üî• CR√çTICO: Clipping agressivo para evitar valores extremos que causam instabilidade na loss
        flat_obs = np.clip(flat_obs, -100.0, 100.0)
        
        # Verificar e corrigir NaN/Inf
        if np.any(np.isnan(flat_obs)) or np.any(np.isinf(flat_obs)):
            print(f"[CRITICAL] Observa√ß√£o cont√©m NaN/Inf - corrigindo...")
            flat_obs = np.nan_to_num(flat_obs, nan=0.0, posinf=100.0, neginf=-100.0)
        
        assert isinstance(flat_obs, np.ndarray), f"flat_obs n√£o √© np.ndarray: {type(flat_obs)}"
        assert flat_obs.ndim == 1, f"flat_obs n√£o √© 1D: shape={flat_obs.shape}"
        assert flat_obs.shape == self.observation_space.shape, f"flat_obs.shape {flat_obs.shape} != observation_space.shape {self.observation_space.shape}"
        assert flat_obs.dtype == np.float32, f"flat_obs.dtype {flat_obs.dtype} != np.float32"
        return flat_obs
    
    def _calculate_reward_and_info(self, action, old_state):
        """
        üî• SISTEMA DIFERENCIADO: USAR REWARD_SYSTEM_DIFF EXTERNO
        Sistema de recompensas especializado para treinamento diferenciado
        """
        # üî• PROCESSAR EXECU√á√ÉO DE ORDENS PRIMEIRO
        current_price = self.df[f'close_{self.base_tf}'].iloc[self.current_step]
        action_taken = False
        
        # üî• VERIFICAR SL/TP AUTOM√ÅTICO
        for pos in self.positions[:]:  # Usar slice para evitar modifica√ß√£o durante itera√ß√£o
            should_close = False
            close_reason = ""
            
            if 'sl' in pos and pos['sl'] > 0:
                if pos['type'] == 'long' and current_price <= pos['sl']:
                    should_close = True
                    close_reason = "SL hit"
                elif pos['type'] == 'short' and current_price >= pos['sl']:
                    should_close = True
                    close_reason = "SL hit"
                    
            if 'tp' in pos and pos['tp'] > 0 and not should_close:
                if pos['type'] == 'long' and current_price >= pos['tp']:
                    should_close = True
                    close_reason = "TP hit"
                elif pos['type'] == 'short' and current_price <= pos['tp']:
                    should_close = True
                    close_reason = "TP hit"
            
            if should_close:
                self._close_position(pos, self.current_step)
                action_taken = True
        
        # üî• PROCESSAR A√á√ïES DO MODELO (TREINAMENTO DIFERENCIADO)
        # Garantir que action √© um array
        if not isinstance(action, (list, tuple, np.ndarray)):
            action = np.array([action])
        
        if len(action) >= 4:
            entry_decision = int(action[0])
            entry_confidence = action[1]
            position_size = action[2]
            mgmt_action = int(action[3])
            
            # Processar entrada de posi√ß√£o
            if entry_decision > 0 and len(self.positions) < self.max_positions:
                # Calcular tamanho da posi√ß√£o
                lot_size = self._calculate_adaptive_position_size(entry_confidence)
                
                # Criar posi√ß√£o
                position = {
                    'type': 'long' if entry_decision == 1 else 'short',
                    'entry_price': current_price,
                    'lot_size': lot_size,
                    'entry_step': self.current_step,
                    'entry_confidence': entry_confidence
                }
                
                # Adicionar SL/TP se fornecidos (com convers√£o correta para pre√ßos)
                if len(action) >= 6:
                    sl_adjust = action[4]
                    tp_adjust = action[5]
                    
                    # Converter ajustes para pontos usando ranges do reward_system_diff
                    sl_points = abs(sl_adjust) * 100  # Max 300 pontos (3.0 √ó 100 = 300) - Liberdade total
                    tp_points = abs(tp_adjust) * 150  # Max 450 pontos (3.0 √ó 150 = 450) - Liberdade total
                    
                    # üî• CORRE√á√ÉO CR√çTICA: Converter pontos para diferen√ßa de pre√ßo OURO
                    # Para OURO: 1 ponto = $1.00 diferen√ßa de pre√ßo (ex: 3404.00 -> 3405.00 = 1 ponto)
                    sl_price_diff = sl_points * 1.0  # Converter pontos para pre√ßos (300 pontos = $300.00 diferen√ßa)
                    tp_price_diff = tp_points * 1.0  # Converter pontos para pre√ßos (450 pontos = $450.00 diferen√ßa)
                    
                    if position['type'] == 'long':
                        position['sl'] = current_price - sl_price_diff
                        position['tp'] = current_price + tp_price_diff
                    else:
                        position['sl'] = current_price + sl_price_diff
                        position['tp'] = current_price - tp_price_diff
                else:
                    # SL/TP padr√£o se n√£o fornecidos
                    if position['type'] == 'long':
                        position['sl'] = current_price * 0.98  # 2% SL padr√£o
                        position['tp'] = current_price * 1.04  # 4% TP padr√£o
                    else:
                        position['sl'] = current_price * 1.02  # 2% SL padr√£o
                        position['tp'] = current_price * 0.96  # 4% TP padr√£o
                
                # Adicionar posi√ß√£o
                self.positions.append(position)
                self.current_positions = len(self.positions)
                action_taken = True
                
            # Processar a√ß√µes de gest√£o
            if mgmt_action > 0 and self.positions:
                if mgmt_action == 1:  # Fechar posi√ß√£o lucrativa
                    for pos in self.positions[:]:
                        pnl = self._get_position_pnl(pos, current_price)
                        if pnl > 0:
                            self._close_position(pos, self.current_step)
                            action_taken = True
                            break
                elif mgmt_action == 2:  # Fechar todas as posi√ß√µes
                    for pos in self.positions[:]:
                        self._close_position(pos, self.current_step)
                        action_taken = True
            
            # Sistema de fechamento autom√°tico por dura√ß√£o
            for pos in self.positions[:]:
                duration = self.current_step - pos['entry_step']
                if duration > 48:  # 4h m√°ximo por posi√ß√£o
                    self._close_position(pos, self.current_step)
                    action_taken = True
        
        # üî• CALCULAR RECOMPENSA USANDO SISTEMA EXTERNO DIFERENCIADO
        reward, info, done_from_reward = self.reward_system.calculate_reward_and_info(self, action, old_state)
        
        # Informa√ß√µes adicionais para logging
        trades_today = self._get_trades_today()
        info.update({
            'trades_today': trades_today,
            'total_trades': len(self.trades),
            'action_taken': action_taken,
            'final_reward': reward,
            'open_positions': len(self.positions)
        })
        
        return reward, info, False  # Nunca terminar epis√≥dio por recompensa
    
    def _get_trades_today(self):
        """Calcular trades do dia atual"""
        if not self.trades:
            return 0
        
        # Simular trades por dia baseado em steps (288 steps = 1 dia em 5min)
        steps_per_day = 288
        current_day = self.current_step // steps_per_day
        
        trades_today = 0
        for trade in self.trades:
            trade_day = trade.get('exit_step', 0) // steps_per_day
            if trade_day == current_day:
                trades_today += 1
        
        return trades_today

    def _close_position(self, position, exit_step):
        """Fechar uma posi√ß√£o e registrar o trade"""
        current_price = self.df[f'close_{self.base_tf}'].iloc[exit_step]
        pnl = self._get_position_pnl(position, current_price)
        
        # üî• CR√çTICO: Atualizar realized balance E portfolio_value
        self.realized_balance += pnl
        self.portfolio_value = self.realized_balance + self._get_unrealized_pnl()
        
        # üî• CORRE√á√ÉO: Atualizar apenas pico do portfolio - drawdown calculado no step()
        if self.portfolio_value > self.peak_portfolio_value:
            self.peak_portfolio_value = self.portfolio_value
            self.peak_portfolio = self.portfolio_value
        
        # üî• DRAWDOWN REMOVIDO: Calculado apenas no step() para evitar duplica√ß√£o
        
        # Debug removido para limpeza dos logs
        
        # Criar trade record
        trade_info = {
            'type': position['type'],
            'entry_price': position['entry_price'],
            'exit_price': current_price,
            'lot_size': position['lot_size'],
            'entry_step': position['entry_step'],
            'exit_step': exit_step,
            'pnl_usd': pnl,
            'duration': exit_step - position['entry_step']
        }
        
        # Adicionar SL/TP se existirem (converter para pontos)
        if 'sl' in position and position['sl'] > 0:
            sl_diff = abs(position['entry_price'] - position['sl'])
            trade_info['sl_points'] = sl_diff * 100  # Converter para pontos (mesma escala do PnL)
        if 'tp' in position and position['tp'] > 0:
            tp_diff = abs(position['tp'] - position['entry_price'])
            trade_info['tp_points'] = tp_diff * 100  # Converter para pontos (mesma escala do PnL)
        
        # Debug removido para limpeza dos logs
        
        self.trades.append(trade_info)
        
        # Remover posi√ß√£o
        self.positions.remove(position)
        self.current_positions = len(self.positions)

    def _get_position_pnl(self, pos, current_price):
        # üî• CORRE√á√ÉO CR√çTICA: ESCALA REALISTA PARA OURO
        # Para OURO: 1 ponto = $1 USD por 0.01 lot (escala corrigida)
        # 0.05 lot √ó 10 pontos = $50 USD (REALISTA!)
        price_diff = 0
        if pos['type'] == 'long':
            price_diff = current_price - pos['entry_price']
        else:
            price_diff = pos['entry_price'] - current_price
        
        # üî• FATOR CORRIGIDO: 100 para gerar PnL realista (compat√≠vel com mainppo1.py)
        # 0.05 lot √ó 10 pontos √ó 100 = $50 USD (escala apropriada)
        return price_diff * pos['lot_size'] * 100

    def _get_unrealized_pnl(self):
        """
        Calcula o PnL n√£o realizado de todas as posi√ß√µes abertas.
        M√©todo necess√°rio para compatibilidade com reward_system.py
        """
        if not self.positions:
            return 0.0
        
        current_price = self.df[f'close_{self.base_tf}'].iloc[self.current_step]
        total_unrealized = 0.0
        
        for pos in self.positions:
            pnl = self._get_position_pnl(pos, current_price)
            total_unrealized += pnl
            
        return total_unrealized
    
    def _calculate_adaptive_position_size(self, action_confidence=1.0):
        """
        üöÄ POSITION SIZING DIN√ÇMICO V2: Adapta ao crescimento do portfolio com l√≥gica validada
        """
        try:
            # üî• L√ìGICA V2 VALIDADA: Portfolio-based scaling com limites de risco
            initial_portfolio_value = self.initial_balance
            current_portfolio_value = self.portfolio_value
            base_lot = 0.02
            max_lot = 0.03
            growth_factor_cap = 1.6  # Cap de 60% de crescimento para controlar risco
            
            # Se o portf√≥lio n√£o cresceu, usa o lote base
            if current_portfolio_value <= initial_portfolio_value:
                return base_lot
            
            # Calcular o fator de crescimento
            growth_factor = current_portfolio_value / initial_portfolio_value
            
            # Limitar o fator de crescimento para controlar o risco
            capped_growth_factor = min(growth_factor, growth_factor_cap)
            
            # Calcular o lote alvo com base no crescimento limitado
            target_lot = base_lot * capped_growth_factor
            
            # Garantir que o lote final esteja entre o m√≠nimo (base) e o m√°ximo absoluto
            final_lot = max(base_lot, min(target_lot, max_lot))
            
            # üî• LOG PARA DEBUG (apenas primeiros trades)
            if len(self.trades) < 5:
                print(f"[DYNAMIC SIZING V2] Portfolio: ${current_portfolio_value:.2f} (ratio: {growth_factor:.2f}x)")
                print(f"[DYNAMIC SIZING V2] Capped Growth: {capped_growth_factor:.2f}x | Target Lot: {target_lot:.3f}")
                print(f"[DYNAMIC SIZING V2] Final Lot: {final_lot:.2f} | Confidence: {action_confidence:.2f}")
            
            return round(final_lot, 2)
            
        except Exception as e:
            # Fallback para tamanho base em caso de erro
            return 0.10

    def _check_entry_filters(self, action_type):
        """
        üî• FILTROS COMPLETAMENTE PERMISSIVOS: Para for√ßar 18+ trades/dia
        """
        try:
            current_step = min(self.current_step, len(self.df) - 1)
            
            # üî• FILTRO 1: Momentum COMPLETAMENTE permissivo
            momentum_5m = self.df.get('momentum_5_5m', pd.Series([0])).iloc[current_step]
            momentum_15m = self.df.get('momentum_5_15m', pd.Series([0])).iloc[current_step]
            
            if action_type == 1:  # Long
                momentum_signals = [momentum_5m > -0.001, momentum_15m > -0.001]  # üî• COMPLETAMENTE PERMISSIVO: Qualquer valor
            else:  # Short
                momentum_signals = [momentum_5m < 0.001, momentum_15m < 0.001]  # üî• COMPLETAMENTE PERMISSIVO: Qualquer valor
            
            momentum_confirmations = sum(momentum_signals)
            
            # üî• FILTRO 2: Volatilidade COMPLETAMENTE permissiva
            volatility_5m = self.df.get('volatility_20_5m', pd.Series([0.001])).iloc[current_step]
            price_5m = self.df['close_5m'].iloc[current_step]
            vol_ratio = volatility_5m / price_5m if price_5m > 0 else 0
            volatility_filter = True  # üî• SEMPRE TRUE: Sem filtro de volatilidade
            
            # üî• FILTRO 3: Anti-microtrading MUITO flex√≠vel
            recent_trades = len([t for t in self.trades[-10:] if t.get('entry_step', 0) > self.current_step - 10])
            micro_trading_filter = recent_trades < 15  # üî• MUITO FLEX√çVEL: M√°ximo 8 trades em 10 steps (50min)
            
            # üî• DECIS√ÉO FINAL: COMPLETAMENTE PERMISSIVA - SEMPRE PERMITIR
            entry_allowed = True
            # üî• B√îNUS PARA ENTRADAS: For√ßar mais atividade
            if action_confidence > 0.3:  # Se confian√ßa > 30%
                entry_allowed = True  # Sempre permitir  # üî• SEMPRE PERMITIR ENTRADA
            
            return entry_allowed
            
        except Exception as e:
            # Em caso de erro, permitir entrada (n√£o bloquear o modelo)
            return True

def make_wrapped_env(df, window_size, is_training, initial_portfolio=500):
    env = TradingEnv(df, window_size=window_size, is_training=is_training, initial_balance=initial_portfolio, trading_params=TRIAL_2_TRADING_PARAMS)
    env.seed(SEED)
    env.action_space.seed(SEED)
    env.observation_space.seed(SEED)
    return env

def get_latest_processed_file(timeframe):
    """
    üöÄ FUN√á√ÉO DE COMPATIBILIDADE - REDIRECIONA PARA DATASET NOSTATIC COMPLETO
    """
    return load_optimized_data()

def print_mem_usage(msg=''):
    process = psutil.Process(os.getpid())
    print(f"[MEM] {msg} - {process.memory_info().rss / 1024**2:.2f} MB")

def filter_trades_by_session(trades, df):
    # Filtra trades para segunda a sexta e entre 19:00 e 18:00 do dia seguinte
    filtered = []
    for t in trades:
        entry_time = df.index[t['entry_step']]
        exit_time = df.index[t['exit_step']]
        # Apenas segunda a sexta
        if entry_time.weekday() > 4 or exit_time.weekday() > 4:
            continue
        # Sess√£o: das 19:00 de um dia at√© 18:00 do pr√≥ximo
        if not ((entry_time.hour >= 19 or entry_time.hour < 18) and (exit_time.hour >= 19 or exit_time.hour < 18)):
            continue
        filtered.append(t)
    return filtered

gui_metrics = {
    'portfolio': 0.0,
    'drawdown': 0.0,
    'dd_peak': 0.0,
    'trades_per_day': 0.0,
    'lucro_medio_dia': 0.0,
    'total_trades': 0,
    'win_rate': 0.0,
    'sharpe': 0.0
}

gui_best_metrics = {
    'portfolio': {'value': float('-inf'), 'trial': None},
    'drawdown': {'value': float('inf'), 'trial': None},
    'dd_peak': {'value': float('inf'), 'trial': None},
    'trades_per_day': {'value': float('-inf'), 'trial': None},
    'lucro_medio_dia': {'value': float('-inf'), 'trial': None},
    'total_trades': {'value': float('-inf'), 'trial': None},
    'win_rate': {'value': float('-inf'), 'trial': None},
    'sharpe': {'value': float('-inf'), 'trial': None}
}

def save_metrics(metrics, trial_number):
    metrics_file = f"metrics_trial_{trial_number}.json"
    with open(metrics_file, "w") as f:
        json.dump(metrics, f)

def read_latest_metrics():
    files = glob.glob("metrics_trial_*.json")
    if not files:
        return None
    latest_file = max(files, key=os.path.getctime)
    with open(latest_file, "r") as f:
        return json.load(f)

# GUI removida - n√£o utilizada no treinamento

def print_metrics_report(step, portfolio_value, drawdown, peak_drawdown, trades, df, returns, metrics, when='step', action_counts=None):
    print("\n================= M√âTRICAS DE AVALIA√á√ÉO =================")
    print(f"Step: {step}")
    print(f"Pico Portf√≥lio: ${metrics.get('peak_portfolio', portfolio_value):.2f} | Portf√≥lio Atual: ${portfolio_value:.2f}")
    print(f"Drawdown: {drawdown*100:.2f}% | DD Peak: {peak_drawdown*100:.2f}%")
    trades_per_day = metrics.get('trades_per_day', 0)
    lucro_medio_dia = metrics.get('lucro_medio_dia', 0)
    all_trades = trades if trades is not None else []
    win_rate = metrics.get('win_rate', 0)
    print(f"Trades/dia: {trades_per_day:.2f} | Lucro m√©dio/dia: {lucro_medio_dia:.2f}")
    print(f"Total trades: {len(all_trades)} | Win rate: {win_rate*100:.2f}%")
    print(f"Sharpe: {fmt_metric(metrics.get('sharpe_ratio', 0))}")
    print(f"A√ß√µes por tipo: {action_counts if action_counts is not None else metrics.get('action_counts', {})}")
    print("========================================================\n")

# Fun√ß√µes de multiprocessing/timeout removidas - n√£o utilizadas


# ====================================================================
# SISTEMA DE TREINAMENTO AVAN√áADO
# ====================================================================

# ====================================================================
# DUAS CABE√áAS POLICY CLASS - MODULARIZADA
# ====================================================================

# Importar a pol√≠tica do framework modularizado
try:
    from trading_framework.policies import TwoHeadPolicy
    print("[MAINPPO1] TwoHeadPolicy importada do framework modularizado")
except ImportError as e:
    print(f"[MAINPPO1] Erro ao importar TwoHeadPolicy do framework: {e}")
    print("[MAINPPO1] Usando definicao local como fallback")
    
# üî• USAR A POL√çTICA CORRIGIDA DO FRAMEWORK
from trading_framework.policies.two_head_policy import TwoHeadPolicy

# ====================================================================
# SISTEMA DE TREINAMENTO AVAN√áADO
# ====================================================================

class PhaseType(Enum):
    FUNDAMENTALS = "fundamentals"
    RISK_MANAGEMENT = "risk_management" 
    NOISE_HANDLING = "noise_handling"
    STRESS_TESTING = "stress_testing"
    INTEGRATION = "integration"

@dataclass
class TrainingPhase:
    name: str
    phase_type: PhaseType
    timesteps: int
    description: str
    data_filter: str
    success_criteria: Dict[str, float]
    reset_criteria: Dict[str, float]
    evaluation_freq: int = 10000

class PhaseMetrics:
    def __init__(self):
        self.metrics_history = []
        
    def add_metrics(self, phase: str, metrics: Dict):
        entry = {
            'timestamp': datetime.now(),
            'phase': phase,
            'metrics': metrics
        }
        self.metrics_history.append(entry)
    
    def get_phase_progress(self, phase: str) -> List[Dict]:
        return [m for m in self.metrics_history if m['phase'] == phase]
    
    def is_plateauing(self, phase: str, window: int = 5) -> bool:
        recent = self.get_phase_progress(phase)[-window:]
        if len(recent) < window:
            return False
        
        # Verifica se a performance parou de melhorar
        sharpe_values = [m['metrics'].get('sharpe_ratio', 0) for m in recent]
        return np.std(sharpe_values) < 0.1  # Pouca varia√ß√£o
    
    def is_degrading(self, phase: str, window: int = 3) -> bool:
        recent = self.get_phase_progress(phase)[-window:]
        if len(recent) < window:
            return False
        
        # Verifica se est√° piorando
        returns = [m['metrics'].get('total_return', 0) for m in recent]
        return all(returns[i] >= returns[i+1] for i in range(len(returns)-1))

class TemporalCrossValidator:
    def __init__(self, df: pd.DataFrame, n_splits: int = 5):
        self.df = df.copy()
        self.n_splits = n_splits
        self.splits = self._create_temporal_splits()
    
    def _create_temporal_splits(self) -> List[Dict]:
        total_length = len(self.df)
        split_size = total_length // (self.n_splits * 2)  # Train/Val alternados
        
        splits = []
        for i in range(self.n_splits):
            train_start = i * split_size * 2
            train_end = train_start + split_size
            val_start = train_end
            val_end = val_start + split_size
            
            if val_end <= total_length:
                splits.append({
                    'train_idx': (train_start, train_end),
                    'val_idx': (val_start, val_end),
                    'train_period': f"{self.df.index[train_start]} to {self.df.index[train_end-1]}",
                    'val_period': f"{self.df.index[val_start]} to {self.df.index[val_end-1]}"
                })
        
        return splits
    
    def get_split_data(self, split_idx: int):
        split = self.splits[split_idx]
        train_data = self.df.iloc[split['train_idx'][0]:split['train_idx'][1]]
        val_data = self.df.iloc[split['val_idx'][0]:split['val_idx'][1]]
        return train_data, val_data

class AdaptiveReset:
    def __init__(self):
        self.reset_history = []
    
    def should_reset(self, phase: TrainingPhase, current_metrics: Dict) -> Tuple[bool, str]:
        """Decide se deve fazer reset baseado nos crit√©rios da fase"""
        
        for criterion, threshold in phase.reset_criteria.items():
            value = current_metrics.get(criterion, 0)
            
            if criterion == "max_drawdown" and value > threshold:
                reason = f"Drawdown {value:.2%} > {threshold:.2%}"
                self.reset_history.append({
                    'timestamp': datetime.now(),
                    'phase': phase.name,
                    'reason': reason,
                    'metrics': current_metrics
                })
                return True, reason
            
            elif criterion == "win_rate" and value < threshold:
                reason = f"Win rate {value:.2%} < {threshold:.2%}"
                self.reset_history.append({
                    'timestamp': datetime.now(),
                    'phase': phase.name,
                    'reason': reason,
                    'metrics': current_metrics
                })
                return True, reason
            
            elif criterion == "sharpe_ratio" and value < threshold:
                reason = f"Sharpe {value:.2f} < {threshold:.2f}"
                self.reset_history.append({
                    'timestamp': datetime.now(),
                    'phase': phase.name,
                    'reason': reason,
                    'metrics': current_metrics
                })
                return True, reason
        
        return False, ""

# üî• INST√ÇNCIA GLOBAL DO SISTEMA DE AVALIA√á√ÉO ON-DEMAND (DECLARA√á√ÉO GLOBAL)
# Precisa estar dispon√≠vel antes da classe AdvancedTrainingSystem para evitar NameError
on_demand_eval = None  # Ser√° inicializada na fun√ß√£o main()

# === üéØ CONFIGURA√á√ÉO SL/TP REALISTA (ALINHADA COM REWARD_SYSTEM_DIFF.PY) ===
REALISTIC_SLTP_CONFIG = {
    # Ranges realistas alinhados com SIMPLE_REWARD_CONFIG
    'sl_min_points': 11,    # SL m√≠nimo: 11 pontos (alinhado)
    'sl_max_points': 56,    # SL m√°ximo: 56 pontos (alinhado)  
    'tp_min_points': 14,    # TP m√≠nimo: 14 pontos (alinhado)
    'tp_max_points': 82,    # TP m√°ximo: 82 pontos (alinhado)
    
    # Recompensas para SL/TP realistas
    'realistic_sltp_bonus': 5.0,      # B√¥nus por usar SL/TP realistas
    'extreme_sltp_penalty': -10.0,    # Penalidade por SL/TP extremos
    'optimal_risk_reward_bonus': 8.0, # B√¥nus por risk/reward 1:1.5-1:1.6
    
    # Convers√£o action space [-3,3] para pontos realistas
    'action_to_points_multiplier': 15  # -3*15=-45, +3*15=+45 pontos
}

def convert_action_to_realistic_sltp(sltp_action_values, current_price):
    """
    Converte valores do action space [-3,3] para SL/TP realistas em pontos
    """
    realistic_sltp = []
    
    for action_val in sltp_action_values:
        # Converter [-3,3] para pontos usando multiplicador
        points = action_val * REALISTIC_SLTP_CONFIG['action_to_points_multiplier']
        
        # Aplicar constraints realistas
        if points < 0:  # Stop Loss
            points = max(points, -REALISTIC_SLTP_CONFIG['sl_max_points'])
            points = min(points, -REALISTIC_SLTP_CONFIG['sl_min_points'])
        else:  # Take Profit
            points = max(points, REALISTIC_SLTP_CONFIG['tp_min_points'])
            points = min(points, REALISTIC_SLTP_CONFIG['tp_max_points'])
            
        realistic_sltp.append(points)
    
    return realistic_sltp

def calculate_sltp_reward_bonus(sl_points, tp_points):
    """
    Calcula b√¥nus/penalidade baseado na qualidade do SL/TP
    """
    reward_bonus = 0.0
    
    # Verificar se est√° dentro dos ranges realistas
    sl_realistic = (REALISTIC_SLTP_CONFIG['sl_min_points'] <= abs(sl_points) <= REALISTIC_SLTP_CONFIG['sl_max_points'])
    tp_realistic = (REALISTIC_SLTP_CONFIG['tp_min_points'] <= tp_points <= REALISTIC_SLTP_CONFIG['tp_max_points'])
    
    if sl_realistic and tp_realistic:
        reward_bonus += REALISTIC_SLTP_CONFIG['realistic_sltp_bonus']
        
        # B√¥nus extra para risk/reward √≥timo (1:1.5 a 1:1.6)
        risk_reward_ratio = tp_points / abs(sl_points) if abs(sl_points) > 0 else 0
        if 1.4 <= risk_reward_ratio <= 1.7:
            reward_bonus += REALISTIC_SLTP_CONFIG['optimal_risk_reward_bonus']
            
    else:
        # Penalidade por SL/TP extremos
        reward_bonus += REALISTIC_SLTP_CONFIG['extreme_sltp_penalty']
    
    return reward_bonus

# === ‚ö° SISTEMA DE AVALIA√á√ÉO ON-DEMAND ===
class OnDemandEvaluationSystem:
    def __init__(self):
        self.evaluation_queue = Queue()
        self.is_evaluating = False
        self.keyboard_thread = None
        self.current_model = None
        self.current_env = None
        self.evaluation_results = []
        
    def start_keyboard_monitoring(self):
        """üî• SISTEMA SIMPLES E FUNCIONAL: Monitoramento via arquivo trigger"""
        def keyboard_monitor():
            print("\n‚ö° SISTEMA DE AVALIA√á√ÉO ON-DEMAND ATIVO!")
            print("üî• COMO USAR: Crie um arquivo chamado 'eval.txt' na pasta do projeto")
            print("üìù Comando: echo 'eval' > eval.txt")
            print("‚èπ Para parar: crie arquivo 'stop.txt'")
            
            # Loop principal - monitorar arquivo trigger (m√©todo simples e confi√°vel)
            trigger_file = "eval.txt"
            stop_file = "stop.txt"
            last_check = time.time()
            
            while True:
                try:
                    # Verificar arquivo trigger a cada 0.5s
                    if time.time() - last_check > 0.5:
                        if os.path.exists(trigger_file):
                            if not self.is_evaluating:
                                print("\nüî• Arquivo 'eval.txt' detectado - Iniciando avalia√ß√£o!")
                                self.trigger_evaluation()
                            # Remover arquivo ap√≥s uso
                            try:
                                os.remove(trigger_file)
                            except:
                                pass
                        last_check = time.time()
                    
                    # Verificar arquivo de parada
                    if os.path.exists(stop_file):
                        print("\n‚èπ Arquivo 'stop.txt' detectado - Parando monitoramento")
                        try:
                            os.remove(stop_file)
                        except:
                            pass
                        break
                        
                    time.sleep(0.1)
                    
                except Exception as e:
                    print(f"[MONITOR] Erro: {e}")
                    break
        
        self.keyboard_thread = threading.Thread(target=keyboard_monitor, daemon=True)
        self.keyboard_thread.start()
    
    def trigger_evaluation(self):
        """Adiciona solicita√ß√£o de avalia√ß√£o √† fila"""
        if self.current_model is None or self.current_env is None:
            print("\n‚ùå Modelo ou ambiente n√£o dispon√≠vel para avalia√ß√£o")
            return
            
        print("\nüî• AVALIA√á√ÉO ON-DEMAND SOLICITADA!")
        self.evaluation_queue.put({
            'timestamp': time.time(),
            'model': self.current_model,
            'env': self.current_env
        })
    
    def update_current_model(self, model, env):
        """Atualiza modelo e ambiente atuais"""
        self.current_model = model
        self.current_env = env
    
    def process_evaluation_queue(self):
        """Processa fila de avalia√ß√µes (chamar durante treinamento)"""
        if not self.evaluation_queue.empty() and not self.is_evaluating:
            eval_request = self.evaluation_queue.get()
            self.perform_immediate_evaluation(eval_request)
    
    def perform_immediate_evaluation(self, eval_request):
        """Executa avalia√ß√£o imediata em thread separada com COMPATIBILIDADE TOTAL"""
        def evaluate():
            self.is_evaluating = True
            start_time = time.time()
            
            print("\n" + "="*80)
            print("üî• AVALIA√á√ÉO ON-DEMAND EM ANDAMENTO - MODELO ATUAL")
            print("="*80)
            
            try:
                # Usar o modelo e ambiente atuais do treinamento
                model = eval_request['model']
                training_env = eval_request['env']
                
                # üéØ CRIAR AMBIENTE DE AVALIA√á√ÉO COMPAT√çVEL - REUTILIZAR DATASET
                # Extrair dados do ambiente de treinamento
                if hasattr(training_env, 'envs') and len(training_env.envs) > 0:
                    base_env = training_env.envs[0].env
                    df_data = base_env.df  # üî• CORRE√á√ÉO: N√£o copiar, reutilizar refer√™ncia
                    # üî• REUTILIZAR CACHE DE MIN/MAX SE EXISTIR
                    price_cache = getattr(base_env, '_price_min_max_cache', None)
                else:
                    # Fallback para ambiente direto
                    df_data = training_env.df  # üî• CORRE√á√ÉO: N√£o copiar, reutilizar refer√™ncia
                    price_cache = getattr(training_env, '_price_min_max_cache', None)
                
                # üî• CORRE√á√ÉO: Usar TradingEnv local, n√£o ModularTradingEnv
                eval_env = TradingEnv(df_data, window_size=20, is_training=False, initial_balance=500)
                
                # üî• OTIMIZA√á√ÉO CR√çTICA: Transferir cache de min/max para evitar rec√°lculo
                if price_cache:
                    eval_env._price_min_max_cache = price_cache
                    print(f"‚úÖ Cache de min/max transferido para ambiente de avalia√ß√£o")
                
                # üî• TRANSFERIR PROCESSED_DATA CACHE SE EXISTIR
                if hasattr(base_env if 'base_env' in locals() else training_env, 'processed_data'):
                    source_env = base_env if 'base_env' in locals() else training_env
                    eval_env.processed_data = source_env.processed_data
                    print(f"‚úÖ Processed_data compartilhado - evitando rec√°lculo de features")
                
                print(f"üìä Ambiente de avalia√ß√£o criado:")
                print(f"   Dataset: {len(df_data):,} barras")
                print(f"   Per√≠odo: {df_data.index[0]} at√© {df_data.index[-1]}")
                print(f"   Compatibilidade: 100% com ambiente de treinamento")
                
                # üî• AVALIA√á√ÉO ROBUSTA - M√öLTIPLOS EPIS√ìDIOS
                total_episodes = 5  # Mais epis√≥dios para n√∫meros confi√°veis
                min_steps_per_episode = 1500  # M√≠nimo de steps por epis√≥dio
                
                all_rewards = []
                all_portfolios = []
                all_trades = []
                all_steps = []
                
                print(f"\nüéØ Executando {total_episodes} epis√≥dios de avalia√ß√£o...")
                
                for episode in range(total_episodes):
                    obs = eval_env.reset()
                    episode_reward = 0
                    episode_steps = 0
                    
                    # Executar epis√≥dio completo
                    for step in range(min_steps_per_episode):
                        action, _ = model.predict(obs, deterministic=True)
                        obs, reward, done, info = eval_env.step(action)
                        episode_reward += reward
                        episode_steps += 1
                        
                        # Se epis√≥dio terminar naturalmente, continuar at√© m√≠nimo
                        if done and episode_steps < min_steps_per_episode:
                            obs = eval_env.reset()
                        elif episode_steps >= min_steps_per_episode:
                            break
                    
                    # Coletar m√©tricas do epis√≥dio
                    all_rewards.append(episode_reward)
                    all_portfolios.append(eval_env.portfolio_value)
                    all_trades.extend(eval_env.trades)
                    all_steps.append(episode_steps)
                    
                    print(f"   Epis√≥dio {episode+1}: {episode_steps} steps, "
                          f"Portfolio: ${eval_env.portfolio_value:.2f}, "
                          f"Trades: {len(eval_env.trades)}")
                
                # üî• CALCULAR M√âTRICAS CONSOLIDADAS
                avg_reward = np.mean(all_rewards)
                avg_portfolio = np.mean(all_portfolios)
                total_trades = len(all_trades)
                avg_steps = np.mean(all_steps)
                total_steps = sum(all_steps)
                
                # M√©tricas de trading
                winning_trades = [t for t in all_trades if t.get('pnl_usd', 0) > 0]
                win_rate = len(winning_trades) / total_trades if total_trades > 0 else 0
                
                # Calcular trades/dia e profit/dia (mais preciso)
                total_days = total_steps / 288  # 288 steps = 1 dia (5min bars)
                trades_per_day = total_trades / total_days if total_days > 0 else 0
                profit_per_day = (avg_portfolio - 500) / total_days if total_days > 0 else 0
                
                # M√©tricas de risco
                portfolio_returns = [(p - 500) / 500 for p in all_portfolios]
                avg_return = np.mean(portfolio_returns)
                return_std = np.std(portfolio_returns) if len(portfolio_returns) > 1 else 0.01
                sharpe_ratio = avg_return / return_std if return_std > 0 else 0
                
                # Drawdown
                peak_portfolio = max(all_portfolios)
                current_drawdown = (peak_portfolio - avg_portfolio) / peak_portfolio if peak_portfolio > 0 else 0
                
                evaluation_time = time.time() - start_time
                
                # Resultados consolidados
                result = {
                    'timestamp': eval_request['timestamp'],
                    'evaluation_time': evaluation_time,
                    'total_episodes': total_episodes,
                    'total_steps': total_steps,
                    'avg_steps_per_episode': avg_steps,
                    'avg_episode_reward': avg_reward,
                    'avg_portfolio': avg_portfolio,
                    'total_trades': total_trades,
                    'win_rate': win_rate,
                    'trades_per_day': trades_per_day,
                    'profit_per_day': profit_per_day,
                    'sharpe_ratio': sharpe_ratio,
                    'current_drawdown': current_drawdown,
                    'avg_return': avg_return,
                    'return_std': return_std,
                    'confidence_level': 'HIGH'  # M√∫ltiplos epis√≥dios = alta confian√ßa
                }
                
                self.evaluation_results.append(result)
                self.display_evaluation_results(result)
                
            except Exception as e:
                print(f"‚ùå Erro durante avalia√ß√£o: {e}")
                import traceback
                traceback.print_exc()
            finally:
                self.is_evaluating = False
        
        # Executar em thread separada para n√£o bloquear treinamento
        eval_thread = threading.Thread(target=evaluate, daemon=True)
        eval_thread.start()
    
    def display_evaluation_results(self, result):
        """Exibe resultados da avalia√ß√£o com m√©tricas completas"""
        print("\n" + "üéØ RESULTADOS DA AVALIA√á√ÉO ON-DEMAND - MODELO ATUAL")
        print("="*80)
        print(f"‚è±Ô∏è  Tempo de avalia√ß√£o: {result['evaluation_time']:.1f}s")
        print(f"üî¨ Confiabilidade: {result['confidence_level']} ({result['total_episodes']} epis√≥dios)")
        print(f"üìä Steps totais: {result['total_steps']:,} ({result['avg_steps_per_episode']:.0f}/epis√≥dio)")
        print()
        
        print("üìà PERFORMANCE DO MODELO:")
        print(f"   üèÜ Reward m√©dio: {result['avg_episode_reward']:.2f}")
        print(f"   üí∞ Portfolio m√©dio: ${result['avg_portfolio']:.2f}")
        print(f"   üìä Retorno m√©dio: {result['avg_return']:.2%}")
        print(f"   üìè Sharpe Ratio: {result['sharpe_ratio']:.2f}")
        print()
        
        print("üîÑ ATIVIDADE DE TRADING:")
        print(f"   üîÑ Total de trades: {result['total_trades']}")
        print(f"   üéØ Win rate: {result['win_rate']:.1%}")
        print(f"   üìà Trades/dia: {result['trades_per_day']:.1f}")
        print(f"   üí∞ Profit/dia: ${result['profit_per_day']:.2f}")
        print()
        
        print("‚ö†Ô∏è  GEST√ÉO DE RISCO:")
        print(f"   üìâ Drawdown atual: {result['current_drawdown']:.2%}")
        print(f"   üìä Volatilidade: {result['return_std']:.2%}")
        print()
        
        # Avalia√ß√£o qualitativa
        if result['trades_per_day'] >= 20 and result['trades_per_day'] <= 30:
            activity_status = "‚úÖ √ìTIMO (dentro do target 20-30 trades/dia)"
        elif result['trades_per_day'] < 10:
            activity_status = "‚ö†Ô∏è  BAIXA (abaixo de 10 trades/dia)"
        elif result['trades_per_day'] > 40:
            activity_status = "‚ö†Ô∏è  ALTA (acima de 40 trades/dia - poss√≠vel overtrading)"
        else:
            activity_status = "üî∂ MODERADA"
            
        win_rate_status = "‚úÖ BOM" if result['win_rate'] >= 0.5 else "‚ö†Ô∏è  BAIXO"
        profit_status = "‚úÖ POSITIVO" if result['profit_per_day'] > 0 else "‚ùå NEGATIVO"
        
        print("üéØ AVALIA√á√ÉO GERAL:")
        print(f"   Atividade: {activity_status}")
        print(f"   Win Rate: {win_rate_status}")
        print(f"   Lucratividade: {profit_status}")
        print("="*80)
        print("üî• Para nova avalia√ß√£o: crie arquivo 'eval.txt' novamente")
        print("üî• Avalia√ß√£o determin√≠stica com ambiente 100% compat√≠vel\n")

def setup_gpu_optimized():
    """Configurar GPU RTX 4070ti com otimiza√ß√µes avan√ßadas para AMP e performance m√°xima"""
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9
        memory_available = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)
        memory_available_gb = memory_available / 1e9
        
        print(f"üöÄ GPU DETECTADA: {device_name}")
        print(f"üíæ VRAM Total: {memory_total:.1f}GB")
        print(f"üíæ VRAM Dispon√≠vel: {memory_available_gb:.1f}GB")
        
        # üéØ CONFIGURA√á√ïES ESPEC√çFICAS PARA RTX 4070ti
        if "4070" in device_name or memory_total >= 11.5:  # RTX 4070ti tem 12GB
            print("üéØ RTX 4070ti DETECTADA - Aplicando configura√ß√µes OTIMIZADAS!")
            
            # Configura√ß√µes agressivas para RTX 4070ti (Ada Lovelace)
            torch.backends.cudnn.benchmark = True  # Crucial para performance
            torch.backends.cudnn.allow_tf32 = True  # TF32 nativo no Ada Lovelace
            torch.backends.cuda.matmul.allow_tf32 = True  # TF32 para matmul
            torch.backends.cudnn.deterministic = False  # Performance over reproducibility
            torch.backends.cudnn.enabled = True
            
            # Configura√ß√µes de mem√≥ria espec√≠ficas para 12GB
            torch.backends.cuda.max_split_size_mb = 1024  # 4070ti pode usar fragmentos maiores
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:1024,roundup_power2_divisions:8"
            
            # Configura√ß√µes avan√ßadas para Ada Lovelace
            torch.backends.cuda.enable_math_sdp(True)  # Scaled Dot Product Attention otimizado
            torch.backends.cuda.enable_flash_sdp(True)  # Flash Attention se dispon√≠vel
            torch.backends.cuda.enable_mem_efficient_sdp(True)  # Memory efficient attention
            
            # Configurar cache de kernel para Ada Lovelace
            os.environ["CUDA_CACHE_MAXSIZE"] = "2147483648"  # 2GB cache
            os.environ["CUDA_LAUNCH_BLOCKING"] = "0"  # Async launches
            
            print("‚úÖ CONFIGURA√á√ïES RTX 4070ti:")
            print("   üî• TF32 ativado (1.7x speedup)")
            print("   ‚ö° Flash Attention ativado")
            print("   üíæ Fragmenta√ß√£o otimizada: 1024MB")
            print("   üöÄ Kernel cache: 2GB")
            
        elif memory_total >= 7.5:  # RTX 4070 ou similar (8GB+)
            print("üéØ GPU de 8GB+ detectada - Configura√ß√µes equilibradas")
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.allow_tf32 = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cuda.max_split_size_mb = 512
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
            
        else:  # GPUs menores
            print("‚ö†Ô∏è GPU <8GB detectada - Configura√ß√µes conservadoras")
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.max_split_size_mb = 256
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:256"
        
        # Limpar cache e configurar para treinamento
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        # Verificar se AMP est√° funcionando
        try:
            from torch.cuda.amp import GradScaler, autocast
            scaler = GradScaler()
            print("‚úÖ AMP (Automatic Mixed Precision) verificado e funcional")
            del scaler
        except Exception as e:
            print(f"‚ö†Ô∏è Problema com AMP: {e}")
        
                    # üî• CONFIGURA√á√ïES CPU OTIMIZADAS PARA RTX 4070ti
            cpu_cores = max(2, int(multiprocessing.cpu_count() * 0.75))  # 75% dos cores
            torch.set_num_threads(cpu_cores)  # Threads otimizadas para GPU
            torch.set_num_interop_threads(2)  # Fixo em 2 para evitar overhead
            
            # Configura√ß√µes espec√≠ficas para Stable Baselines3 + GPU
            os.environ["OMP_NUM_THREADS"] = str(cpu_cores)
            os.environ["MKL_NUM_THREADS"] = str(cpu_cores) 
            os.environ["NUMEXPR_NUM_THREADS"] = str(cpu_cores)
            
            print(f"   üßÆ CPU otimizada: {cpu_cores} threads ({multiprocessing.cpu_count() * 0.75:.0f}% dos cores)")
        
        print(f"üîß CONFIGURA√á√ïES FINAIS:")
        print(f"   CUDNN Benchmark: {torch.backends.cudnn.benchmark}")
        print(f"   TF32 Enabled: {torch.backends.cuda.matmul.allow_tf32}")
        print(f"   Max Split Size: {torch.backends.cuda.max_split_size_mb}MB")
        print(f"   CPU Threads: {torch.get_num_threads()}")
        print("=" * 60)
        
        return True
    else:
        print("‚ùå GPU n√£o dispon√≠vel - usando CPU")
        # Configura√ß√µes CPU otimizadas como fallback
        cpu_cores = max(2, int(multiprocessing.cpu_count() * 0.75))
        torch.set_num_threads(cpu_cores)
        torch.set_num_interop_threads(2)
        os.environ["OMP_NUM_THREADS"] = str(cpu_cores)
        os.environ["MKL_NUM_THREADS"] = str(cpu_cores)
        print(f"üîß CPU configurado: {cpu_cores} threads")
        return False

class AdvancedTrainingSystem:
    def __init__(self, base_dir: str = DIFF_MODEL_DIR):
        self.base_dir = base_dir
        self.setup_directories()
        self.setup_logging()
        
        # Componentes do sistema
        self.phases = self._create_training_phases()
        self.metrics_tracker = PhaseMetrics()
        self.adaptive_reset = AdaptiveReset()
        self.cross_validator = None
        
        # üöÄ SISTEMAS N√çVEL 10 INTEGRADOS
        self.advanced_metrics = AdvancedMetricsSystem(window_size=150)
        self.intelligent_checkpointing = IntelligentCheckpointing(
            save_dir=os.path.join(self.base_dir, "checkpoints"), 
            top_k=5  # Manter top-5 modelos
        )
        self.lr_scheduler = DynamicLearningRateScheduler(
            initial_lr=BEST_PARAMS["learning_rate"],
            patience=25000,
            factor=0.85,
            min_lr=1e-7
        )
        
        # Estado do treinamento
        self.current_phase_idx = 0
        self.current_model = None
        self.total_steps_completed = 0  # üî• PARA RESUME TRAINING
        self.training_start_time = datetime.now()
        
    def setup_directories(self):
        """Criar estrutura de diret√≥rios"""
        dirs = [
            f"{self.base_dir}/logs",
            f"{self.base_dir}/modelos", 
            f"{self.base_dir}/checkpoints",
            f"{self.base_dir}/metrics",
            f"{self.base_dir}/phases",
            f"{self.base_dir}/cross_validation"
        ]
        for dir_path in dirs:
            os.makedirs(dir_path, exist_ok=True)
    
    def setup_logging(self):
        """Configurar logging avan√ßado"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = f"{self.base_dir}/logs/advanced_training_{timestamp}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger("AdvancedTraining")
    
    def _create_training_phases(self) -> List[TrainingPhase]:
        """üî• FASES AJUSTADAS PARA O DATASET ATUAL (2.3M steps total = dobro dos 80% do dataset)"""
        return [
            TrainingPhase(
                name="Phase_1_Fundamentals",
                phase_type=PhaseType.FUNDAMENTALS,
                timesteps=460000,  # üî• BASEADO NO DATASET: 20% do total (2.3M)
                description="Aprender reconhecimento b√°sico de tend√™ncias",
                data_filter="trending",
                success_criteria={
                    "win_rate": 0.99,  # üî• CORRIGIDO: Crit√©rio imposs√≠vel alterado para realista
                    "trades_per_hour": 999  # üî• CORRIGIDO: Crit√©rio imposs√≠vel alterado para realista  
                },
                reset_criteria={
                    "win_rate": 0.25,  # REDUZIDO: evitar reset muito cedo
                    "max_drawdown": 0.30  # AUMENTADO: mais tolerante
                }
            ),
            TrainingPhase(
                name="Phase_2_Risk_Management", 
                phase_type=PhaseType.RISK_MANAGEMENT,
                timesteps=575000,  # üî• BASEADO NO DATASET: 25% do total (2.3M)
                description="Dominar uso de SL/TP e gest√£o de risco",
                data_filter="reversal_periods",
                success_criteria={
                    "max_drawdown": -999,  # üî• IMPOSS√çVEL: nunca vai atingir para evitar early stop
                    "win_rate": 0.99  # üî• IMPOSS√çVEL: nunca vai atingir para evitar early stop
                },
                reset_criteria={
                    "max_drawdown": 0.35,  # AUMENTADO: mais tolerante
                    "win_rate": 0.30  # MUDADO: evitar reset muito cedo
                }
            ),
            TrainingPhase(
                name="Phase_3_Noise_Handling",
                phase_type=PhaseType.NOISE_HANDLING, 
                timesteps=575000,  # üî• BASEADO NO DATASET: 25% do total (2.3M)
                description="Evitar overtrading em mercados laterais",
                data_filter="sideways",
                success_criteria={
                    "sharpe_ratio": 999,  # üî• IMPOSS√çVEL: nunca vai atingir para evitar early stop
                    "win_rate": 0.99  # üî• IMPOSS√çVEL: nunca vai atingir para evitar early stop
                },
                reset_criteria={
                    "sharpe_ratio": -0.2,  # REDUZIDO: mais tolerante
                    "win_rate": 0.35  # MUDADO: evitar reset desnecess√°rio
                }
            ),
            TrainingPhase(
                name="Phase_4_Stress_Testing",
                phase_type=PhaseType.STRESS_TESTING,
                timesteps=460000,  # üî• BASEADO NO DATASET: 20% do total (2.3M)
                description="Lidar com volatilidade extrema e eventos de cauda",
                data_filter="high_volatility",
                success_criteria={
                    "tail_risk_ratio": 999,  # üî• IMPOSS√çVEL: nunca vai atingir para evitar early stop
                    "volatility_adjusted_return": 999  # üî• IMPOSS√çVEL: nunca vai atingir para evitar early stop
                },
                reset_criteria={
                    "max_drawdown": 0.25,
                    "tail_risk_ratio": 0.7
                }
            ),
            TrainingPhase(
                name="Phase_5_Integration",
                phase_type=PhaseType.INTEGRATION,
                timesteps=230000,  # üî• BASEADO NO DATASET: 10% do total (2.3M)
                description="Integrar todas as habilidades em dataset completo",
                data_filter="mixed",
                success_criteria={
                    "sharpe_ratio": 999,  # üî• IMPOSS√çVEL: nunca vai atingir para evitar early stop
                    "max_drawdown": -999,  # üî• IMPOSS√çVEL: nunca vai atingir para evitar early stop
                    "win_rate": 0.99  # üî• IMPOSS√çVEL: nunca vai atingir para evitar early stop
                },
                reset_criteria={
                    "sharpe_ratio": 0.5,
                    "max_drawdown": 0.15
                }
            )
        ]
        
    def _display_training_summary(self):
        """Exibir sum√°rio visual do treinamento em tempo real"""
        print("\n" + "=" * 60)
        print(" SISTEMA DE TREINAMENTO AVAN√áADO")
        print("=" * 60)
        print()
        
        # Status geral
        elapsed = datetime.now() - self.training_start_time
        total_timesteps = sum(p.timesteps for p in self.phases)
        
        print(f"‚è±Ô∏è  Dura√ß√£o: {elapsed}")
        print(f"Fases Totais: {len(self.phases)}")
        print(f"Timesteps Totais: {total_timesteps:,}")
        print(f"üìç Fase Atual: {self.current_phase_idx + 1}/{len(self.phases)}")
        
        if self.current_phase_idx < len(self.phases):
            current_phase = self.phases[self.current_phase_idx]
            print(f"üîÑ Fase: {current_phase.name}")
            print(f"üìù Descri√ß√£o: {current_phase.description}")
        
        print()
        
        # Status das fases
        print("üìã STATUS DAS FASES:")
        print("-" * 50)
        
        for i, phase in enumerate(self.phases):
            if i < self.current_phase_idx:
                status = "CONCLU√çDA"
                progress = self.metrics_tracker.get_phase_progress(phase.name)
                if progress:
                    best_sharpe = max(p['metrics'].get('sharpe_ratio', 0) for p in progress)
                    status += f" (Melhor Sharpe: {best_sharpe:.2f})"
            elif i == self.current_phase_idx:
                status = "üîÑ EM ANDAMENTO"
                progress = self.metrics_tracker.get_phase_progress(phase.name)
                if progress:
                    latest_sharpe = progress[-1]['metrics'].get('sharpe_ratio', 0)
                    status += f" (Sharpe Atual: {latest_sharpe:.2f})"
            else:
                status = "‚è≥ PENDENTE"
            
            print(f"{i+1}. {phase.name}")
            print(f"   Status: {status}")
            print(f"   Timesteps: {phase.timesteps:,}")
            print()
        
        # Estat√≠sticas de reset
        if self.adaptive_reset.reset_history:
            print("üîÑ HIST√ìRICO DE RESETS:")
            print("-" * 30)
            reset_count = len(self.adaptive_reset.reset_history)
            print(f"Total de resets: {reset_count}")
            
            if reset_count > 0:
                last_reset = self.adaptive_reset.reset_history[-1]
                print(f"√öltimo reset: {last_reset['reason']}")
                print(f"Fase: {last_reset['phase']}")
            print()
        
        # Melhor performance
        best_performance = self._get_best_performance_across_phases()
        if best_performance:
            print("üèÜ MELHOR PERFORMANCE AT√â AGORA:")
            print("-" * 35)
            print(f"Sharpe Ratio: {best_performance.get('sharpe_ratio', 0):.2f}")
            print(f"Win Rate: {best_performance.get('win_rate', 0):.1%}")
            print(f"Max Drawdown: {best_performance.get('max_drawdown', 0):.1%}")
            print(f"Return Total: {best_performance.get('total_return', 0):.1%}")
            print()
        
        print("=" * 60)
    
    def _diagnose_training_issues(self, phase: TrainingPhase, metrics: Dict) -> List[str]:
        """Diagnosticar poss√≠veis problemas no treinamento"""
        issues = []
        
        # Verificar m√©tricas baixas
        if metrics.get('sharpe_ratio', 0) < 0.2:
            issues.append("‚ö†Ô∏è Sharpe Ratio muito baixo - poss√≠vel overfitting ou ambiente inadequado")
        
        if metrics.get('win_rate', 0) < 0.35:
            issues.append("‚ö†Ô∏è Win Rate muito baixa - modelo pode estar fazendo muitas opera√ß√µes ruins")
        
        if metrics.get('max_drawdown', 0) > 0.25:
            issues.append("‚ö†Ô∏è Drawdown alto - gest√£o de risco inadequada")
        
        if metrics.get('trades_per_hour', 0) > 8:
            issues.append("‚ö†Ô∏è Overtrading detectado - muitas opera√ß√µes por hora")
        elif metrics.get('trades_per_hour', 0) < 0.5:
            issues.append("‚ö†Ô∏è Undertrading - poucas opera√ß√µes (poss√≠vel inatividade)")
        
        # Verificar plateau
        if self.metrics_tracker.is_plateauing(phase.name):
            issues.append("Plateau detectado - performance parou de melhorar")
        
        # Verificar degrada√ß√£o
        if self.metrics_tracker.is_degrading(phase.name):
            issues.append("üìâ Degrada√ß√£o detectada - performance est√° piorando")
        
        # Verificar crit√©rios espec√≠ficos da fase
        unmet_criteria = []
        for criterion, target in phase.success_criteria.items():
            current = metrics.get(criterion, 0)
            if current < target:
                unmet_criteria.append(f"{criterion}: {current:.3f} < {target:.3f}")
        
        if unmet_criteria:
            issues.append(f"Crit√©rios n√£o atingidos: {', '.join(unmet_criteria)}")
        
        return issues
    
    def _log_phase_progress(self, phase: TrainingPhase, steps: int, metrics: Dict):
        """Log detalhado do progresso da fase com diagn√≥stico"""
        progress = steps / phase.timesteps * 100
        
        # Log b√°sico
        self.logger.info(f"\n--- PROGRESSO {phase.name} ---")
        self.logger.info(f"Steps: {steps:,}/{phase.timesteps:,} ({progress:.1f}%)")
        self.logger.info(f"Win Rate: {metrics['win_rate']:.2%}")
        self.logger.info(f"Sharpe: {metrics['sharpe_ratio']:.2f}")
        self.logger.info(f"Max DD: {metrics['max_drawdown']:.2%}")
        self.logger.info(f"Return: {metrics['total_return']:.2%}")
        self.logger.info(f"Trades/h: {metrics['trades_per_hour']:.1f}")
        
        # Diagn√≥stico de problemas
        issues = self._diagnose_training_issues(phase, metrics)
        if issues:
            self.logger.warning("\nüîç DIAGN√ìSTICO:")
            for issue in issues:
                self.logger.warning(f"  {issue}")
        else:
            self.logger.info("Sem problemas detectados")
        
        # Progresso visual (a cada 25%)
        if progress % 25 < (steps - 10000) / phase.timesteps * 100 % 25:
            self._display_training_summary()
    
    def train(self):
        """üî• TREINAMENTO COMPLETO COM RESUME AUTOM√ÅTICO E AVALIA√á√ÉO ON-DEMAND"""
        try:
            # Configura√ß√£o de checkpoints
            checkpoint_freq = 10000  # Salvar a cada 10k passos
            checkpoint_path = DIFF_MODEL_DIR
            os.makedirs(checkpoint_path, exist_ok=True)
            
            # üî• CARREGAR DATASET COMPLETO SEM SPLIT
            df_train = self._load_training_data()
            if df_train is None:
                raise ValueError("N√£o foi poss√≠vel carregar os dados de treinamento")
            
            # Criar ambiente de treinamento com dataset completo
            env = self._create_phase_environment(df_train, None)
            self._current_env = env  # üî• COMPATIBILIDADE: Manter refer√™ncia para salvar VecNormalize
            print("‚úÖ Ambiente criado com dataset completo - compatibilidade 100%")
            
            # üî• SISTEMA DE RESUME TRAINING INTELIGENTE
            checkpoint_path_found, resume_phase_idx, resume_steps = self._find_latest_checkpoint()
            
            # Criar ou carregar modelo com detec√ß√£o autom√°tica de fase
            if checkpoint_path_found and os.path.exists(checkpoint_path_found):
                print(f"\nüîÑ RESUME TRAINING ATIVADO!")
                try:
                    self.current_model = RecurrentPPO.load(checkpoint_path_found, env=env)
                    self.current_phase_idx = resume_phase_idx
                    self.total_steps_completed = resume_steps
                    
                    # üî• CORRE√á√ÉO CR√çTICA: Sincronizar num_timesteps do modelo com steps resumidos
                    self.current_model.num_timesteps = resume_steps
                    print(f"‚úÖ Modelo sincronizado: num_timesteps = {self.current_model.num_timesteps:,}")
                    
                    current_phase = self.phases[self.current_phase_idx]
                    remaining_steps = current_phase.timesteps - (resume_steps % current_phase.timesteps)
                    
                    print(f"‚úÖ Modelo carregado: {resume_steps:,} steps")
                    print(f"üéØ Continuando da fase: {current_phase.name}")
                    print(f"üìä Steps restantes na fase: {remaining_steps:,}")
                    
                except Exception as model_load_error:
                    print(f"‚ùå ERRO ao carregar modelo: {model_load_error}")
                    print(f"üîÑ Criando novo modelo...")
                    self.current_model = self._create_model(env)
                    self.current_phase_idx = 0
                    self.total_steps_completed = 0
                
                # üî• SISTEMA DE ESTADOS REMOVIDO: Evitar m√©tricas congeladas
                print("‚úÖ Sistema de estados do ambiente DESABILITADO - evitando m√©tricas congeladas")
                print("üîÑ Ambiente sempre inicia com estado limpo para m√©tricas din√¢micas")
                    
            else:
                print("\nüìù Iniciando treinamento do zero...")
                self.current_model = self._create_model(env)
                self.current_phase_idx = 0
                self.total_steps_completed = 0
                print("‚úÖ Novo modelo criado com sucesso")
                
            # üî• SISTEMA DE SALVAMENTO ROBUSTO - SUBSTITUIR CHECKPOINTCALLBACK PROBLEM√ÅTICO
            class RobustSaveCallback(BaseCallback):
                def __init__(self, save_freq=50000, save_path=DIFF_MODEL_DIR, name_prefix="DIFF", total_steps_offset=0, training_env=None):
                    super().__init__()
                    self.save_freq = save_freq
                    self.save_path = save_path
                    self.name_prefix = name_prefix
                    self.total_steps_offset = total_steps_offset
                    self.training_env = training_env  # üî• CORRE√á√ÉO: Passar environment via par√¢metro  # üî• NOVO: Offset para steps acumulados
                    os.makedirs(save_path, exist_ok=True)
                    
                def _on_step(self) -> bool:
                    # üî• CORRE√á√ÉO: Usar steps acumulados reais para decidir quando salvar
                    real_timesteps = self.num_timesteps + self.total_steps_offset
                    if real_timesteps % self.save_freq == 0:
                        try:
                            from datetime import datetime
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            
                            # üî• SALVAMENTO ROBUSTO EM LOCAIS ORGANIZADOS (SEM RAIZ DO PROJETO)
                            # 1. Framework directory
                            framework_dir = DIFF_ENVSTATE_DIR
                            os.makedirs(framework_dir, exist_ok=True)
                            framework_path = f"{framework_dir}/checkpoint_{real_timesteps}_steps_{timestamp}.zip"
                            
                            # 2. Original save path
                            model_path = f"{self.save_path}/{self.name_prefix}_{real_timesteps}_steps_{timestamp}.zip"
                            
                            print(f"\n>>> üíæ SALVANDO CHECKPOINT ROBUSTO - Step {real_timesteps:,} (Atual: {self.num_timesteps:,} + Offset: {self.total_steps_offset:,}) <<<")
                            
                            # üî• SISTEMA DE SALVAMENTO DE ESTADOS REMOVIDO: Evitar m√©tricas congeladas
                            print("‚úÖ Salvamento de estados do ambiente DESABILITADO - evitando m√©tricas congeladas")
                            
                            # Salvar no framework
                            print(f"üíæ Salvando em: {framework_path}")
                            self.model.save(framework_path)
                            
                            # Salvar no path original
                            print(f"üíæ Salvando em: {model_path}")
                            self.model.save(model_path)
                            print("‚úÖ model.save() executado em locais organizados (SEM raiz do projeto)")
                            
                            # üî• SALVAR VEC_NORMALIZE_FINAL.PKL AUTOMATICAMENTE
                            try:
                                # Salvar VecNormalize em m√∫ltiplos locais
                                vec_normalize_paths = [
                                    f"{framework_dir}/vec_normalize_final.pkl",
                                    f"{self.save_path}/vec_normalize_final.pkl",
                                    "vec_normalize_final.pkl"  # Raiz do projeto para compatibilidade
                                ]
                                
                                for vec_path in vec_normalize_paths:
                                    try:
                                        os.makedirs(os.path.dirname(vec_path), exist_ok=True) if os.path.dirname(vec_path) else None
                                        # Usar fun√ß√£o robusta para salvar VecNormalize pronto para produ√ß√£o
                                        save_production_ready_vecnormalize(self.training_env, vec_path)
                                        print(f"‚úÖ VecNormalize pronto para produ√ß√£o salvo: {vec_path}")
                                    except Exception as vec_error:
                                        print(f"‚ùå Erro ao salvar VecNormalize em {vec_path}: {vec_error}")
                                        
                            except Exception as vec_general_error:
                                print(f"‚ùå Erro geral ao salvar VecNormalize: {vec_general_error}")
                            
                            # üî• VERIFICA√á√ÉO P√ìS-SALVAMENTO COMPLETA (SEM RAIZ DO PROJETO)
                            for path_name, path in [("Framework", framework_path), ("Original", model_path)]:
                                if os.path.exists(path):
                                    size_bytes = os.path.getsize(path)
                                    size_mb = size_bytes / (1024*1024)
                                    print(f"‚úÖ {path_name}: {size_mb:.1f}MB")
                                    
                                    # üî• VERIFICA√á√ÉO DE TAMANHO CR√çTICA
                                    if size_mb < 5:
                                        print(f"üö® ERRO CR√çTICO: Modelo {path_name} muito pequeno ({size_mb:.1f}MB)!")
                                        print("üö® POSS√çVEIS CAUSAS:")
                                        print("   - Modelo n√£o foi treinado")
                                        print("   - Erro no salvamento")
                                        print("   - Pesos n√£o foram atualizados")
                                        
                                        # Tentar salvar novamente com nome diferente
                                        backup_path = f"{os.path.dirname(path)}/EMERGENCY_{self.name_prefix}_{real_timesteps}_{timestamp}.zip"
                                        print(f"üÜò Tentando salvamento de emerg√™ncia: {backup_path}")
                                        self.model.save(backup_path)
                                
                                    elif size_mb > 50:
                                        print(f"‚ö†Ô∏è AVISO: Modelo {path_name} muito grande ({size_mb:.1f}MB) - verificar se normal")
                                    else:
                                        print(f"üéØ TAMANHO NORMAL: Modelo {path_name} v√°lido!")
                                        
                                    # üî• TESTE DE CARREGAMENTO R√ÅPIDO (apenas para o framework path)
                                    if path_name == "Framework":
                                        try:
                                            print("üß™ Testando carregamento do checkpoint...")
                                            test_model = RecurrentPPO.load(path, env=None)
                                            if test_model is not None:
                                                print("‚úÖ Checkpoint pode ser carregado corretamente!")
                                                del test_model  # Liberar mem√≥ria
                                            else:
                                                print("‚ùå ERRO: Checkpoint n√£o pode ser carregado!")
                                        except Exception as load_error:
                                            print(f"‚ùå ERRO no teste de carregamento: {load_error}")
                                else:
                                    print(f"‚ùå ERRO CR√çTICO: Arquivo {path_name} n√£o foi criado!")
                                    
                            print(f">>> üíæ CHECKPOINT ROBUSTO COMPLETO <<<\n")
                            
                        except Exception as e:
                            print(f"‚ùå ERRO CR√çTICO ao salvar checkpoint: {e}")
                            import traceback
                            traceback.print_exc()
                            
                            # üÜò SALVAMENTO DE EMERG√äNCIA
                            try:
                                emergency_path = f"{DIFF_ENVSTATE_DIR}/EMERGENCY_SAVE_{real_timesteps}.zip"
                                print(f"üÜò Tentando salvamento de emerg√™ncia: {emergency_path}")
                                self.model.save(emergency_path)
                                print("üÜò Salvamento de emerg√™ncia conclu√≠do")
                            except Exception as emergency_error:
                                print(f"üÜò Falha no salvamento de emerg√™ncia: {emergency_error}")
                                
                    return True
                        
            # Configurar callbacks
            robust_callback = RobustSaveCallback(
                save_freq=50000,
                save_path=checkpoint_path,
                                        name_prefix="DIFF_phase1",
                total_steps_offset=self.total_steps_completed  # üî• PASSAR OFFSET CORRETO
                )
            
                                # üéØ ADICIONAR M√âTRICAS CALLBACK + AVALIA√á√ÉO ON-DEMAND
            metrics_callback = MetricsCallback(env=env, log_freq=2000, verbose=1)
            
            # üî• INICIAR SISTEMA DE AVALIA√á√ÉO ON-DEMAND
            print("\n‚ö° SISTEMA DE AVALIA√á√ÉO ON-DEMAND ATIVO!")
            print("üî• Para avaliar: crie arquivo 'eval.txt' na pasta do projeto")
            
            # üî• CORRE√á√ÉO: Verificar se on_demand_eval foi inicializada
            global on_demand_eval
            if on_demand_eval is not None:
                on_demand_eval.start_keyboard_monitoring()
                on_demand_eval.update_current_model(self.current_model, env)
            else:
                print("‚ö†Ô∏è Sistema de avalia√ß√£o on-demand n√£o inicializado - criando inst√¢ncia local")
                on_demand_eval = OnDemandEvaluationSystem()
                on_demand_eval.start_keyboard_monitoring()
                on_demand_eval.update_current_model(self.current_model, env)
            
            print(f"üî• Para avaliar: crie arquivo 'eval.txt' na pasta do projeto")
            
            print("üî• Sistema de avalia√ß√£o on-demand continua ativo - crie arquivo 'eval.txt' para avaliar")
            
            # üî• ADICIONAR BARRA DE PROGRESSO
            progress_callback = ProgressBarCallback(total_timesteps=200000, verbose=1)
            
            # üî• EXECUTAR TREINAMENTO EM 5 FASES COM STEPS DOBRADOS
            total_phases = len(self.phases)
            
            for phase_idx in range(self.current_phase_idx, total_phases):
                current_phase = self.phases[phase_idx]
                
                # Configurar callbacks para a fase atual
                phase_name = current_phase.name.replace('_', '').lower()
                robust_callback = RobustSaveCallback(
                    save_freq=50000,
                    save_path=checkpoint_path,
                                                name_prefix=f"DIFF_{phase_name}",
                    total_steps_offset=self.total_steps_completed  # üî• PASSAR OFFSET CORRETO
                )
                
                metrics_callback = MetricsCallback(env=env, log_freq=2000, verbose=1)
                progress_callback = ProgressBarCallback(total_timesteps=current_phase.timesteps, verbose=1)
                
                # Combinar callbacks
                from stable_baselines3.common.callbacks import CallbackList
                combined_callback = CallbackList([robust_callback, metrics_callback, progress_callback])
                
                # Calcular steps restantes se resumindo treinamento
                if phase_idx == self.current_phase_idx and self.total_steps_completed > 0:
                    completed_in_phase = self.total_steps_completed % current_phase.timesteps
                    remaining_steps = current_phase.timesteps - completed_in_phase
                    print(f"\nüîÑ RESUMINDO {current_phase.name}: {remaining_steps:,} steps restantes")
                else:
                    remaining_steps = current_phase.timesteps
                    print(f"\nüöÄ INICIANDO {current_phase.name}: {remaining_steps:,} steps")
                
                print(f"üìù Descri√ß√£o: {current_phase.description}")
                print(f"üíæ Salvamento autom√°tico a cada 50k steps em: {checkpoint_path}")
                print(f"üìä M√©tricas detalhadas a cada 2000 steps")
                print(f"üî• Para avalia√ß√£o on-demand: crie arquivo 'eval.txt' na pasta")
                
                # Executar treinamento da fase
                self.current_model.learn(
                    total_timesteps=remaining_steps,
                    callback=combined_callback
                )
                
                # Salvar modelo final da fase
                try:
                    from datetime import datetime
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    total_steps_after_phase = self.total_steps_completed + remaining_steps
                    final_phase_path = f"{checkpoint_path}/FINAL_{phase_name}_{total_steps_after_phase}_steps_{timestamp}.zip"
                    
                    print(f"\nüíæ SALVANDO MODELO FINAL {current_phase.name}: {final_phase_path}")
                    self.current_model.save(final_phase_path)
                    
                    if os.path.exists(final_phase_path):
                        size_mb = os.path.getsize(final_phase_path) / (1024*1024)
                        print(f"‚úÖ {current_phase.name} completa: {size_mb:.1f}MB")
                        print(f"üéØ Total de steps acumulados: {total_steps_after_phase:,}")
                    else:
                        print(f"‚ùå ERRO: Modelo final {current_phase.name} n√£o foi salvo!")
                        
                    # Atualizar contador de steps
                    self.total_steps_completed = total_steps_after_phase
                    
                except Exception as e:
                    print(f"‚ùå ERRO ao salvar modelo final {current_phase.name}: {e}")
                
                print(f"üéâ {current_phase.name} CONCLU√çDA!")
                print("="*80)

            # üéâ TREINAMENTO COMPLETO - TODAS AS FASES CONCLU√çDAS
            print("\n" + "="*80)
            print("üéâ TREINAMENTO COMPLETO - TODAS AS 5 FASES CONCLU√çDAS!")
            print(f"üéØ Total de steps executados: {self.total_steps_completed:,}")
            print(f"üìÅ Modelos salvos em: {checkpoint_path}")
            print(f"üî• Sistema de avalia√ß√£o on-demand permanece ativo")
            print("="*80)
            
            # Salvar modelo FINAL ABSOLUTO com informa√ß√µes completas
            try:
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                final_absolute_path = f"{checkpoint_path}/FINAL_ABSOLUTE_{self.total_steps_completed}_steps_{timestamp}.zip"
                
                print(f"\nüíæ SALVANDO MODELO FINAL ABSOLUTO: {final_absolute_path}")
                self.current_model.save(final_absolute_path)
                
                if os.path.exists(final_absolute_path):
                    size_mb = os.path.getsize(final_absolute_path) / (1024*1024)
                    print(f"‚úÖ MODELO FINAL ABSOLUTO: {size_mb:.1f}MB")
                    print(f"üìÅ Localiza√ß√£o: {final_absolute_path}")
                    print(f"üéØ Steps totais: {self.total_steps_completed:,}")
                    
                    # üî• SALVAR VEC_NORMALIZE_FINAL.PKL NO FINAL DO TREINAMENTO
                    try:
                        final_vec_paths = [
                            f"{checkpoint_path}/vec_normalize_final.pkl",
                            "vec_normalize_final.pkl",  # Raiz
                            "Modelo PPO Trader/vec_normalize_final.pkl"  # Para robot
                        ]
                        
                        for final_vec_path in final_vec_paths:
                            try:
                                os.makedirs(os.path.dirname(final_vec_path), exist_ok=True) if os.path.dirname(final_vec_path) else None
                                if hasattr(env, 'save_running_average'):
                                    env.save_running_average(final_vec_path)
                                    print(f"‚úÖ VecNormalize FINAL salvo: {final_vec_path}")
                                elif hasattr(env, 'save'):
                                    env.save(final_vec_path)
                                    print(f"‚úÖ VecNormalize FINAL salvo: {final_vec_path}")
                            except Exception as final_vec_error:
                                print(f"‚ùå Erro ao salvar VecNormalize FINAL em {final_vec_path}: {final_vec_error}")
                                
                    except Exception as final_vec_general:
                        print(f"‚ùå Erro geral ao salvar VecNormalize FINAL: {final_vec_general}")
                    
                    if size_mb > 10:
                        print(f"üéâ SUCESSO! Modelo com tamanho adequado!")
                    else:
                        print(f"‚ö†Ô∏è AVISO: Modelo pequeno demais - verificar treinamento!")
                else:
                    print(f"‚ùå ERRO CR√çTICO: Modelo final absoluto n√£o foi salvo!")
                    
            except Exception as e:
                print(f"‚ùå ERRO CR√çTICO ao salvar modelo final absoluto: {e}")
                import traceback
                traceback.print_exc()
            
            print("\n‚úÖ Treinamento conclu√≠do com sucesso!")
            print("üî• Sistema de avalia√ß√£o on-demand continua ativo - crie arquivo 'eval.txt' para avaliar")
                
        except Exception as e:
            print(f"\n‚ùå ERRO durante treinamento: {str(e)}")
            raise
    
    def _load_training_data(self):
        """üöÄ CARREGAR DATASET GOLD COMPLETO SEM SPLIT - ESTRUTURA MAINPPO1.PY"""
        try:
            # üöÄ CARREGAR DATASET GOLD COMBINADO OTIMIZADO
            df = load_optimized_data()
            
            if df is None or len(df) == 0:
                self.logger.error("‚ùå Dataset vazio ou inv√°lido")
                return None
            
            self.logger.info(f"üöÄ Dataset GOLD carregado: {len(df):,} registros")
            self.logger.info(f"üöÄ Per√≠odo: {df.index[0]} at√© {df.index[-1]}")
            
            # üéØ USAR DATASET COMPLETO - SEM SPLIT E SEM CORTE DOS 20% INICIAIS
            # Usar dataset completo sem qualquer limita√ß√£o
            df_final = df
            
            self.logger.info(f"‚úÖ DATASET COMPLETO SEM SPLIT: {len(df_final):,} barras")
            self.logger.info(f"üìÖ Per√≠odo completo: {df_final.index[0]} at√© {df_final.index[-1]}")
            self.logger.info(f"‚è∞ Dura√ß√£o total: {(df_final.index[-1] - df_final.index[0]).days} dias")
            
            # üéØ SEM SPLIT E SEM CORTE - DATASET NOSTATIC COMPLETO
            self.logger.info(f"üìä CONFIGURA√á√ÉO FINAL:")
            self.logger.info(f"   üî• Dataset: GOLD_final_nostatic.pkl (completo)")
            self.logger.info(f"   üî• Treinamento: {len(df_final):,} barras (100% do dataset)")
            self.logger.info(f"   üî• Avalia√ß√£o: mesmo dataset (sem split)")
            
            return df_final
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao carregar dados: {e}")
            return None
    
    def _create_phase_environment(self, df: pd.DataFrame, phase: TrainingPhase):
        """Criar ambiente √∫nico simples e r√°pido"""
        phase_name = phase.name if phase and hasattr(phase, 'name') else "principal"
        self.logger.info(f"üèóÔ∏è Criando ambiente √öNICO para fase: {phase_name}")
        
        # üî• CORRE√á√ÉO: Fun√ß√£o separada para evitar problemas de lambda closure
        def create_env():
            return Monitor(make_wrapped_env(df, BEST_PARAMS["window_size"], True))
        
        # üî• AMBIENTE √öNICO - M√ÅXIMA PERFORMANCE
        env = DummyVecEnv([create_env])
        
        # Aplicar VecNormalize se habilitado
        if USE_VECNORM:
            vec_normalize_file = f'{DIFF_MODEL_DIR}/vec_normalize.pkl'
            self.logger.info("üîÑ Aplicando VecNormalize ROBUSTO...")
            env = create_robust_vecnormalize(env, vec_normalize_file)
            self.logger.info("‚úÖ VecNormalize ROBUSTO ativado!")
            
            # üî• CONFIRMA√á√ÉO VECNORMALIZE
            self.logger.info("=" * 60)
            self.logger.info("üìä VECNORMALIZE INTELIGENTE ATIVADO:")
            self.logger.info("=" * 60)
            self.logger.info(f"‚úÖ Normaliza√ß√£o de Observa√ß√µes: {env.norm_obs}")
            self.logger.info(f"‚úÖ Normaliza√ß√£o de Rewards: {env.norm_reward}")
            self.logger.info(f"üìè Clip Observa√ß√µes: [-{env.clip_obs}, {env.clip_obs}]")
            self.logger.info(f"üéØ Clip Rewards: [-{env.clip_reward}, {env.clip_reward}]")
            self.logger.info(f"üîÑ Modo Treinamento: {env.training}")
            self.logger.info(f"üß† Sistema Inteligente: AUTO-CALIBRA√á√ÉO + MONITORAMENTO")
            self.logger.info("=" * 60)
        else:
            self.logger.info("‚ö†Ô∏è VecNormalize DESABILITADO")
            self.logger.info("   Observa√ß√µes e rewards n√£o ser√£o normalizados")
        
        self.logger.info(f"‚úÖ Ambiente √öNICO criado:")
        self.logger.info(f"   Dataset: {len(df):,} barras")
        self.logger.info(f"   Tipo: {type(env).__name__}")
        
        return env
    
    def _train_with_monitoring(self, phase: TrainingPhase, env) -> bool:
        """FUN√á√ÉO REMOVIDA - CAUSAVA ENCERRAMENTO PRECOCE"""
        self.logger.warning("‚ö†Ô∏è _train_with_monitoring foi removida - usar train() principal")
        return True
    
    def _evaluate_current_performance(self, env) -> Dict:
        """Avaliar performance atual do modelo com m√©tricas reais"""
        try:
            # Implementar avalia√ß√£o real
            obs = env.reset()
            total_reward = 0
            episode_returns = []
            trades_info = []
            steps = 0
            episodes = 0
            max_episodes = 3  # Avaliar em m√∫ltiplos epis√≥dios
            
            while episodes < max_episodes and steps < 50000:  # üî• REDUZIDO: 200k -> 50k para evitar travamento em avalia√ß√£o
                action, _ = self.current_model.predict(obs, deterministic=True)
                
                obs, reward, done, info = env.step(action)
                total_reward += reward[0] if isinstance(reward, (list, np.ndarray)) else reward
                steps += 1
                
                if done[0] if isinstance(done, (list, np.ndarray)) else done:
                    episodes += 1
                    if isinstance(info, list) and info:
                        info = info[0]
                    
                    # Extrair m√©tricas do epis√≥dio
                    final_balance = info.get('final_balance', 1000)
                    episode_return = (final_balance - 1000) / 1000
                    episode_returns.append(episode_return)
                    
                    # Extrair informa√ß√µes dos trades
                    if 'total_trades' in info and info['total_trades'] > 0:
                        trades_info.append({
                            'total_trades': info['total_trades'],
                            'win_rate': info.get('win_rate', 0),
                            'final_balance': final_balance,
                            'peak_portfolio': info.get('peak_portfolio', 500),
                            'drawdown': info.get('peak_drawdown_episode', 0)
                        })
                    
                    obs = env.reset()
            
            # Calcular m√©tricas consolidadas
            if episode_returns:
                avg_return = np.mean(episode_returns)
                return_std = np.std(episode_returns) if len(episode_returns) > 1 else 0.1
                sharpe_ratio = avg_return / max(return_std, 0.01) if return_std > 0 else avg_return / 0.01
                max_return = max(episode_returns)
                min_return = min(episode_returns)
                max_drawdown = abs(min_return) if min_return < 0 else 0
            else:
                avg_return = 0
                return_std = 0.1
                sharpe_ratio = 0
                max_drawdown = 0.1
                max_return = 0
            
            # M√©tricas de trading
            if trades_info:
                avg_win_rate = np.mean([t['win_rate'] for t in trades_info])
                avg_trades_per_episode = np.mean([t['total_trades'] for t in trades_info])
                avg_final_balance = np.mean([t['final_balance'] for t in trades_info])
                avg_drawdown = np.mean([t['drawdown'] for t in trades_info])
            else:
                avg_win_rate = 0.5
                avg_trades_per_episode = 0
                avg_final_balance = 1000
                avg_drawdown = 0.1
            
            # Calcular trades per hour (aproxima√ß√£o)
            trades_per_hour = avg_trades_per_episode / max(steps / max_episodes / 12, 1)  # 12 steps ‚âà 1 hora
            
            # M√©tricas espec√≠ficas de performance
            risk_adjusted_return = avg_return / max(avg_drawdown, 0.01)
            tail_risk_ratio = min(1.0, max(0.0, 1 - (max_drawdown / 0.2)))  # 20% como limite
            volatility_adjusted_return = avg_return / max(return_std, 0.01)
            trend_accuracy = min(1.0, max(0.0, avg_win_rate + 0.1))  # Aproxima√ß√£o
            
            metrics = {
                "win_rate": avg_win_rate,
                "sharpe_ratio": sharpe_ratio,
                "max_drawdown": max(avg_drawdown, max_drawdown),
                "total_return": avg_return,
                "trades_per_hour": trades_per_hour,
                "risk_adjusted_return": risk_adjusted_return,
                "tail_risk_ratio": tail_risk_ratio,
                "volatility_adjusted_return": volatility_adjusted_return,
                "trend_accuracy": trend_accuracy,
                "final_balance": avg_final_balance,
                "episodes_evaluated": episodes,
                "total_steps": steps
            }
            
            self.logger.info(f"Avalia√ß√£o: {episodes} epis√≥dios, {steps} steps")
            self.logger.info(f"Retorno m√©dio: {avg_return:.3f}, Sharpe: {sharpe_ratio:.2f}")

            
            return metrics
            
        except Exception as e:
            self.logger.error(f"Erro na avalia√ß√£o: {str(e)}")
            import traceback
            self.logger.error(f"Traceback: {traceback.format_exc()}")
            # Fallback para m√©tricas padr√£o em caso de erro - VALORES BAIXOS PARA FOR√áAR MELHORIA
            return {
                "win_rate": 0.20,  # REDUZIDO: for√ßar melhoria se houver erro
                "sharpe_ratio": -0.5,  # NEGATIVO: for√ßar melhoria
                "max_drawdown": 0.50,  # ALTO: for√ßar melhoria
                "total_return": -0.20,  # NEGATIVO: for√ßar melhoria
                "trades_per_hour": 0.1,  # BAIXO: for√ßar mais trading
                "risk_adjusted_return": -1.0,  # NEGATIVO: for√ßar melhoria
                "tail_risk_ratio": 0.3,  # BAIXO: for√ßar melhoria
                "volatility_adjusted_return": -1.0,  # NEGATIVO: for√ßar melhoria
                "trend_accuracy": 0.20  # BAIXO: for√ßar melhoria
            }
    
    def _should_early_stop(self, phase: TrainingPhase) -> bool:
        """üî• EARLY STOPPING DESABILITADO - Nunca parar antecipadamente"""
        # üö® COMPLETAMENTE DESABILITADO - Continuar sempre
        return False
        
        # C√≥digo original comentado para evitar early stopping
        # recent_metrics = self.metrics_tracker.get_phase_progress(phase.name)
        # if not recent_metrics:
        #     return False
        # latest = recent_metrics[-1]['metrics']
        # for criterion, target in phase.success_criteria.items():
        #     current = latest.get(criterion, 0)
        #     if current < target:
        #         return False
        # return True
    
    def _perform_adaptive_reset(self, phase: TrainingPhase, env):
        """Executar reset adaptativo do modelo"""
        self.logger.info("Executando reset adaptativo...")
        
        # Recriar modelo
        self.current_model = self._create_model(env)
        
        # Log do reset
        reset_info = {
            'timestamp': datetime.now().isoformat(),
            'phase': phase.name,
            'reason': 'Adaptive reset triggered'
        }
        
        reset_file = f"{self.base_dir}/metrics/resets.json"
        if os.path.exists(reset_file):
            with open(reset_file, 'r') as f:
                resets = json.load(f)
        else:
            resets = []
        
        resets.append(reset_info)
        with open(reset_file, 'w') as f:
            json.dump(resets, f, indent=2)
    
    def _check_phase_success(self, phase: TrainingPhase, metrics: Dict) -> bool:
        """Verificar se a fase foi bem-sucedida"""
        for criterion, target in phase.success_criteria.items():
            current = metrics.get(criterion, 0)
            if current < target:
                self.logger.warning(f"Crit√©rio n√£o atingido: {criterion} = {current:.3f} < {target:.3f}")
                return False
        
        return True
    
    def _save_phase_checkpoint(self, phase: TrainingPhase):
        """Salvar checkpoint espec√≠fico da fase"""
        checkpoint_dir = f"{self.base_dir}/phases/{phase.name}"
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # Salvar modelo
        model_path = f"{checkpoint_dir}/model.zip"
        self.current_model.save(model_path)
        
        # Salvar m√©tricas da fase
        phase_metrics = self.metrics_tracker.get_phase_progress(phase.name)
        metrics_path = f"{checkpoint_dir}/metrics.json"
        
        with open(metrics_path, 'w') as f:
            json.dump({
                'phase_info': {
                    'name': phase.name,
                    'description': phase.description,
                    'timesteps': phase.timesteps
                },
                'metrics_history': [
                    {
                        'timestamp': m['timestamp'].isoformat(),
                        'metrics': m['metrics']
                    } for m in phase_metrics
                ]
            }, f, indent=2)
        
        self.logger.info(f"Checkpoint salvo: {checkpoint_dir}")
    
    def _cross_validate_phase(self, phase: TrainingPhase):
        """Cross-validation temporal da fase"""
        self.logger.info(f"\n=== CROSS-VALIDATION: {phase.name} ===")
        
        cv_results = []
        for i, split in enumerate(self.cross_validator.splits):
            self.logger.info(f"CV Split {i+1}/{len(self.cross_validator.splits)}")
            self.logger.info(f"Train: {split['train_period']}")
            self.logger.info(f"Val: {split['val_period']}")
            
            # Carregar dados do split
            train_data, val_data = self.cross_validator.get_split_data(i)
            
            # Filtrar dados para a fase
            train_filtered = self._filter_data_for_phase(train_data, phase)
            val_filtered = self._filter_data_for_phase(val_data, phase)
            
            if len(train_filtered) < 1000 or len(val_filtered) < 100:
                self.logger.warning(f"Split {i+1} - dados insuficientes ap√≥s filtro")
                continue
            
            # Treinar modelo tempor√°rio no split
            temp_env = self._create_phase_environment(train_filtered, phase)
            temp_model = self._create_model(temp_env)
            temp_model.learn(total_timesteps=min(50000, phase.timesteps // 4))
            
            # Validar no per√≠odo de valida√ß√£o
            val_env = self._create_phase_environment(val_filtered, phase)
            val_metrics = self._evaluate_model_on_env(temp_model, val_env)
            
            cv_results.append({
                'split': i+1,
                'train_period': split['train_period'],
                'val_period': split['val_period'],
                'metrics': val_metrics
            })
            
            self.logger.info(f"Split {i+1} - Val Sharpe: {val_metrics['sharpe_ratio']:.2f}")
        
        # Salvar resultados de CV
        cv_path = f"{self.base_dir}/cross_validation/{phase.name}_cv_results.json"
        with open(cv_path, 'w') as f:
            json.dump(cv_results, f, indent=2)
        
        # Log summary
        if cv_results:
            avg_sharpe = np.mean([r['metrics']['sharpe_ratio'] for r in cv_results])
            self.logger.info(f"CV M√©dio - Sharpe: {avg_sharpe:.2f}")
    
    def _evaluate_model_on_env(self, model, env) -> Dict:
        """Avaliar modelo em ambiente espec√≠fico"""
        # Implementa√ß√£o simplificada - avaliar por alguns steps
        obs = env.reset()
        total_reward = 0
        steps = 0
        
        for _ in range(1000):  # Avaliar por 1000 steps
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            total_reward += reward[0]
            steps += 1
            
            if done[0]:
                obs = env.reset()
        
        # M√©tricas simuladas baseadas na avalia√ß√£o
        return {
            "win_rate": np.random.uniform(0.4, 0.7),
            "sharpe_ratio": total_reward / max(steps, 1) * 100,  # Aproxima√ß√£o
            "max_drawdown": np.random.uniform(0.05, 0.15),
            "total_return": total_reward / 1000,
            "trades_per_hour": np.random.uniform(1.0, 5.0)
        }
    
    def _comprehensive_evaluation(self, df: pd.DataFrame, is_training: bool, eval_name: str) -> Dict:
        """Avalia√ß√£o abrangente em um conjunto de dados"""
        self.logger.info(f"   Executando avalia√ß√£o {eval_name}...")
        
        try:
            # Criar ambiente espec√≠fico para avalia√ß√£o
            eval_env = self._create_phase_environment(df, self.phases[-1])
            eval_env.envs[0].df = df.copy()  # üî• DATASET COMPLETO SEM SPLIT
            
            # Configurar para avalia√ß√£o longa
            obs = eval_env.reset()
            lstm_states = None
            episode_starts = torch.ones(1, dtype=torch.bool, device=DEVICE)
            
            # M√©tricas de tracking
            total_reward = 0
            episode_rewards = []
            episode_lengths = []
            all_portfolio_values = []
            all_drawdowns = []
            all_trades = []
            episodes_completed = 0
            steps_total = 0
            
            # Executar avalia√ß√£o por 20.000 steps ou 10 epis√≥dios completos
            MAX_STEPS = 1500
            max_episodes = 10
            current_episode_reward = 0
            current_episode_steps = 0
            
            self.logger.info(f"   Iniciando {eval_name} - Meta: {max_steps} steps ou {max_episodes} epis√≥dios")
            
            for step in range(max_steps):
                with torch.no_grad():
                    action, lstm_states = self.current_model.predict(
                        obs, state=lstm_states, episode_start=episode_starts, deterministic=True
                    )
                
                obs, rewards, dones, infos = eval_env.step(action)
                episode_starts = torch.tensor(dones, dtype=torch.bool).to(DEVICE)  # üî• CORRIGIR DEVICE
                
                current_episode_reward += rewards[0]
                current_episode_steps += 1
                total_reward += rewards[0]
                steps_total += 1
                
                # Coletar m√©tricas do ambiente
                env_unwrapped = eval_env.envs[0]
                all_portfolio_values.append(env_unwrapped.portfolio_value)
                all_drawdowns.append(env_unwrapped.current_drawdown)
                
                # Se epis√≥dio terminou
                if dones[0]:
                    episodes_completed += 1
                    episode_rewards.append(current_episode_reward)
                    episode_lengths.append(current_episode_steps)
                    
                    # Coletar trades do epis√≥dio
                    if hasattr(env_unwrapped, 'trades'):
                        all_trades.extend(env_unwrapped.trades)
                    
                    # Reset para pr√≥ximo epis√≥dio
                    obs = eval_env.reset()
                    current_episode_reward = 0
                    current_episode_steps = 0
                    
                    # Parar se atingiu n√∫mero m√°ximo de epis√≥dios
                    if episodes_completed >= max_episodes:
                        self.logger.info(f"   ‚úÖ {eval_name}: {episodes_completed} epis√≥dios completados")
                        break
                
                # Log de progresso a cada 5000 steps
                if step % 5000 == 0 and step > 0:
                    self.logger.info(f"   üìä {eval_name}: {step}/{max_steps} steps, {episodes_completed} epis√≥dios, Portfolio: ${all_portfolio_values[-1]:.2f}")
            
            # Calcular m√©tricas finais detalhadas
            metrics = self._calculate_detailed_metrics(
                episode_rewards, all_portfolio_values, all_drawdowns, 
                all_trades, steps_total, eval_name
            )
            
            # Salvar m√©tricas detalhadas
            eval_file = f"{self.base_dir}/metrics/evaluation_{eval_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(eval_file, 'w') as f:
                # üî• CORRIGIR JSON SERIALIZATION: Converter todos os tipos numpy
                def convert_numpy_types(obj):
                    if isinstance(obj, (np.integer, np.int32, np.int64)):
                        return int(obj)
                    elif isinstance(obj, (np.floating, np.float32, np.float64)):
                        return float(obj)
                    elif isinstance(obj, np.ndarray):
                        return obj.tolist()
                    elif isinstance(obj, dict):
                        return {k: convert_numpy_types(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [convert_numpy_types(v) for v in obj]
                    else:
                        return obj
                
                json_metrics = convert_numpy_types(metrics)
                json.dump(json_metrics, f, indent=2)
            
            self.logger.info(f"   ‚úÖ {eval_name} conclu√≠da: {episodes_completed} epis√≥dios, Sharpe: {metrics.get('sharpe_ratio', 0):.2f}")
            
            eval_env.close()
            return metrics
            
        except Exception as e:
            self.logger.error(f"   ‚ùå Erro na avalia√ß√£o {eval_name}: {str(e)}")
            return {"error": str(e), "sharpe_ratio": 0, "total_return": 0}
    
    def _stress_test_evaluation(self, df: pd.DataFrame) -> Dict:
        """Teste de estresse em condi√ß√µes adversas"""
        self.logger.info("   Executando teste de estresse...")
        
        try:
            stress_results = {}
            
            # Teste 1: Per√≠odo de alta volatilidade (√∫ltimos 20% dos dados)
            volatile_data = df.iloc[-int(len(df) * 0.2):].copy()
            stress_results['high_volatility'] = self._comprehensive_evaluation(volatile_data, False, "stress_volatility")
            
            # Teste 2: Per√≠odo de baixa atividade (dados com pouca varia√ß√£o)
            # Simular reduzindo a volatilidade dos dados
            low_activity_data = df.copy()
            for col in ['close_5m', 'close_15m', 'close_4h']:
                if col in low_activity_data.columns:
                    low_activity_data[col] = low_activity_data[col].rolling(10).mean().fillna(method='bfill')
            stress_results['low_activity'] = self._comprehensive_evaluation(low_activity_data.iloc[-5000:], False, "stress_low_activity")
            
            # Teste 3: Condi√ß√µes extremas (dados invertidos para simular crash)
            extreme_data = df.iloc[-3000:].copy()
            for col in ['close_5m', 'close_15m', 'close_4h']:
                if col in extreme_data.columns:
                    # Inverter tend√™ncia para simular crash
                    extreme_data[col] = extreme_data[col].iloc[0] - (extreme_data[col] - extreme_data[col].iloc[0])
            stress_results['extreme_conditions'] = self._comprehensive_evaluation(extreme_data, False, "stress_extreme")
            
            # M√©tricas consolidadas de estresse
            stress_metrics = {
                'individual_tests': stress_results,
                'stress_score': np.mean([r.get('sharpe_ratio', 0) for r in stress_results.values()]),
                'worst_case_drawdown': max([r.get('max_drawdown', 0) for r in stress_results.values()]),
                'stress_resilience': min([r.get('total_return', 0) for r in stress_results.values()])
            }
            
            self.logger.info(f"   ‚úÖ Teste de estresse conclu√≠do - Score: {stress_metrics['stress_score']:.2f}")
            return stress_metrics
            
        except Exception as e:
            self.logger.error(f"   ‚ùå Erro no teste de estresse: {str(e)}")
            return {"error": str(e), "stress_score": 0}
    
    def _consistency_evaluation(self, df: pd.DataFrame) -> Dict:
        """Teste de consist√™ncia com m√∫ltiplas execu√ß√µes"""
        self.logger.info("   Executando teste de consist√™ncia...")
        
        try:
            consistency_results = []
            val_data = df.iloc[int(len(df) * 0.8):].copy()
            
            # Executar 5 avalia√ß√µes independentes
            for run in range(5):
                self.logger.info(f"   üîÑ Execu√ß√£o de consist√™ncia {run + 1}/5")
                
                # Resetar seeds para variabilidade
                np.random.seed(SEED + run)
                torch.manual_seed(SEED + run)
                
                run_result = self._comprehensive_evaluation(val_data, False, f"consistency_run_{run+1}")
                consistency_results.append(run_result)
            
            # Calcular estat√≠sticas de consist√™ncia
            sharpe_values = [r.get('sharpe_ratio', 0) for r in consistency_results]
            return_values = [r.get('total_return', 0) for r in consistency_results]
            drawdown_values = [r.get('max_drawdown', 0) for r in consistency_results]
            
            consistency_metrics = {
                'runs': consistency_results,
                'sharpe_mean': np.mean(sharpe_values),
                'sharpe_std': np.std(sharpe_values),
                'sharpe_cv': np.std(sharpe_values) / max(np.mean(sharpe_values), 1e-6),  # Coefficient of variation
                'return_mean': np.mean(return_values),
                'return_std': np.std(return_values),
                'drawdown_mean': np.mean(drawdown_values),
                'drawdown_std': np.std(drawdown_values),
                'consistency_score': 1.0 / max(np.std(sharpe_values), 0.01)  # Menor variabilidade = maior consist√™ncia
            }
            
            self.logger.info(f"   ‚úÖ Teste de consist√™ncia conclu√≠do - Sharpe m√©dio: {consistency_metrics['sharpe_mean']:.2f} ¬± {consistency_metrics['sharpe_std']:.2f}")
            return consistency_metrics
            
        except Exception as e:
            self.logger.error(f"   ‚ùå Erro no teste de consist√™ncia: {str(e)}")
            return {"error": str(e), "consistency_score": 0}
    
    def _temporal_backtest(self, df: pd.DataFrame) -> Dict:
        """Backtest temporal com an√°lise por per√≠odos"""
        self.logger.info("   Executando backtest temporal...")
        
        try:
            # Dividir dados em per√≠odos temporais
            total_len = len(df)
            period_size = total_len // 4  # 4 per√≠odos
            
            period_results = {}
            
            for i in range(4):
                start_idx = i * period_size
                end_idx = min((i + 1) * period_size, total_len)
                period_data = df.iloc[start_idx:end_idx].copy()
                
                period_name = f"period_{i+1}"
                self.logger.info(f"   üìà Avaliando per√≠odo {i+1}/4 ({len(period_data)} samples)")
                
                period_results[period_name] = self._comprehensive_evaluation(
                    period_data, False, f"temporal_{period_name}"
                )
            
            # An√°lise temporal
            sharpe_trend = [period_results[f"period_{i+1}"].get('sharpe_ratio', 0) for i in range(4)]
            return_trend = [period_results[f"period_{i+1}"].get('total_return', 0) for i in range(4)]
            
            temporal_metrics = {
                'period_results': period_results,
                'sharpe_trend': sharpe_trend,
                'return_trend': return_trend,
                'performance_stability': 1.0 - np.std(sharpe_trend) / max(np.mean(sharpe_trend), 1e-6),
                'trend_direction': 'improving' if sharpe_trend[-1] > sharpe_trend[0] else 'declining',
                'best_period': max(range(4), key=lambda i: sharpe_trend[i]) + 1,
                'worst_period': min(range(4), key=lambda i: sharpe_trend[i]) + 1
            }
            
            self.logger.info(f"   ‚úÖ Backtest temporal conclu√≠do - Tend√™ncia: {temporal_metrics['trend_direction']}")
            return temporal_metrics
            
        except Exception as e:
            self.logger.error(f"   ‚ùå Erro no backtest temporal: {str(e)}")
            return {"error": str(e), "performance_stability": 0}
    
    def _calculate_detailed_metrics(self, episode_rewards, portfolio_values, drawdowns, trades, total_steps, eval_name):
        """Calcular m√©tricas detalhadas de uma avalia√ß√£o"""
        try:
            # M√©tricas b√°sicas
            total_return = portfolio_values[-1] - 500 if portfolio_values else 0
            max_drawdown = max(drawdowns) if drawdowns else 0
            avg_portfolio = np.mean(portfolio_values) if portfolio_values else 500
            
            # M√©tricas de trading
            profitable_trades = len([t for t in trades if t.get('pnl_usd', 0) > 0])
            total_trades = len(trades)
            win_rate = profitable_trades / max(total_trades, 1)
            
            total_pnl = sum(t.get('pnl_usd', 0) for t in trades)
            avg_trade_pnl = total_pnl / max(total_trades, 1)
            
            # Sharpe ratio aproximado
            returns_series = np.diff(portfolio_values) if len(portfolio_values) > 1 else [0]
            if len(returns_series) > 1 and np.std(returns_series) > 0:
                sharpe_ratio = np.mean(returns_series) / np.std(returns_series) * np.sqrt(252 * 288)  # Annualized
            else:
                sharpe_ratio = 0
            
            # M√©tricas de risco
            downside_returns = [r for r in returns_series if r < 0]
            if len(downside_returns) > 1:
                sortino_ratio = np.mean(returns_series) / np.std(downside_returns) * np.sqrt(252 * 288)
            else:
                sortino_ratio = sharpe_ratio
            
            return {
                'eval_name': eval_name,
                'total_return': float(total_return),
                'total_return_pct': float(total_return / 1000 * 100),
                'max_drawdown': float(max_drawdown),
                'avg_portfolio': float(avg_portfolio),
                'sharpe_ratio': float(sharpe_ratio),
                'sortino_ratio': float(sortino_ratio),
                'calmar_ratio': float(total_return / max(max_drawdown, 0.01)),
                'win_rate': float(win_rate),
                'total_trades': int(total_trades),
                'profitable_trades': int(profitable_trades),
                'avg_trade_pnl': float(avg_trade_pnl),
                'total_pnl': float(total_pnl),
                'trades_per_day': float(total_trades / max(total_steps / 288, 1)),  # 288 steps = 1 day
                'total_steps': int(total_steps),
                'final_portfolio': float(portfolio_values[-1]) if portfolio_values else 500.0
            }
            
        except Exception as e:
            self.logger.error(f"Erro ao calcular m√©tricas detalhadas: {str(e)}")
            return {'error': str(e), 'sharpe_ratio': 0, 'total_return': 0}
    
    def _calculate_overall_score(self, train_metrics, val_metrics, stress_metrics, consistency_metrics):
        """Calcular score geral baseado em todas as m√©tricas"""
        try:
            # Pesos para diferentes aspectos
            weights = {
                'performance': 0.3,      # Performance em valida√ß√£o
                'consistency': 0.25,     # Consist√™ncia entre execu√ß√µes
                'stress_resilience': 0.2, # Resist√™ncia a estresse
                'overfitting': 0.25      # Penalidade por overfitting
            }
            
            # Score de performance (valida√ß√£o)
            performance_score = max(0, min(100, val_metrics.get('sharpe_ratio', 0) * 10))
            
            # Score de consist√™ncia
            consistency_score = max(0, min(100, consistency_metrics.get('consistency_score', 0) * 10))
            
            # Score de resist√™ncia ao estresse
            stress_score = max(0, min(100, stress_metrics.get('stress_score', 0) * 10 + 50))
            
            # Penalidade por overfitting (diferen√ßa entre train e validation)
            train_sharpe = train_metrics.get('sharpe_ratio', 0)
            val_sharpe = val_metrics.get('sharpe_ratio', 0)
            if train_sharpe > 0:
                overfit_penalty = abs(train_sharpe - val_sharpe) / train_sharpe * 100
            else:
                overfit_penalty = 50
            overfitting_score = max(0, 100 - overfit_penalty)
            
            # Score final ponderado
            overall_score = (
                performance_score * weights['performance'] +
                consistency_score * weights['consistency'] +
                stress_score * weights['stress_resilience'] +
                overfitting_score * weights['overfitting']
            )
            
            return {
                'overall_score': float(overall_score),
                'performance_score': float(performance_score),
                'consistency_score': float(consistency_score),
                'stress_score': float(stress_score),
                'overfitting_score': float(overfitting_score),
                'weights_used': weights,
                'interpretation': self._interpret_score(overall_score)
            }
            
        except Exception as e:
            self.logger.error(f"Erro ao calcular score geral: {str(e)}")
            return {'overall_score': 0, 'error': str(e)}
    
    def _interpret_score(self, score):
        """Interpretar o score geral"""
        if score >= 80:
            return "Excelente - Modelo pronto para produ√ß√£o"
        elif score >= 65:
            return "Bom - Modelo aceit√°vel com monitoramento"
        elif score >= 50:
            return "Regular - Requer melhorias antes da produ√ß√£o"
        elif score >= 35:
            return "Fraco - Necessita retreinamento significativo"
        else:
            return "Cr√≠tico - Modelo n√£o recomendado para uso"
    
    def _final_integration(self):
        """Treinamento final integrado com todas as habilidades"""
        self.logger.info("\n=== FASE FINAL: INTEGRA√á√ÉO COMPLETA ===")
        
        try:
            # Carregar dados completos
            df_full = pd.read_csv(get_latest_processed_file('5m'))
            env_full = self._create_phase_environment(df_full, self.phases[-1])
            
            # Verificar se temos um modelo
            if self.current_model is None:
                self.logger.info("Criando modelo para integra√ß√£o final...")
                self.current_model = self._create_model(env_full)
            else:
                # Treinamento final
                self.current_model.set_env(env_full)
            
            if self.current_model is None:
                self.logger.error("N√£o foi poss√≠vel criar modelo para integra√ß√£o final!")
                return
            
            # Treinamento final adicional (opcional - j√° est√° bem treinado)
            self.logger.info("Modelo j√° est√° bem treinado nas fases. Pulando treinamento adicional.")
            # self.current_model.learn(total_timesteps=100000)  # Comentado para evitar problemas de batch
            
            # === CICLOS DE AVALIA√á√ÉO FINAL ABRANGENTES ===
            self.logger.info("\n=== INICIANDO CICLOS DE AVALIA√á√ÉO FINAL ===")
            
            # 1. Avalia√ß√£o em dados de treino (in-sample)
            self.logger.info("üîç Ciclo 1: Avalia√ß√£o em dados de treino (in-sample)")
            train_metrics = self._comprehensive_evaluation(df_full, is_training=True, eval_name="train")
            
            # 2. Avalia√ß√£o em dados de valida√ß√£o (out-of-sample)  
            self.logger.info("üîç Ciclo 2: Avalia√ß√£o em dados de valida√ß√£o (out-of-sample)")
            val_metrics = self._comprehensive_evaluation(df_full, is_training=False, eval_name="validation")
            
            # 3. Avalia√ß√£o estressante (diferentes condi√ß√µes de mercado)
            self.logger.info("üîç Ciclo 3: Avalia√ß√£o de estresse (condi√ß√µes adversas)")
            stress_metrics = self._stress_test_evaluation(df_full)
            
            # 4. Avalia√ß√£o de consist√™ncia (m√∫ltiplas execu√ß√µes)
            self.logger.info("üîç Ciclo 4: Teste de consist√™ncia (m√∫ltiplas execu√ß√µes)")
            consistency_metrics = self._consistency_evaluation(df_full)
            
            # 5. Backtest completo com an√°lise temporal
            self.logger.info("üîç Ciclo 5: Backtest temporal completo")
            backtest_metrics = self._temporal_backtest(df_full)
            
            # Consolidar m√©tricas finais
            final_metrics = {
                'train_performance': train_metrics,
                'validation_performance': val_metrics,
                'stress_test': stress_metrics,
                'consistency_test': consistency_metrics,
                'temporal_backtest': backtest_metrics,
                'overall_score': self._calculate_overall_score(train_metrics, val_metrics, stress_metrics, consistency_metrics)
            }
            
            # Salvar modelo final em Otimizacao/treino_principal/models conforme solicitado
            final_model_path = f"{self.base_dir}/models/advanced_trained_model_final.zip"
            os.makedirs(f"{self.base_dir}/models", exist_ok=True)
            self.current_model.save(final_model_path)
            
            # üî• SALVAR VECNORMALIZE FINAL PARA LEGION V1
            if USE_VECNORM and hasattr(self, '_current_env') and hasattr(self._current_env, 'save'):
                try:
                    # Salvar VecNormalize no diret√≥rio organizado
                    vec_normalize_path = f'{DIFF_MODEL_DIR}/vec_normalize.pkl'
                    self._current_env.save(vec_normalize_path)
                    self.logger.info(f"‚úÖ VecNormalize compartilhado salvo: {vec_normalize_path}")
                    
                    # üöÄ SALVAR VECNORMALIZE_FINAL.PKL PARA LEGION V1
                    final_vec_path = f'{DIFF_MODEL_DIR}/vecnormalize_final.pkl'
                    self._current_env.save(final_vec_path)
                    self.logger.info(f"‚úÖ VecNormalize FINAL salvo: {final_vec_path}")
                    
                    # Criar pasta Legion V1 e salvar l√° tamb√©m
                    legion_dir = "Modelo PPO Trader/Legion V1"
                    os.makedirs(legion_dir, exist_ok=True)
                    legion_vec_path = f"{legion_dir}/vecnormalize_final.pkl"
                    self._current_env.save(legion_vec_path)
                    self.logger.info(f"‚úÖ VecNormalize Legion V1 salvo: {legion_vec_path}")
                    
                except Exception as vec_error:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao salvar VecNormalize: {vec_error}")
            
            # Relat√≥rio final expandido
            self._generate_final_report(final_metrics)
            
            self.logger.info(f"‚úÖ Modelo final salvo: {final_model_path}")
            self.logger.info("‚úÖ CICLOS DE AVALIA√á√ÉO FINAL CONCLU√çDOS COM SUCESSO!")
            
        except Exception as e:
            self.logger.error(f"Erro na integra√ß√£o final: {str(e)}")
            import traceback
            self.logger.error(f"Traceback: {traceback.format_exc()}")
    
    def _generate_final_report(self, final_metrics: Dict):
        """Gerar relat√≥rio final completo"""
        training_duration = datetime.now() - self.training_start_time
        
        report = {
            'training_summary': {
                'start_time': self.training_start_time.isoformat(),
                'end_time': datetime.now().isoformat(),
                'duration_hours': training_duration.total_seconds() / 3600,
                'total_phases': len(self.phases),
                'total_timesteps': sum(p.timesteps for p in self.phases)
            },
            'phase_results': [],
            'final_metrics': final_metrics,
            'reset_history': self.adaptive_reset.reset_history,
            'best_performance': self._get_best_performance_across_phases()
        }
        
        # Adicionar resultados de cada fase
        for phase in self.phases:
            phase_progress = self.metrics_tracker.get_phase_progress(phase.name)
            if phase_progress:
                best_metrics = max(phase_progress, key=lambda x: x['metrics'].get('sharpe_ratio', 0))
                report['phase_results'].append({
                    'phase_name': phase.name,
                    'description': phase.description,
                    'timesteps': phase.timesteps,
                    'best_metrics': best_metrics['metrics'],
                    'total_evaluations': len(phase_progress)
                })
        
        # üî• CORRIGIR JSON SERIALIZATION: Converter todos os tipos numpy
        def convert_numpy_types(obj):
            if isinstance(obj, (np.integer, np.int32, np.int64)):
                return int(obj)
            elif isinstance(obj, (np.floating, np.float32, np.float64)):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_numpy_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(v) for v in obj]
            else:
                return obj
        
        # Salvar relat√≥rio
        report_path = f"{self.base_dir}/metrics/final_training_report.json"
        with open(report_path, 'w') as f:
            json_report = convert_numpy_types(report)
            json.dump(json_report, f, indent=2)
        
        # Log do relat√≥rio
        self.logger.info("\n=== RELAT√ìRIO FINAL DETALHADO ===")
        self.logger.info(f"Dura√ß√£o total: {training_duration}")
        self.logger.info(f"Timesteps totais: {sum(p.timesteps for p in self.phases):,}")
        
        # Log das m√©tricas de avalia√ß√£o final
        if 'train_performance' in final_metrics:
            train_metrics = final_metrics['train_performance']
            self.logger.info(f"üìä Performance Treinamento - Sharpe: {train_metrics.get('sharpe_ratio', 0):.2f}")
            self.logger.info(f"   Retorno Total: {train_metrics.get('total_return_pct', 0):.2f}%")
            self.logger.info(f"   Max Drawdown: {train_metrics.get('max_drawdown', 0):.2f}%")
            self.logger.info(f"   Win Rate: {train_metrics.get('win_rate', 0):.2f}%")
            self.logger.info(f"   Total Trades: {train_metrics.get('total_trades', 0)}")
            
        if 'validation_performance' in final_metrics:
            val_metrics = final_metrics['validation_performance']
            self.logger.info(f"üìä Performance Valida√ß√£o - Dataset: {val_metrics.get('dataset_size', 0):,} barras")
            self.logger.info(f"   Epis√≥dios: {val_metrics.get('episodes', 0)}")
            self.logger.info(f"   Steps: {val_metrics.get('steps', 0):,}")
            # üî• CORRIGIR: Converter numpy para float antes de formatar
            avg_reward = val_metrics.get('avg_reward', 0)
            if isinstance(avg_reward, (np.ndarray, np.number)):
                avg_reward = float(avg_reward)
            
            reward_std = val_metrics.get('reward_std', 0)
            if isinstance(reward_std, (np.ndarray, np.number)):
                reward_std = float(reward_std)
                
            sharpe_ratio = val_metrics.get('sharpe_ratio', 0)
            if isinstance(sharpe_ratio, (np.ndarray, np.number)):
                sharpe_ratio = float(sharpe_ratio)
            
            self.logger.info(f"   Avg Reward: {avg_reward:.3f}")
            self.logger.info(f"   Reward Std: {reward_std:.3f}")
            self.logger.info(f"   Sharpe Ratio: {sharpe_ratio:.2f}")
            # üî• CORRIGIR: Converter para float antes de formatar
            total_rewards = val_metrics.get('total_rewards', 0)
            if isinstance(total_rewards, (np.ndarray, np.number)):
                total_rewards = float(total_rewards)
            self.logger.info(f"   Total Rewards: {total_rewards:.2f}")
            
            # M√©tricas tradicionais se dispon√≠veis
            if 'total_return_pct' in val_metrics:
                self.logger.info(f"   Retorno Total: {val_metrics.get('total_return_pct', 0):.2f}%")
                self.logger.info(f"   Max Drawdown: {val_metrics.get('max_drawdown', 0):.2f}%")
                self.logger.info(f"   Win Rate: {val_metrics.get('win_rate', 0):.2f}%")
                self.logger.info(f"   Total Trades: {val_metrics.get('total_trades', 0)}")
        
        if 'overall_score' in final_metrics:
            overall = final_metrics['overall_score']
            self.logger.info(f"üéØ Score Geral: {overall.get('overall_score', 0):.1f}/100")
            self.logger.info(f"   Interpreta√ß√£o: {overall.get('interpretation', 'N/A')}")
            self.logger.info(f"   Performance Score: {overall.get('performance_score', 0):.1f}")
            self.logger.info(f"   Consistency Score: {overall.get('consistency_score', 0):.1f}")
            self.logger.info(f"   Stress Score: {overall.get('stress_score', 0):.1f}")
        
        if 'stress_test' in final_metrics:
            stress = final_metrics['stress_test']
            self.logger.info(f"üî• Teste de Estresse - Score: {stress.get('stress_score', 0):.2f}")
            self.logger.info(f"   Pior Caso Drawdown: {stress.get('worst_case_drawdown', 0):.2f}%")
        
        if 'consistency_test' in final_metrics:
            consistency = final_metrics['consistency_test']
            self.logger.info(f"üîÑ Consist√™ncia - Sharpe: {consistency.get('sharpe_mean', 0):.2f} ¬± {consistency.get('sharpe_std', 0):.2f}")
        
        if 'temporal_backtest' in final_metrics:
            temporal = final_metrics['temporal_backtest']
            self.logger.info(f"üìà Backtest Temporal - Tend√™ncia: {temporal.get('trend_direction', 'N/A')}")
            self.logger.info(f"   Estabilidade Performance: {temporal.get('performance_stability', 0):.2f}")
        
        self.logger.info(f"üíæ Relat√≥rio salvo: {report_path}")
        self.logger.info("="*70)
    
    def _get_best_performance_across_phases(self) -> Dict:
        """Obter melhor performance atrav√©s de todas as fases"""
        all_metrics = []
        for entry in self.metrics_tracker.metrics_history:
            all_metrics.append(entry['metrics'])
        
        if not all_metrics:
            return {}
        
        best = max(all_metrics, key=lambda x: x.get('sharpe_ratio', 0))
        return best
    
    def _generate_final_model_report(self):
        """üî• RELAT√ìRIO FINAL - SEMPRE MOSTRAR ONDE OS MODELOS FORAM SALVOS"""
        
        print("\n" + "="*80)
        print("üöÄ RELAT√ìRIO FINAL DE MODELOS SALVOS")
        print("="*80)
        
        # Lista de poss√≠veis locais onde modelos podem ter sido salvos
        possible_model_paths = [
            f"{self.base_dir}/modelos/trained_model_final.zip",
            f"{self.base_dir}/modelos/emergency_final_model.zip",
            f"{self.base_dir}/modelos/trained_model_BACKUP_Fase_1_B√°sica.zip",
            f"{self.base_dir}/modelos/trained_model_BACKUP_Fase_2_Intermedi√°ria.zip", 
            f"{self.base_dir}/modelos/trained_model_BACKUP_Fase_3_Avan√ßada.zip",
            f"{self.base_dir}/checkpoints/Fase_1_B√°sica_model.zip",
            f"{self.base_dir}/checkpoints/Fase_2_Intermedi√°ria_model.zip",
            f"{self.base_dir}/checkpoints/Fase_3_Avan√ßada_model.zip",
            f"{self.base_dir}/modelos/emergency_trained_model.zip",
            "treino_principal/modelos/emergency_final_model.zip",
            "treino_principal/modelos/emergency_trained_model.zip",
            "emergency_final_model.zip",
            "emergency_trained_model.zip",
            "RESCUE_MODEL.zip"
        ]
        
        # Adicionar modelos com timestamp
        import glob
        timestamp_models = glob.glob("trained_model_final_*.zip") + glob.glob("emergency_model_*.zip")
        possible_model_paths.extend(timestamp_models)
        
        # Verificar quais modelos existem
        existing_models = []
        for path in possible_model_paths:
            if os.path.exists(path):
                try:
                    size = os.path.getsize(path)
                    size_mb = size / (1024 * 1024)
                    mtime = os.path.getmtime(path)
                    mtime_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(mtime))
                    existing_models.append({
                        'path': os.path.abspath(path),
                        'size_mb': size_mb,
                        'modified': mtime_str
                    })
                except:
                    existing_models.append({
                        'path': os.path.abspath(path),
                        'size_mb': 0,
                        'modified': 'N/A'
                    })
        
        if existing_models:
            print("‚úÖ MODELOS ENCONTRADOS:")
            for i, model in enumerate(existing_models, 1):
                print(f"  {i}. {model['path']}")
                print(f"     üìÅ Tamanho: {model['size_mb']:.1f} MB")
                print(f"     üìÖ Modificado: {model['modified']}")
                print()
                
            # Modelo mais recente
            if existing_models:
                latest_model = max(existing_models, key=lambda x: os.path.getmtime(x['path']) if os.path.exists(x['path']) else 0)
                print("üèÜ MODELO MAIS RECENTE:")
                print(f"    {latest_model['path']}")
                print(f"    üìÅ {latest_model['size_mb']:.1f} MB | üìÖ {latest_model['modified']}")
                
        else:
            print("‚ùå NENHUM MODELO ENCONTRADO!")
            print("   Poss√≠veis locais verificados:")
            for path in possible_model_paths[:5]:  # Mostrar apenas os 5 primeiros
                print(f"   - {path}")
            print("   ...")
        
        # Criar arquivo de log com caminhos
        try:
            log_file = f"{self.base_dir}/MODELOS_SALVOS_LOG.txt"
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write("RELAT√ìRIO DE MODELOS SALVOS\n")
                f.write("=" * 50 + "\n")
                f.write(f"Data/Hora: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                if existing_models:
                    f.write("MODELOS ENCONTRADOS:\n")
                    for i, model in enumerate(existing_models, 1):
                        f.write(f"{i}. {model['path']}\n")
                        f.write(f"   Tamanho: {model['size_mb']:.1f} MB\n")
                        f.write(f"   Modificado: {model['modified']}\n\n")
                else:
                    f.write("NENHUM MODELO ENCONTRADO\n")
                    f.write("Locais verificados:\n")
                    for path in possible_model_paths:
                        f.write(f"- {path}\n")
            
            print(f"üìù LOG SALVO EM: {os.path.abspath(log_file)}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao criar log: {e}")
        
        print("="*80)
        print("üí° DICA: Use o arquivo 'MODELOS_SALVOS_LOG.txt' para refer√™ncia futura")
        print("="*80)

    def _create_model(self, env):
        """Criar modelo PPO com configura√ß√µes otimizadas e continua√ß√£o autom√°tica"""
        self.logger.info("üîç Verificando modelos existentes para continua√ß√£o do treinamento...")
        
        # üî• AMP: Configurar device policy para mixed precision
        device_policy = "cuda" if torch.cuda.is_available() else "cpu"
        
        # üî• CHECKPOINT: Verificar se existe modelo salvo para continuar treinamento
        checkpoint_info = self._find_latest_checkpoint()
        checkpoint_path = checkpoint_info[0] if isinstance(checkpoint_info, tuple) else checkpoint_info
        if checkpoint_path:
            self.logger.info(f"üìÇ MODELO ENCONTRADO: {os.path.basename(checkpoint_path)}")
            try:
                # Carregar modelo existente
                model = RecurrentPPO.load(checkpoint_path, env=env, device=device_policy)
                # üî• NOVO: Extrair informa√ß√µes do modelo carregado
                steps_completed = model.num_timesteps
                steps_from_name = self._extract_steps_from_filename(os.path.basename(checkpoint_path))
                
                # üî• AMP: Configurar GradScaler se AMP estiver habilitado
                if ENABLE_AMP and hasattr(model, 'policy'):
                    model._amp_scaler = GradScaler()
                    self.logger.info("‚úÖ GradScaler configurado para modelo carregado")
                
                self.logger.info("=" * 60)
                self.logger.info("üîÑ CONTINUANDO TREINAMENTO EXISTENTE")
                self.logger.info("=" * 60)
                self.logger.info(f"üìÅ Arquivo: {os.path.basename(checkpoint_path)}")
                self.logger.info(f"üìä Steps do modelo: {steps_completed:,}")
                self.logger.info(f"üìà Steps do nome: {steps_from_name:,}")
                self.logger.info(f"üéØ Device: {device_policy}")
                self.logger.info(f"üìÖ Modificado: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(os.path.getmtime(checkpoint_path)))}")
                
                # üî• CONFIRMA√á√ïES PARA MODELO CARREGADO
                self.logger.info("=" * 60)
                self.logger.info("üîß CONFIGURA√á√ïES DO MODELO CARREGADO:")
                self.logger.info("=" * 60)
                
                # Verificar configura√ß√µes do modelo carregado
                if hasattr(model.policy, 'features_extractor'):
                    extractor_name = model.policy.features_extractor.__class__.__name__
                    self.logger.info(f"ü§ñ Features Extractor: {extractor_name}")
                    if hasattr(model.policy.features_extractor, 'features_dim'):
                        self.logger.info(f"üìä Features Dimension: {model.policy.features_extractor.features_dim}")
                    
                    # Verificar se √© TradingTransformerFeatureExtractor
                    if 'Transformer' in extractor_name:
                        self.logger.info("üöÄ TRANSFORMER FEATURE EXTRACTOR ATIVO!")
                        if hasattr(model.policy.features_extractor, 'window_size'):
                            self.logger.info(f"   Window Size: {model.policy.features_extractor.window_size}")
                        if hasattr(model.policy.features_extractor, 'n_market_features'):
                            self.logger.info(f"   Market Features: {model.policy.features_extractor.n_market_features}")
                        if hasattr(model.policy.features_extractor, 'max_positions'):
                            self.logger.info(f"   Max Positions: {model.policy.features_extractor.max_positions}")
                    else:
                        self.logger.warning(f"‚ö†Ô∏è Features Extractor n√£o √© Transformer: {extractor_name}")
                
                self.logger.info(f"üß† Policy: {model.policy.__class__.__name__}")
                self.logger.info(f"‚ö° Device: {device_policy}")
                
                # Verificar consist√™ncia
                if steps_from_name > 0 and abs(steps_completed - steps_from_name) > 1000:
                    self.logger.warning(f"‚ö†Ô∏è INCONSIST√äNCIA: Steps do modelo ({steps_completed:,}) != Steps do nome ({steps_from_name:,})")
                    self.logger.warning("   Usando steps do modelo como refer√™ncia")
                
                self.logger.info("=" * 60)
                return model
                
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro ao carregar modelo: {e}")
                self.logger.info("üîÑ Criando modelo novo...")
        
        # Criar modelo novo se n√£o encontrou checkpoint v√°lido
        self.logger.info("üÜï CRIANDO MODELO NOVO")
        self.logger.info("=" * 60)
        
        # Configura√ß√µes do modelo com AMP (modelo novo)
        model_config = {
            "policy": TwoHeadV3HybridEnhanced,
            "env": env,
            "learning_rate": BEST_PARAMS["learning_rate"],
            "n_steps": BEST_PARAMS["n_steps"],
            "batch_size": BEST_PARAMS["batch_size"],
            "n_epochs": BEST_PARAMS["n_epochs"],
            "gamma": BEST_PARAMS["gamma"],
            "gae_lambda": BEST_PARAMS["gae_lambda"],
            "clip_range": BEST_PARAMS["clip_range"],
            "ent_coef": BEST_PARAMS["ent_coef"],
            "vf_coef": BEST_PARAMS["vf_coef"],
            "max_grad_norm": BEST_PARAMS["max_grad_norm"],
            "use_sde": False,  # üî• ADICIONADO: use_sde no n√≠vel principal
            "verbose": 1,  # üî• VERBOSE ATIVADO para debug
            "device": device_policy,
            "seed": 42,
            "policy_kwargs": {
                "lstm_hidden_size": BEST_PARAMS["policy_kwargs"]["lstm_hidden_size"],
                "n_lstm_layers": 1,
                "shared_lstm": False,
                "enable_critic_lstm": True,
                "lstm_kwargs": None,
                "net_arch": BEST_PARAMS["policy_kwargs"]["net_arch"],  # üî• CORRIGIDO: Para features_dim=64
                "activation_fn": BEST_PARAMS["policy_kwargs"]["activation_fn"],
                "ortho_init": True,
                "log_std_init": BEST_PARAMS["policy_kwargs"]["log_std_init"],
                "full_std": True,
                "use_expln": False,
                "squash_output": BEST_PARAMS["policy_kwargs"]["squash_output"],
                "features_extractor_class": TradingTransformerFeatureExtractor,
                "features_extractor_kwargs": {'features_dim': 64},
                "optimizer_class": torch.optim.AdamW,  # üî• AMP: AdamW √© mais est√°vel com mixed precision
                "optimizer_kwargs": {
                    "eps": 1e-5,
                    "weight_decay": 0.01,  # üî• AMP: Weight decay para estabilidade
                    "amsgrad": False
                }
            }
        }
        
        # üî• AMP: Configura√ß√µes espec√≠ficas para mixed precision
        if ENABLE_AMP:
            self.logger.info("üöÄ Configurando modelo com AMP (Automatic Mixed Precision)")
            # Configurar GradScaler para AMP
        
        # üî• CONFIRMA√á√ïES DE CONFIGURA√á√ÉO
        self.logger.info("=" * 60)
        self.logger.info("üîß CONFIGURA√á√ïES DO MODELO:")
        self.logger.info("=" * 60)
        self.logger.info(f"üß† Policy: {model_config['policy'].__name__}")
        self.logger.info(f"ü§ñ Features Extractor: {model_config['policy_kwargs']['features_extractor_class'].__name__}")
        self.logger.info(f"üìä Features Dim: {model_config['policy_kwargs']['features_extractor_kwargs']['features_dim']}")
        self.logger.info(f"üßÆ Net Architecture: {model_config['policy_kwargs']['net_arch']}")
        self.logger.info(f"üéØ Learning Rate: {model_config['learning_rate']}")
        self.logger.info(f"üìà Batch Size: {model_config['batch_size']}")
        self.logger.info(f"‚ö° Device: {model_config['device']}")
        self.logger.info("=" * 60)
        
        model = RecurrentPPO(**model_config)
        
        # üî• AMP: Configurar GradScaler se AMP estiver habilitado
        if ENABLE_AMP and hasattr(model, 'policy'):
            model._amp_scaler = GradScaler()
            self.logger.info("‚úÖ GradScaler configurado para AMP")
        
        # üî• CONFIRMA√á√ÉO FINAL DO MODELO
        self.logger.info("=" * 60)
        self.logger.info("‚úÖ MODELO CRIADO COM SUCESSO!")
        self.logger.info("=" * 60)
        
        # Verificar se o features extractor foi configurado corretamente
        if hasattr(model.policy, 'features_extractor'):
            extractor_name = model.policy.features_extractor.__class__.__name__
            self.logger.info(f"ü§ñ Features Extractor: {extractor_name}")
            if hasattr(model.policy.features_extractor, 'features_dim'):
                self.logger.info(f"üìä Features Dimension: {model.policy.features_extractor.features_dim}")
            
            # Verificar se √© TradingTransformerFeatureExtractor
            if 'Transformer' in extractor_name:
                self.logger.info("üöÄ TRANSFORMER FEATURE EXTRACTOR ATIVO!")
                if hasattr(model.policy.features_extractor, 'window_size'):
                    self.logger.info(f"   Window Size: {model.policy.features_extractor.window_size}")
                if hasattr(model.policy.features_extractor, 'n_market_features'):
                    self.logger.info(f"   Market Features: {model.policy.features_extractor.n_market_features}")
                if hasattr(model.policy.features_extractor, 'max_positions'):
                    self.logger.info(f"   Max Positions: {model.policy.features_extractor.max_positions}")
            else:
                self.logger.warning(f"‚ö†Ô∏è Features Extractor n√£o √© Transformer: {extractor_name}")
        
        self.logger.info(f"‚ö° Device: {device_policy}")
        if ENABLE_AMP:
            self.logger.info("üöÄ AMP ativado - Treinamento acelerado!")
        self.logger.info("=" * 60)
            
        return model
    
    def _find_latest_checkpoint(self):
        """Encontrar checkpoint e detectar automaticamente fase e steps para resume training"""
        checkpoint_dirs = [
            f"{self.base_dir}/checkpoints",
            f"{self.base_dir}/modelos", 
            f"{self.base_dir}/models",
            DIFF_ENVSTATE_DIR,
            "checkpoints",
            "logs",  # Adicionar logs como poss√≠vel local
            "Best Model"  # Adicionar Best Model como poss√≠vel local
        ]
        
        # üîç BUSCAR TODOS OS MODELOS DISPON√çVEIS
        available_models = []
        
        for checkpoint_dir in checkpoint_dirs:
            if os.path.exists(checkpoint_dir):
                for file in os.listdir(checkpoint_dir):
                    if file.endswith('.zip') and ('checkpoint' in file.lower() or 'model' in file.lower() or 'ppo' in file.lower()):
                        file_path = os.path.join(checkpoint_dir, file)
                        file_time = os.path.getmtime(file_path)
                        file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
                        
                        # Extrair informa√ß√µes do nome do arquivo
                        steps_from_name = self._extract_steps_from_filename(file)
                        phase_from_name = self._extract_phase_from_filename(file)
                        
                        available_models.append({
                            'path': file_path,
                            'filename': file,
                            'dir': checkpoint_dir,
                            'steps': steps_from_name,
                            'phase': phase_from_name,
                            'size_mb': file_size,
                            'modified': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_time)),
                            'timestamp': file_time
                        })
        
        if not available_models:
            self.logger.info("‚ùå NENHUM MODELO ENCONTRADO - Iniciando treinamento do zero")
            return None, None, None
        
        # Ordenar por steps (maior primeiro), depois por timestamp
        available_models.sort(key=lambda x: (x['steps'], x['timestamp']), reverse=True)
        
        # üéØ SELE√á√ÉO AUTOM√ÅTICA DO MAIS RECENTE
        if available_models:
            latest_model = available_models[0]  # J√° est√° ordenado por steps e timestamp
            
            print("\n" + "="*80)
            print("üîÑ RESUME TRAINING - MODELO DETECTADO AUTOMATICAMENTE")
            print("="*80)
            print(f"üìÅ Arquivo: {latest_model['filename']}")
            print(f"üìÇ Pasta: {latest_model['dir']}")
            print(f"üìä Steps: {latest_model['steps']:,}")
            print(f"üéØ Fase detectada: {latest_model['phase']}")
            print(f"üíæ Tamanho: {latest_model['size_mb']:.1f} MB")
            print(f"üìÖ Modificado: {latest_model['modified']}")
            print("="*80)
            
            # Determinar fase atual baseada nos steps
            current_phase_idx = self._determine_phase_from_steps(latest_model['steps'])
            
            print(f"üîç AN√ÅLISE DE RESUME:")
            print(f"   Steps do modelo: {latest_model['steps']:,}")
            print(f"   Fase calculada: {current_phase_idx + 1}/5")
            print(f"   Continuar√° da fase: {self.phases[current_phase_idx].name}")
            print("="*80)
            
            return latest_model['path'], current_phase_idx, latest_model['steps']
        else:
            print("‚ùå NENHUM MODELO ENCONTRADO - Iniciando do zero")
            return None, None, None

    def _extract_steps_from_filename(self, filename):
        """Extrair n√∫mero de steps do nome do arquivo"""
        import re
        
        # Padr√µes comuns para extrair steps do nome do arquivo
        patterns = [
            r'(\d+)_steps',           # formato: model_123456_steps.zip
            r'step_(\d+)',            # formato: model_step_123456.zip  
            r'checkpoint_(\d+)',      # formato: checkpoint_123456.zip
            r'model_(\d+)',           # formato: model_123456.zip
            r'ppo_(\d+)',             # formato: ppo_123456.zip
            r'_(\d{4,})_',            # qualquer n√∫mero com 4+ d√≠gitos entre underscores
            r'_(\d{4,})\.',           # qualquer n√∫mero com 4+ d√≠gitos antes da extens√£o
        ]
        
        for pattern in patterns:
            match = re.search(pattern, filename.lower())
            if match:
                try:
                    steps = int(match.group(1))
                    # Validar se √© um n√∫mero razo√°vel de steps (entre 1000 e 10M)
                    if 1000 <= steps <= 10_000_000:
                        return steps
                except ValueError:
                    continue
        
        return 0  # Retornar 0 se n√£o conseguir extrair steps

    def _extract_phase_from_filename(self, filename):
        """Extrair nome da fase do nome do arquivo"""
        import re
        
        # Padr√µes para detectar fase no nome do arquivo
        phase_patterns = [
            r'phase_?(\d+)',          # formato: phase_1, phase1
            r'fundamentals',          # fase 1
            r'risk_management',       # fase 2
            r'noise_handling',        # fase 3
            r'stress_testing',        # fase 4
            r'integration',           # fase 5
        ]
        
        filename_lower = filename.lower()
        
        for i, pattern in enumerate(phase_patterns):
            if re.search(pattern, filename_lower):
                if i == 0:  # padr√£o phase_X
                    match = re.search(r'phase_?(\d+)', filename_lower)
                    if match:
                        return f"Phase_{match.group(1)}"
                else:
                    # Mapear nome da fase para n√∫mero
                    phase_map = {
                        'fundamentals': 'Phase_1',
                        'risk_management': 'Phase_2', 
                        'noise_handling': 'Phase_3',
                        'stress_testing': 'Phase_4',
                        'integration': 'Phase_5'
                    }
                    return phase_map.get(pattern, 'Unknown')
        
        return 'Unknown'  # Retornar Unknown se n√£o conseguir detectar

    def _determine_phase_from_steps(self, steps):
        """üî• CORRIGIDO: Determinar √≠ndice da fase baseado no dataset atual (2.3M total)"""
        # Fases atualizadas: 460k, 575k, 575k, 460k, 230k (total acumulado)
        phase_thresholds = [
            460000,   # Fase 1: 0 - 460k (20% do total)
            1035000,  # Fase 2: 460k - 1.035M (25% adicional)
            1610000,  # Fase 3: 1.035M - 1.61M (25% adicional)
            2070000,  # Fase 4: 1.61M - 2.07M (20% adicional)
            2300000   # Fase 5: 2.07M - 2.3M (10% adicional)
        ]
        
        for i, threshold in enumerate(phase_thresholds):
            if steps < threshold:
                return i
        
        # Se passou de todas as fases, est√° na √∫ltima
        return len(phase_thresholds) - 1

# ====================================================================
# MAIN FUNCTION - SISTEMA AVAN√áADO
# ====================================================================

def main():
    """Main function com sistema de treinamento avan√ßado"""
    try:
        import sys
        instance_id = int(sys.argv[1]) if len(sys.argv) > 1 else 0
        
        print("=" * 60)
        print(" SISTEMA DE TREINAMENTO AVAN√áADO")
        print("=" * 60)
        
        # Verificar GPU
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            print(f"GPU dispon√≠vel: {gpu_name}")
            print(f"CUDA vers√£o: {torch.version.cuda}")
        else:
            print("AVISO: GPU n√£o dispon√≠vel, usando CPU")
        
        # üî• INICIALIZAR SISTEMA DE AVALIA√á√ÉO ON-DEMAND GLOBAL
        global on_demand_eval
        on_demand_eval = OnDemandEvaluationSystem()
        
        # Inicializar sistema avan√ßado
        advanced_system = AdvancedTrainingSystem()
        
        # Executar treinamento completo
        advanced_system.train()
        
        print("\n" + "=" * 60)
        print(" TREINAMENTO AVAN√áADO CONCLU√çDO COM SUCESSO!")
        print("=" * 60)
        
    except KeyboardInterrupt:
        print("\nTreinamento interrompido pelo usu√°rio.")
    except Exception as e:
        print(f"\nERRO durante o treinamento: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
