# üîç AN√ÅLISE HYPERPAR√ÇMETROS HeadV6.py - Causas Poss√≠veis dos Zeros

## **Hyperpar√¢metros Cr√≠ticos Encontrados:**

```python
BEST_PARAMS = {
    "learning_rate": 2.678385767462569e-05,  # 2.68e-5 - MUITO BAIXO!
    "n_steps": 1792,                         
    "batch_size": 64,                        
    "n_epochs": 4,                           
    "gamma": 0.99,                          
    "gae_lambda": 0.95,                     
    "clip_range": 0.0824,                    
    "ent_coef": 0.01709320402078782,         
    "vf_coef": 0.6017559963200034,           
    "max_grad_norm": 0.5,                    # GRADIENT CLIPPING RIGOROSO!
}
```

## **üö® POSS√çVEIS CAUSAS DOS 30.9% ZEROS:**

### **1. LEARNING RATE EXTREMAMENTE BAIXO**
- **Valor**: `2.678e-05` (0.00002678)
- **Problema**: Learning rate muito baixo pode fazer gradientes ficarem pr√≥ximos de zero
- **LayerNorm sens√≠vel**: LayerNorms s√£o especialmente sens√≠veis a LR baixo

### **2. GRADIENT CLIPPING MUITO AGRESSIVO**  
- **Valor**: `max_grad_norm = 0.5`
- **Problema**: Clipping muito rigoroso pode zerar gradientes pequenos
- **LayerNorm vulner√°vel**: Gradientes de LayerNorm s√£o tipicamente menores

### **3. SCHEDULER DIN√ÇMICO PROBLEM√ÅTICO**
```python
self.lr_scheduler = DynamicLearningRateScheduler(
    initial_lr=BEST_PARAMS["learning_rate"],  # J√° baixo
    patience=25000,
    factor=0.85,                              # Reduz mais ainda
    min_lr=1e-7                              # Pode chegar a quase zero!
)
```

### **4. CONFIGURA√á√ÉO DE DROPOUT (poss√≠vel)**
- N√£o visto diretamente, mas pode estar em `get_v6_kwargs()`
- Dropout alto pode causar zeros artificiais

## **üéØ SOLU√á√ïES RECOMENDADAS:**

### **Solu√ß√£o 1: Aumentar Learning Rate**
```python
"learning_rate": 1e-4,  # Ao inv√©s de 2.68e-5
```

### **Solu√ß√£o 2: Relaxar Gradient Clipping**  
```python
"max_grad_norm": 1.0,  # Ao inv√©s de 0.5
```

### **Solu√ß√£o 3: Desabilitar/Ajustar LR Scheduler**
```python
# Comentar ou aumentar min_lr
min_lr=1e-5  # Ao inv√©s de 1e-7
```

### **Solu√ß√£o 4: LayerNorm espec√≠fico**
- Usar learning rate diferente para LayerNorms
- Ou desabilitar weight decay em LayerNorms

## **üî¨ TESTE IMEDIATO:**
Criar um experimento V8Heritage com:
1. `learning_rate: 1e-4` (4x maior)
2. `max_grad_norm: 1.0` (2x menos rigoroso)  
3. LR scheduler desabilitado temporariamente

Isso deve resolver os 30.9% zeros no `entry_quality_head.1.weight` (LayerNorm).