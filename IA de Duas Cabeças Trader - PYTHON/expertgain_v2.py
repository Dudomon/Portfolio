#!/usr/bin/env python3
"""
üéØ EXPERTGAIN V2 - FINE-TUNING INTELIGENTE
Sistema especializado que REALMENTE funciona para melhorar Entry Quality
"""

import sys
import os
sys.path.append("D:/Projeto")

# Copiar todo o c√≥digo base do expertgain.py
# MAS com estas configura√ß√µes especializadas:

# ========== CONFIGURA√á√ÉO EXPERTGAIN V2 ==========

# üî• HIPERPAR√ÇMETROS OTIMIZADOS PARA FINE-TUNING
EXPERTGAIN_V2_PARAMS = {
    # üéØ LR com WARM-UP e DECAY
    "learning_rate": 3.5e-04,  # Come√ßa ALTO para escapar do m√≠nimo local
    "lr_schedule": {
        "warmup_steps": 50000,      # Warm-up gradual
        "decay_rate": 0.95,          # Decay a cada milestone
        "milestones": [500000, 1000000, 1500000]  # Pontos de redu√ß√£o
    },
    
    # üéØ BATCH E EPOCHS OTIMIZADOS
    "n_steps": 2048,
    "batch_size": 128,              # MAIOR para estabilidade
    "n_epochs": 8,                  # MAIS epochs para explora√ß√£o
    
    # üéØ PPO PARAMETERS AJUSTADOS
    "gamma": 0.99,                  # Mant√©m vis√£o de longo prazo
    "gae_lambda": 0.95,             # GAE padr√£o
    "clip_range": 0.25,             # MAIS liberdade para explorar
    "clip_range_vf": None,          # Sem clip no value function
    
    # üéØ ENTROPY PROGRESSIVO
    "ent_coef": 0.02,               # COME√áA com mais explora√ß√£o
    "ent_coef_schedule": {
        "initial": 0.02,
        "final": 0.005,
        "decay_steps": 1000000
    },
    
    # üéØ VALUE FUNCTION
    "vf_coef": 0.5,                 # Balanceado
    "max_grad_norm": 1.0,           # Permite gradientes maiores
    
    # üéØ TARGET KL DIN√ÇMICO
    "target_kl": 0.03,              # Permite mudan√ßas maiores
    "target_kl_schedule": {
        "initial": 0.05,            # Come√ßa permitindo grandes mudan√ßas
        "final": 0.01,              # Termina conservador
        "decay_steps": 1500000
    }
}

# üéØ FASES ESPECIALIZADAS COM OBJETIVOS CLAROS
EXPERTGAIN_V2_PHASES = [
    {
        "name": "Phase_1_Unlock_Gates",
        "steps": 500000,  # 500k steps apenas
        "objective": "Desbloquear gates travadas em 0.038",
        "config": {
            "learning_rate": 4.0e-04,  # LR ALTO para quebrar in√©rcia
            "ent_coef": 0.03,          # MUITA explora√ß√£o
            "clip_range": 0.3,         # Liberdade m√°xima
            "target_entry_quality": 0.15  # Meta modesta inicial
        },
        "success_metrics": {
            "entry_quality_min": 0.10,
            "trades_per_episode": 1
        }
    },
    {
        "name": "Phase_2_Calibrate_Quality",
        "steps": 750000,  # 750k steps
        "objective": "Elevar Entry Quality para 0.30+",
        "config": {
            "learning_rate": 2.5e-04,  # LR m√©dio
            "ent_coef": 0.015,         # Explora√ß√£o moderada
            "clip_range": 0.2,         # Liberdade controlada
            "target_entry_quality": 0.30
        },
        "success_metrics": {
            "entry_quality_min": 0.25,
            "trades_per_episode": 3,
            "win_rate_min": 0.45
        }
    },
    {
        "name": "Phase_3_Optimize_Trading",
        "steps": 750000,  # 750k steps
        "objective": "Atingir Entry Quality 0.50+ com trades consistentes",
        "config": {
            "learning_rate": 1.5e-04,  # LR conservador
            "ent_coef": 0.008,         # Pouca explora√ß√£o
            "clip_range": 0.15,        # Mais focado
            "target_entry_quality": 0.55
        },
        "success_metrics": {
            "entry_quality_min": 0.45,
            "trades_per_episode": 5,
            "win_rate_min": 0.50,
            "positive_return": True
        }
    }
]

# üéØ SISTEMA DE REWARD MODIFICADO PARA EXPERTGAIN
class ExpertGainRewardShaper:
    """
    Sistema de reward especializado para aumentar Entry Quality
    """
    def __init__(self, target_quality=0.5):
        self.target_quality = target_quality
        self.quality_history = []
        
    def shape_reward(self, original_reward, action, info):
        """
        Adiciona bonus/penalidade baseado em Entry Quality
        """
        entry_quality = action[1] if len(action) > 1 else 0.0
        
        # Hist√≥rico para suaviza√ß√£o
        self.quality_history.append(entry_quality)
        if len(self.quality_history) > 100:
            self.quality_history.pop(0)
        
        # Reward shaping baseado em quality
        quality_bonus = 0.0
        
        # 1. BONUS por quality alto
        if entry_quality > self.target_quality:
            quality_bonus += 0.5 * (entry_quality - self.target_quality)
        
        # 2. PENALIDADE por quality muito baixo
        if entry_quality < 0.1:
            quality_bonus -= 0.3
        
        # 3. BONUS por MELHORIA
        if len(self.quality_history) > 10:
            recent_avg = sum(self.quality_history[-10:]) / 10
            old_avg = sum(self.quality_history[:10]) / 10
            if recent_avg > old_avg:
                quality_bonus += 0.2 * (recent_avg - old_avg)
        
        # 4. BONUS por executar trades com quality alto
        if info.get("trade_executed") and entry_quality > 0.4:
            quality_bonus += 1.0 * entry_quality
        
        # 5. PENALIDADE por 100% HOLD
        if info.get("episode_done"):
            if info.get("total_trades", 0) == 0:
                quality_bonus -= 2.0  # Forte penalidade por n√£o tradear
        
        return original_reward + quality_bonus

# üéØ CALLBACK ESPECIALIZADO PARA MONITORAMENTO
class ExpertGainMonitor:
    """
    Monitor especializado para acompanhar Entry Quality
    """
    def __init__(self):
        self.entry_qualities = []
        self.trade_counts = []
        self.phase_start_quality = None
        
    def on_step(self, action, reward, done, info):
        """
        Monitora cada step
        """
        entry_quality = action[1] if len(action) > 1 else 0.0
        self.entry_qualities.append(entry_quality)
        
        if done:
            avg_quality = sum(self.entry_qualities) / len(self.entry_qualities)
            print(f"üìä Episode Entry Quality Avg: {avg_quality:.3f}")
            
            # Alert se quality est√° travado
            if len(set(self.entry_qualities[-100:])) < 5:
                print("‚ö†Ô∏è ALERTA: Entry Quality TRAVADO! Aumentar LR ou entropy!")
            
            self.entry_qualities = []
    
    def check_phase_progress(self, current_quality):
        """
        Verifica progresso da fase
        """
        if self.phase_start_quality is None:
            self.phase_start_quality = current_quality
            return
        
        improvement = current_quality - self.phase_start_quality
        
        if improvement < 0.05:  # Menos de 5% de melhoria
            print("‚ö†Ô∏è PROGRESSO LENTO: Considere aumentar LR ou entropy")
            return "adjust_lr"
        elif improvement > 0.15:  # Mais de 15% de melhoria
            print("‚úÖ EXCELENTE PROGRESSO: Pode reduzir LR para consolidar")
            return "reduce_lr"
        
        return "continue"

# üéØ DYNAMIC LEARNING RATE ADJUSTER
class DynamicLRAdjuster:
    """
    Ajusta LR dinamicamente baseado em performance
    """
    def __init__(self, model):
        self.model = model
        self.quality_history = []
        self.lr_history = []
        self.stagnation_counter = 0
        
    def update(self, current_quality):
        """
        Atualiza LR baseado em Entry Quality
        """
        self.quality_history.append(current_quality)
        
        if len(self.quality_history) > 10:
            # Detecta estagna√ß√£o
            recent = self.quality_history[-10:]
            if max(recent) - min(recent) < 0.01:  # Varia√ß√£o < 1%
                self.stagnation_counter += 1
                
                if self.stagnation_counter > 5:
                    # AUMENTA LR para escapar do m√≠nimo local
                    current_lr = self.model.learning_rate
                    new_lr = min(current_lr * 1.5, 5e-04)  # Cap em 5e-04
                    
                    print(f"üî• ESTAGNA√á√ÉO DETECTADA! LR: {current_lr:.2e} ‚Üí {new_lr:.2e}")
                    self.model.learning_rate = new_lr
                    self.stagnation_counter = 0
            else:
                self.stagnation_counter = 0
        
        # Se quality muito alto, reduz LR para refinar
        if current_quality > 0.5 and len(self.quality_history) > 50:
            recent_std = np.std(self.quality_history[-50:])
            if recent_std < 0.05:  # Est√°vel e bom
                current_lr = self.model.learning_rate
                new_lr = max(current_lr * 0.9, 5e-05)  # Floor em 5e-05
                print(f"‚úÖ PERFORMANCE BOA! LR: {current_lr:.2e} ‚Üí {new_lr:.2e}")
                self.model.learning_rate = new_lr

# üéØ EARLY STOPPING INTELIGENTE
class SmartEarlyStopping:
    """
    Para treinamento se n√£o houver progresso REAL
    """
    def __init__(self, patience=100000, min_improvement=0.01):
        self.patience = patience
        self.min_improvement = min_improvement
        self.best_quality = 0
        self.steps_without_improvement = 0
        
    def should_stop(self, current_quality, current_trades):
        """
        Decide se deve parar
        """
        # Se n√£o est√° tradeando, n√£o vale a pena continuar
        if current_trades == 0 and self.steps_without_improvement > 50000:
            print("‚ùå MODELO TRAVADO EM HOLD! Parando treinamento.")
            return True
        
        # Verifica melhoria em quality
        if current_quality > self.best_quality + self.min_improvement:
            self.best_quality = current_quality
            self.steps_without_improvement = 0
            print(f"‚úÖ Nova melhor Entry Quality: {self.best_quality:.3f}")
        else:
            self.steps_without_improvement += 1
        
        if self.steps_without_improvement > self.patience:
            print(f"‚ö†Ô∏è Sem melhoria h√° {self.patience} steps. Parando.")
            return True
        
        return False

# üéØ MAIN FUNCTION ESPECIALIZADA
def train_expertgain_v2():
    """
    Treina ExpertGain V2 com todas as otimiza√ß√µes
    """
    print("üöÄ EXPERTGAIN V2 - FINE-TUNING INTELIGENTE")
    print("=" * 60)
    
    # 1. Carregar checkpoint base do DayTrader (8M steps)
    base_checkpoint = "D:/Projeto/Otimizacao/treino_principal/models/DAYTRADER/DAYTRADER_phase4stresstesting_8000000_steps_20250808_173027.zip"
    if not os.path.exists(base_checkpoint):
        print(f"‚ùå Checkpoint DayTrader 8M n√£o encontrado: {base_checkpoint}")
        return
    
    print(f"‚úÖ Usando checkpoint DayTrader 8M: {os.path.basename(base_checkpoint)}")
    
    # 2. Criar ambiente com reward shaping
    env = create_expertgain_env(reward_shaper=ExpertGainRewardShaper())
    
    # 3. Carregar modelo com novos hiperpar√¢metros
    model = load_with_new_params(base_checkpoint, EXPERTGAIN_V2_PARAMS)
    
    # 4. Inicializar sistemas de monitoramento
    monitor = ExpertGainMonitor()
    lr_adjuster = DynamicLRAdjuster(model)
    early_stopping = SmartEarlyStopping()
    
    # 5. Treinar por fases
    for phase in EXPERTGAIN_V2_PHASES:
        print(f"\nüéØ INICIANDO {phase['name']}")
        print(f"   Objetivo: {phase['objective']}")
        print(f"   Steps: {phase['steps']:,}")
        
        # Aplicar configura√ß√µes da fase
        apply_phase_config(model, phase['config'])
        
        # Treinar
        for step in range(phase['steps']):
            model.learn(total_timesteps=1000, callback=monitor)
            
            # Ajustes din√¢micos
            if step % 10000 == 0:
                current_quality = get_average_entry_quality(model)
                lr_adjuster.update(current_quality)
                
                if early_stopping.should_stop(current_quality, get_trade_count(model)):
                    break
        
        # Verificar sucesso da fase
        if check_phase_success(phase['success_metrics']):
            print(f"‚úÖ {phase['name']} CONCLU√çDA COM SUCESSO!")
        else:
            print(f"‚ö†Ô∏è {phase['name']} n√£o atingiu todas as metas")
    
    print("\nüèÜ EXPERTGAIN V2 TREINAMENTO CONCLU√çDO!")

if __name__ == "__main__":
    train_expertgain_v2()