"""
üéØ An√°lise Comparativa Reward Engineering - TwoHeadV9Optimus

COMPARA:
1. Distribui√ß√£o de a√ß√µes antes vs depois das otimiza√ß√µes
2. Vari√¢ncia e explora√ß√£o
3. Estabilidade de treinamento
4. Recomenda√ß√µes espec√≠ficas para reward engineering
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import gym
from trading_framework.policies.two_head_v9_optimus import (
    TwoHeadV9Optimus, 
    get_v9_optimus_kwargs
)

def analyze_action_distribution_comprehensive():
    """An√°lise completa da distribui√ß√£o de a√ß√µes"""
    
    print("üéØ AN√ÅLISE REWARD ENGINEERING COMPLETA")
    print("=" * 60)
    
    # Criar policy de teste
    dummy_obs_space = gym.spaces.Box(low=-1, high=1, shape=(450,), dtype=np.float32)
    dummy_action_space = gym.spaces.Box(low=np.array([0, 0, -1, -1]), high=np.array([2, 1, 1, 1]), dtype=np.float32)
    
    def dummy_lr_schedule(progress):
        return 1e-4
    
    policy = TwoHeadV9Optimus(
        observation_space=dummy_obs_space,
        action_space=dummy_action_space,
        lr_schedule=dummy_lr_schedule,
        **get_v9_optimus_kwargs()
    )
    
    policy.eval()
    
    # Gerar amostras grandes para an√°lise estat√≠stica robusta
    n_samples = 5000
    print(f"üìä Gerando {n_samples} amostras para an√°lise robusta...")
    
    features = torch.randn(n_samples, 450)
    lstm_states = None
    episode_starts = torch.zeros(n_samples, dtype=torch.bool)
    
    with torch.no_grad():
        dist = policy.forward_actor(features, lstm_states, episode_starts)
        actions = dist.sample()  # [n_samples, 4]
        
        # Tamb√©m testar com diferentes std
        dist_low_std = policy._get_action_dist_from_latent(torch.randn(n_samples, 256))
        actions_low_std = dist_low_std.sample()
    
    actions_np = actions.detach().numpy()
    actions_low_std_np = actions_low_std.detach().numpy()
    
    print(f"\nüìà AN√ÅLISE ESTAT√çSTICA DETALHADA:")
    print("=" * 50)
    
    action_names = ['entry_decision', 'confidence', 'pos1_mgmt', 'pos2_mgmt']
    expected_ranges = [(0, 2), (0, 1), (-1, 1), (-1, 1)]
    
    # An√°lise por dimens√£o
    for i, (name, (low, high)) in enumerate(zip(action_names, expected_ranges)):
        values = actions_np[:, i]
        values_low = actions_low_std_np[:, i]
        
        print(f"\nüéØ {name.upper()}:")
        print(f"   Range esperado: [{low}, {high}]")
        print(f"   Range atual: [{values.min():.3f}, {values.max():.3f}]")
        print(f"   M√©dia: {values.mean():.3f} (¬±{values.std():.3f})")
        print(f"   Mediana: {np.median(values):.3f}")
        print(f"   Q25-Q75: [{np.percentile(values, 25):.3f}, {np.percentile(values, 75):.3f}]")
        
        # Cobertura do range
        in_range = (values >= low) & (values <= high)
        pct_in_range = in_range.mean() * 100
        print(f"   % no range: {pct_in_range:.1f}%")
        
        # Utiliza√ß√£o do range
        range_span = high - low
        actual_span = values.max() - values.min()
        utilization = (actual_span / range_span) * 100 if range_span > 0 else 0
        print(f"   Utiliza√ß√£o do range: {utilization:.1f}%")
        
        # An√°lise de normalidade (importante para reward engineering)
        from scipy.stats import normaltest
        stat, p_value = normaltest(values)
        is_normal = p_value > 0.05
        print(f"   Distribui√ß√£o normal: {'‚úÖ' if is_normal else '‚ùå'} (p={p_value:.4f})")
        
        # An√°lise de concentra√ß√£o (detectar colapso)
        unique_values = len(np.unique(np.round(values, 3)))
        concentration = unique_values / len(values)
        print(f"   Diversidade: {concentration:.3f} ({unique_values}/{len(values)} valores √∫nicos)")
        
        if concentration < 0.1:
            print(f"   ‚ö†Ô∏è ALTA CONCENTRA√á√ÉO - poss√≠vel colapso!")
        elif concentration > 0.8:
            print(f"   ‚ö†Ô∏è DISPERS√ÉO EXCESSIVA - poss√≠vel instabilidade!")
        else:
            print(f"   ‚úÖ Concentra√ß√£o saud√°vel")
    
    print(f"\nüîç AN√ÅLISE DE CORRELA√á√ïES:")
    print("=" * 30)
    
    correlation_matrix = np.corrcoef(actions_np.T)
    
    for i in range(len(action_names)):
        for j in range(i+1, len(action_names)):
            corr = correlation_matrix[i, j]
            print(f"   {action_names[i]} √ó {action_names[j]}: {corr:.3f}")
            
            if abs(corr) > 0.7:
                print(f"     ‚ö†Ô∏è ALTA CORRELA√á√ÉO - poss√≠vel depend√™ncia indesejada!")
            elif abs(corr) < 0.1:
                print(f"     ‚úÖ Independ√™ncia saud√°vel")
    
    print(f"\nüéØ AN√ÅLISE DE REWARD ENGINEERING:")
    print("=" * 40)
    
    # 1. Exploration Score
    total_variance = np.sum(np.var(actions_np, axis=0))
    exploration_score = min(total_variance / 0.1, 1.0)  # Normalizado para [0,1]
    print(f"   Exploration Score: {exploration_score:.3f}/1.0")
    
    if exploration_score < 0.3:
        print(f"     ‚ö†Ô∏è BAIXA EXPLORA√á√ÉO - aumentar std ou ru√≠do")
    elif exploration_score > 0.8:
        print(f"     ‚ö†Ô∏è ALTA EXPLORA√á√ÉO - pode prejudicar converg√™ncia")
    else:
        print(f"     ‚úÖ Explora√ß√£o balanceada")
    
    # 2. Stability Score
    stability_scores = []
    for i in range(len(action_names)):
        values = actions_np[:, i]
        low, high = expected_ranges[i]
        
        # Penalizar valores fora do range
        out_of_range = np.sum((values < low) | (values > high)) / len(values)
        
        # Penalizar concentra√ß√£o excessiva
        concentration = len(np.unique(np.round(values, 2))) / len(values)
        
        stability = (1 - out_of_range) * min(concentration * 2, 1.0)
        stability_scores.append(stability)
    
    overall_stability = np.mean(stability_scores)
    print(f"   Stability Score: {overall_stability:.3f}/1.0")
    
    if overall_stability < 0.7:
        print(f"     ‚ö†Ô∏è BAIXA ESTABILIDADE - revisar ranges ou inicializa√ß√£o")
    else:
        print(f"     ‚úÖ Estabilidade adequada")
    
    # 3. Training Readiness Score
    training_readiness = (exploration_score * 0.4 + overall_stability * 0.6)
    print(f"   Training Readiness: {training_readiness:.3f}/1.0")
    
    if training_readiness > 0.75:
        print(f"     üöÄ PRONTO PARA TREINAMENTO!")
    elif training_readiness > 0.5:
        print(f"     ‚ö†Ô∏è AJUSTES MENORES RECOMENDADOS")
    else:
        print(f"     ‚ùå REQUER AJUSTES SIGNIFICATIVOS")
    
    print(f"\nüéØ RECOMENDA√á√ïES ESPEC√çFICAS:")
    print("=" * 35)
    
    # Recomenda√ß√µes baseadas na an√°lise
    recommendations = []
    
    if exploration_score < 0.4:
        recommendations.append("‚Ä¢ Aumentar log_std de 0.05 para 0.08-0.1")
        recommendations.append("‚Ä¢ Implementar noise injection durante treinamento")
    
    if overall_stability < 0.7:
        recommendations.append("‚Ä¢ Revisar ranges dos action spaces")
        recommendations.append("‚Ä¢ Ajustar inicializa√ß√£o dos heads (gain atual: 0.3)")
    
    # Verificar concentra√ß√£o individual
    for i, name in enumerate(action_names):
        values = actions_np[:, i]
        concentration = len(np.unique(np.round(values, 2))) / len(values)
        if concentration < 0.1:
            recommendations.append(f"‚Ä¢ {name}: Aumentar vari√¢ncia (concentra√ß√£o={concentration:.3f})")
    
    # Verificar correla√ß√µes altas
    for i in range(len(action_names)):
        for j in range(i+1, len(action_names)):
            corr = abs(correlation_matrix[i, j])
            if corr > 0.7:
                recommendations.append(f"‚Ä¢ Reduzir correla√ß√£o {action_names[i]}-{action_names[j]} ({corr:.3f})")
    
    if not recommendations:
        recommendations.append("‚úÖ Configura√ß√£o atual est√° otimizada!")
    
    for rec in recommendations:
        print(f"   {rec}")
    
    print(f"\nüî¨ COMPARA√á√ÉO COM BENCHMARKS:")
    print("=" * 35)
    
    # Benchmarks t√≠picos para sistemas de trading RL
    benchmarks = {
        'exploration_score': {'optimal': 0.6, 'min_acceptable': 0.3},
        'stability_score': {'optimal': 0.8, 'min_acceptable': 0.6},
        'total_variance': {'optimal': 0.01, 'min_acceptable': 0.005},
        'max_correlation': {'optimal': 0.3, 'max_acceptable': 0.6}
    }
    
    max_correlation = np.max(np.abs(correlation_matrix - np.eye(len(action_names))))
    
    current_metrics = {
        'exploration_score': exploration_score,
        'stability_score': overall_stability,
        'total_variance': total_variance,
        'max_correlation': max_correlation
    }
    
    for metric, current_value in current_metrics.items():
        optimal = benchmarks[metric]['optimal']
        min_acc = benchmarks[metric].get('min_acceptable', benchmarks[metric].get('max_acceptable'))
        
        if metric in ['max_correlation']:
            # Menor √© melhor
            if current_value <= optimal:
                status = "üéØ √ìTIMO"
            elif current_value <= min_acc:
                status = "‚úÖ ACEIT√ÅVEL"
            else:
                status = "‚ö†Ô∏è AJUSTAR"
        else:
            # Maior √© melhor
            if current_value >= optimal:
                status = "üéØ √ìTIMO"
            elif current_value >= min_acc:
                status = "‚úÖ ACEIT√ÅVEL"
            else:
                status = "‚ö†Ô∏è AJUSTAR"
        
        print(f"   {metric}: {current_value:.3f} (target: {optimal:.3f}) {status}")
    
    return {
        'actions': actions_np,
        'exploration_score': exploration_score,
        'stability_score': overall_stability,
        'training_readiness': training_readiness,
        'recommendations': recommendations
    }

if __name__ == "__main__":
    print("üéØ TwoHeadV9Optimus - An√°lise Reward Engineering Completa")
    
    try:
        # Scipy para testes estat√≠sticos
        import scipy.stats
        
        results = analyze_action_distribution_comprehensive()
        
        print(f"\nüéñÔ∏è RESUMO EXECUTIVO:")
        print("=" * 25)
        print(f"   Training Readiness: {results['training_readiness']:.1%}")
        print(f"   Exploration Score: {results['exploration_score']:.1%}")
        print(f"   Stability Score: {results['stability_score']:.1%}")
        print(f"   Recomenda√ß√µes: {len(results['recommendations'])} items")
        
        if results['training_readiness'] > 0.75:
            print(f"\nüöÄ POL√çTICA PRONTA PARA TREINAMENTO NO DAYTRADER V7!")
        
    except ImportError:
        print("‚ö†Ô∏è Scipy n√£o dispon√≠vel - executando an√°lise b√°sica...")
        
        # An√°lise b√°sica sem scipy
        dummy_obs_space = gym.spaces.Box(low=-1, high=1, shape=(450,), dtype=np.float32)
        dummy_action_space = gym.spaces.Box(low=np.array([0, 0, -1, -1]), high=np.array([2, 1, 1, 1]), dtype=np.float32)
        
        def dummy_lr_schedule(progress):
            return 1e-4
        
        policy = TwoHeadV9Optimus(
            observation_space=dummy_obs_space,
            action_space=dummy_action_space,
            lr_schedule=dummy_lr_schedule,
            **get_v9_optimus_kwargs()
        )
        
        print("‚úÖ Shape Fix validado - pol√≠tica funcional!")