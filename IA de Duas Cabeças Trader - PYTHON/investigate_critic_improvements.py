#!/usr/bin/env python3
"""
ğŸ” INVESTIGAÃ‡ÃƒO: MELHORIAS PARA O CRÃTICO - EXPLAINED VARIANCE
Baseado nos problemas encontrados no reward system, investigar soluÃ§Ãµes
"""
import sys
import os
sys.path.append("D:/Projeto")

import numpy as np
import torch

def investigate_critic_improvements():
    """Investigar possÃ­veis melhorias para o explained variance do crÃ­tico"""
    
    print("ğŸ” INVESTIGAÃ‡ÃƒO: MELHORIAS PARA O CRÃTICO")
    print("=" * 60)
    
    print("\nğŸ“Š PROBLEMAS IDENTIFICADOS NO REWARD SYSTEM:")
    print("   âŒ CorrelaÃ§Ã£o muito baixa (-0.01) entre reward e portfolio change")
    print("   âš ï¸ Alta autocorrelaÃ§Ã£o (0.96) - rewards muito relacionados")
    print("   âš ï¸ 95% HOLD actions - modelo muito conservador")
    print("   âš ï¸ Mean reward negativo (-0.42)")
    
    print("\nğŸ’¡ HIPÃ“TESES PARA EXPLAINED VARIANCE RUIM:")
    print("=" * 60)
    
    print("\nğŸ¯ HIPÃ“TESE 1: REWARD SYSTEM INCONSISTENTE")
    print("-" * 50)
    print("   ğŸ” Problema: Rewards nÃ£o refletem performance real")
    print("   ğŸ’Š SoluÃ§Ã£o: Reformular reward para ser mais aligned")
    print("   ğŸ“ ImplementaÃ§Ã£o:")
    print("      - Usar mais componente de PnL real vs artificial")
    print("      - Reduzir peso de penalties abstratas")
    print("      - Aumentar reward por trades bem-sucedidos")
    
    print("\nğŸ¯ HIPÃ“TESE 2: LEARNING RATE DO CRÃTICO MUITO BAIXO") 
    print("-" * 50)
    print("   ğŸ” Problema: CrÃ­tico aprende muito devagar")
    print("   ğŸ’Š SoluÃ§Ã£o: LR diferencial ainda mais agressivo")
    print("   ğŸ“ ImplementaÃ§Ã£o:")
    print("      - Critic LR: 5-8x maior que actor")
    print("      - Warm-up mais longo para crÃ­tico")
    print("      - Update frequency diferente")
    
    print("\nğŸ¯ HIPÃ“TESE 3: ARQUITETURA DO CRÃTICO LIMITADA")
    print("-" * 50)
    print("   ğŸ” Problema: MLP pode nÃ£o capturar complexidade temporal")
    print("   ğŸ’Š SoluÃ§Ã£o: HÃ­brido MLP + Attention ou LSTM leve")
    print("   ğŸ“ ImplementaÃ§Ã£o:")
    print("      - Adicionar camada de atenÃ§Ã£o ao crÃ­tico")
    print("      - LSTM shallow para crÃ­tico (1 layer)")
    print("      - Residual connections")
    
    print("\nğŸ¯ HIPÃ“TESE 4: VALUE CLIPPING MUITO RESTRITIVO")
    print("-" * 50)
    print("   ğŸ” Problema: Gradientes do crÃ­tico sendo clipped demais")
    print("   ğŸ’Š SoluÃ§Ã£o: Relaxar clipping especÃ­fico do crÃ­tico")
    print("   ğŸ“ ImplementaÃ§Ã£o:")
    print("      - Critic-specific clip range")
    print("      - Adaptive clipping baseado na variÃ¢ncia")
    print("      - Gradient norm especÃ­fico para crÃ­tico")
    
    print("\nğŸ¯ HIPÃ“TESE 5: NORMALIZATION DOS REWARDS INADEQUADA")
    print("-" * 50)
    print("   ğŸ” Problema: VecNormalize pode estar mascarando signal")
    print("   ğŸ’Š SoluÃ§Ã£o: Normalization customizada ou desabilitada")
    print("   ğŸ“ ImplementaÃ§Ã£o:")
    print("      - Reward scaling manual")
    print("      - Whitening especÃ­fico para rewards")
    print("      - Running statistics mais conservadoras")
    
    print("\nğŸ¯ HIPÃ“TESE 6: BATCH SIZE INADEQUADO PARA CRÃTICO")
    print("-" * 50)
    print("   ğŸ” Problema: Batch muito pequeno para estimar value")
    print("   ğŸ’Š SoluÃ§Ã£o: Batch size maior ou mini-batches para crÃ­tico")
    print("   ğŸ“ ImplementaÃ§Ã£o:")
    print("      - Critic-specific batch size")
    print("      - Multiple critic updates per policy update")
    print("      - Experience replay buffer para crÃ­tico")
    
    # TESTE PRÃTICO: VERIFICAR GRADIENTES ATUAIS
    print("\nğŸ”¬ TESTE PRÃTICO: VERIFICAR ESTADO ATUAL")
    print("=" * 60)
    
    try:
        from sb3_contrib import RecurrentPPO
        
        # Carregar checkpoint atual
        checkpoint_path = "./Otimizacao/treino_principal/models/DAYTRADER/DAYTRADER_phase3noisehandlingfixed_4800000_steps_20250814_111420.zip"
        
        if os.path.exists(checkpoint_path):
            print(f"ğŸ“‚ Carregando: {os.path.basename(checkpoint_path)}")
            
            model = RecurrentPPO.load(checkpoint_path)
            policy = model.policy
            
            # Verificar learning rates atuais
            if hasattr(model, 'learning_rate'):
                current_lr = model.learning_rate
                print(f"   ğŸ“Š Learning Rate atual: {current_lr}")
            
            # Verificar critic architecture
            if hasattr(policy, 'v7_critic_mlp'):
                critic = policy.v7_critic_mlp
                total_params = sum(p.numel() for p in critic.parameters())
                print(f"   ğŸ§  CrÃ­tico MLP params: {total_params:,}")
                
                # Verificar estrutura
                print(f"   ğŸ“ Critic architecture: {critic}")
            
            # Verificar hyperparameters
            if hasattr(model, 'clip_range'):
                print(f"   âœ‚ï¸ Clip range: {model.clip_range}")
            if hasattr(model, 'vf_coef'):
                print(f"   âš–ï¸ Value function coef: {model.vf_coef}")
            if hasattr(model, 'max_grad_norm'):
                print(f"   ğŸ“ Max grad norm: {model.max_grad_norm}")
                
        else:
            print("   âš ï¸ Checkpoint nÃ£o encontrado para anÃ¡lise")
            
    except Exception as e:
        print(f"   âŒ Erro na anÃ¡lise: {e}")
    
    # RECOMENDAÃ‡Ã•ES PRIORITÃRIAS
    print("\nğŸ† RECOMENDAÃ‡Ã•ES PRIORITÃRIAS")
    print("=" * 60)
    
    recommendations = [
        {
            'priority': 1,
            'title': 'Fix Reward System Correlation',
            'description': 'Reformular reward para correlacionar melhor com performance real',
            'implementation': 'Aumentar peso de PnL real, reduzir penalties abstratas',
            'effort': 'MEDIUM'
        },
        {
            'priority': 2, 
            'title': 'Critic Learning Rate Boost',
            'description': 'Aumentar LR do crÃ­tico para 6-8x o do actor',
            'implementation': 'Dynamic LR Manager com ratios mais agressivos',
            'effort': 'LOW'
        },
        {
            'priority': 3,
            'title': 'Reward Normalization Review', 
            'description': 'Revisar se VecNormalize estÃ¡ prejudicando o signal',
            'implementation': 'Testar com reward scaling manual',
            'effort': 'LOW'
        },
        {
            'priority': 4,
            'title': 'Critic Architecture Enhancement',
            'description': 'Adicionar capacidade temporal limitada ao crÃ­tico',
            'implementation': 'Shallow LSTM ou attention layer',
            'effort': 'HIGH'
        },
        {
            'priority': 5,
            'title': 'Value Function Clipping Adjustment',
            'description': 'Relaxar clipping especÃ­fico do crÃ­tico', 
            'implementation': 'Critic-specific hyperparameters',
            'effort': 'LOW'
        }
    ]
    
    for i, rec in enumerate(recommendations, 1):
        print(f"\n   {i}. {rec['title']} [EFFORT: {rec['effort']}]")
        print(f"      ğŸ“‹ {rec['description']}")
        print(f"      ğŸ”§ {rec['implementation']}")
    
    print("\nğŸ’¡ PRÃ“XIMOS PASSOS SUGERIDOS:")
    print("   1. Implementar fix #1 (reward correlation) e #2 (critic LR)")
    print("   2. Testar com reward normalization desabilitada (#3)")
    print("   3. Se ainda nÃ£o resolver, considerar #4 (architecture)")
    print("   4. Monitorar explained variance a cada 500k steps")
    
    return recommendations

if __name__ == "__main__":
    recommendations = investigate_critic_improvements()
    print(f"\nâœ… INVESTIGAÃ‡ÃƒO CONCLUÃDA - {len(recommendations)} recomendaÃ§Ãµes identificadas")