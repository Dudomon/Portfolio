#!/usr/bin/env python3
"""
ğŸš€ ADAPTIVE LEARNING RATE CALLBACK
Ajusta learning rate automaticamente baseado na saÃºde dos gradientes
"""

import torch
import numpy as np
from stable_baselines3.common.callbacks import BaseCallback
from typing import Dict, Any

class AdaptiveLearningRateCallback(BaseCallback):
    """Callback para ajustar learning rate baseado na saÃºde dos gradientes"""
    
    def __init__(self, 
                 initial_lr: float = 2.68e-5,
                 min_lr: float = 1e-6,
                 max_lr: float = 1e-3,
                 adaptation_freq: int = 2000,
                 dead_gradient_threshold: float = 0.3,
                 verbose: int = 0):
        super().__init__(verbose)
        self.initial_lr = initial_lr
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.adaptation_freq = adaptation_freq
        self.dead_gradient_threshold = dead_gradient_threshold
        
        # Tracking
        self.current_lr = initial_lr
        self.lr_history = []
        self.gradient_health_history = []
        self.step_count = 0
        self.adaptations_count = 0
        
    def _on_training_start(self) -> None:
        """Executado no inÃ­cio do treinamento"""
        print(f"ğŸš€ ADAPTIVE LR CALLBACK ATIVADO - LR inicial: {self.initial_lr:.2e}")
        print(f"   ğŸ“Š Range: [{self.min_lr:.2e}, {self.max_lr:.2e}] | Freq: {self.adaptation_freq} steps")
    
    def _on_step(self) -> bool:
        """Executado a cada step do treinamento"""
        self.step_count += 1
        
        # Verificar se Ã© hora de adaptar
        if self.step_count % self.adaptation_freq == 0:
            self._adapt_learning_rate()
        
        return True
    
    def _adapt_learning_rate(self):
        """Adaptar learning rate baseado na saÃºde dos gradientes"""
        try:
            if not hasattr(self.model, 'policy'):
                print(f"âš ï¸ ADAPTIVE LR: Modelo nÃ£o tem policy")
                return
            
            # Calcular saÃºde dos gradientes
            gradient_health = self._calculate_gradient_health()
            self.gradient_health_history.append(gradient_health)
            
            # DEBUG: Sempre mostrar o gradient health
            print(f"ğŸ” ADAPTIVE LR DEBUG - Step {self.num_timesteps}")
            print(f"   ğŸ“Š Gradient Health: {gradient_health:.3f}")
            print(f"   ğŸ“ˆ Current LR: {self.current_lr:.2e}")
            
            # Determinar novo learning rate
            new_lr = self._determine_new_lr(gradient_health)
            
            # Aplicar novo learning rate
            if abs(new_lr - self.current_lr) > 1e-7:  # SÃ³ atualizar se mudanÃ§a significativa
                self._set_learning_rate(new_lr)
                self.adaptations_count += 1
                
                print(f"ğŸ”§ ADAPTIVE LR MUDANÃ‡A - Step {self.num_timesteps}")
                print(f"   ğŸ“ˆ LR: {self.current_lr:.2e} â†’ {new_lr:.2e}")
            else:
                print(f"   âœ… LR mantido (mudanÃ§a < 1e-7)")
            
        except Exception as e:
            print(f"âš ï¸ Erro na adaptaÃ§Ã£o de LR: {e}")
            import traceback
            traceback.print_exc()
    
    def _calculate_gradient_health(self) -> float:
        """Calcular saÃºde geral dos gradientes"""
        total_params = 0
        healthy_gradients = 0
        
        for name, param in self.model.policy.named_parameters():
            if param.grad is not None:
                grad_array = param.grad.detach().cpu().numpy().flatten()
                total_params += len(grad_array)
                
                # Contar gradientes "saudÃ¡veis" (nÃ£o zero extremo)
                healthy_mask = np.abs(grad_array) > 1e-8
                healthy_gradients += np.sum(healthy_mask)
        
        health_ratio = healthy_gradients / max(total_params, 1)
        return health_ratio
    
    def _determine_new_lr(self, gradient_health: float) -> float:
        """Determinar novo learning rate baseado na saÃºde"""
        # EstratÃ©gia adaptativa
        if gradient_health < 0.4:  # Gradientes muito mortos
            # Diminuir LR para estabilizar neurÃ´nios mortos
            scale_factor = 0.5
            print(f"   ğŸ”¥ GRADIENTES MORTOS ({gradient_health:.1%}) - DIMINUINDO LR")
        elif gradient_health < 0.6:  # Gradientes moderadamente mortos
            # Diminuir LR moderadamente
            scale_factor = 0.8
            print(f"   âš ï¸ GRADIENTES FRACOS ({gradient_health:.1%}) - DIMINUINDO LR")
        elif gradient_health > 0.85:  # Gradientes muito ativos
            # Diminuir LR para estabilizar
            scale_factor = 0.8
            print(f"   âš¡ GRADIENTES ATIVOS ({gradient_health:.1%}) - DIMINUINDO LR")
        else:  # Gradientes saudÃ¡veis
            # Manter LR prÃ³ximo ao atual
            scale_factor = 1.0
        
        # Calcular novo LR
        new_lr = self.current_lr * scale_factor
        
        # Aplicar limites
        new_lr = max(self.min_lr, min(self.max_lr, new_lr))
        
        return new_lr
    
    def _set_learning_rate(self, new_lr: float):
        """Definir novo learning rate no optimizer"""
        try:
            if hasattr(self.model, 'policy') and hasattr(self.model.policy, 'optimizer'):
                print(f"ğŸ”§ DEBUG: Aplicando LR {new_lr:.2e} no optimizer...")
                old_lrs = []
                for i, param_group in enumerate(self.model.policy.optimizer.param_groups):
                    old_lrs.append(param_group['lr'])
                    param_group['lr'] = new_lr
                    print(f"   Param group {i}: {old_lrs[i]:.2e} â†’ {new_lr:.2e}")
                
                self.current_lr = new_lr
                self.lr_history.append(new_lr)
                
                # Verificar se foi aplicado
                verification_lrs = [pg['lr'] for pg in self.model.policy.optimizer.param_groups]
                print(f"ğŸ” VERIFICAÃ‡ÃƒO: LRs apÃ³s aplicaÃ§Ã£o: {[f'{lr:.2e}' for lr in verification_lrs]}")
                
            else:
                print(f"âŒ ERRO: Optimizer nÃ£o encontrado!")
                print(f"   Has policy: {hasattr(self.model, 'policy')}")
                if hasattr(self.model, 'policy'):
                    print(f"   Has optimizer: {hasattr(self.model.policy, 'optimizer')}")
                
        except Exception as e:
            print(f"âŒ Erro ao definir LR: {e}")
            import traceback
            traceback.print_exc()
    
    def _on_training_end(self) -> None:
        """Executado no final do treinamento"""
        print(f"ğŸ ADAPTIVE LR CALLBACK FINALIZADO")
        print(f"   ğŸ“Š AdaptaÃ§Ãµes realizadas: {self.adaptations_count}")
        print(f"   ğŸ“ˆ LR final: {self.current_lr:.2e}")
        if self.lr_history:
            print(f"   ğŸ“‰ LR mÃ©dio: {np.mean(self.lr_history):.2e}")

def create_adaptive_lr_callback(
    initial_lr: float = 2.68e-5,
    min_lr: float = 1e-6,
    max_lr: float = 1e-3,
    adaptation_freq: int = 2000,
    verbose: int = 0
) -> AdaptiveLearningRateCallback:
    """Factory function para criar callback de LR adaptativo"""
    return AdaptiveLearningRateCallback(
        initial_lr=initial_lr,
        min_lr=min_lr,
        max_lr=max_lr,
        adaptation_freq=adaptation_freq,
        verbose=verbose
    )

if __name__ == "__main__":
    print("ğŸš€ Adaptive Learning Rate Callback - Ajusta LR baseado na saÃºde dos gradientes")