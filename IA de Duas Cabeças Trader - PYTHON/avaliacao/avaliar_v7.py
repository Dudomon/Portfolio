#!/usr/bin/env python3
"""
üéØ AVALIA√á√ÉO V7INTUITION - MODELOS DAYTRADER
Configura√ß√£o EXATA do daytrader para avalia√ß√£o V7:
- Portfolio: $500 
- Base lot: 0.02
- Max lot: 0.03
- Modo infer√™ncia (deterministic=False)
- Action Space: 8D OTIMIZADO (entry[0,1] + SL/TP[2-7] sistema global+espec√≠fico)
"""

import sys
import os
import traceback
from datetime import datetime
sys.path.append("D:/Projeto")

import numpy as np
import pandas as pd
import torch

# Configura√ß√£o espec√≠fica do usu√°rio - Checkpoint 1.5M steps (P√ìS-PICO DOURADO)
CHECKPOINT_PATH = "D:/Projeto/Otimizacao/treino_principal/models/DAYTRADER/DAYTRADER_phase4integration_7350000_steps_20250820_103721.zip"
INITIAL_PORTFOLIO = 500.0  # $500 conforme solicitado
BASE_LOT_SIZE = 0.02
MAX_LOT_SIZE = 0.03
TEST_STEPS = 3000
NUM_EPISODES = 3  # N√∫mero de epis√≥dios para testar
EPISODE_SPACING = 5000  # Espa√ßamento entre epis√≥dios no dataset

def find_8m_checkpoint():
    """üîç Encontra o checkpoint de 1.1M steps do daytrader (8D OTIMIZADO)"""
    import glob
    
    # Padr√µes para procurar checkpoint de 1.1M
    patterns = [
        "D:/Projeto/Otimizacao/treino_principal/models/DAYTRADER/DAYTRADER_*1100000*.zip",
        "D:/Projeto/Otimizacao/treino_principal/models/DAYTRADER/DAYTRADER_*1.1M*.zip",
        "D:/Projeto/Otimizacao/treino_principal/models/**/DAYTRADER_*1100000*.zip",
        "D:/Projeto/**/DAYTRADER_*1100000*.zip"
    ]
    
    all_checkpoints = []
    for pattern in patterns:
        all_checkpoints.extend(glob.glob(pattern, recursive=True))
    
    if not all_checkpoints:
        # Fallback: procurar qualquer checkpoint recente do DAYTRADER
        fallback_patterns = [
            "D:/Projeto/Otimizacao/treino_principal/models/DAYTRADER/DAYTRADER_*2000000*.zip",
            "D:/Projeto/Otimizacao/treino_principal/models/DAYTRADER/DAYTRADER_*.zip",
            "D:/Projeto/**/DAYTRADER_*.zip"
        ]
        for pattern in fallback_patterns:
            all_checkpoints.extend(glob.glob(pattern, recursive=True))
    
    if not all_checkpoints:
        return None
    
    # Ordenar por data de modifica√ß√£o (mais recente primeiro)
    all_checkpoints.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    
    print(f"üîç Checkpoints DAYTRADER encontrados:")
    for i, cp in enumerate(all_checkpoints[:5]):  # Mostrar top 5
        mod_time = datetime.fromtimestamp(os.path.getmtime(cp)).strftime('%Y-%m-%d %H:%M:%S')
        size_mb = os.path.getsize(cp) / (1024*1024)
        steps = "2.2M" if "2200000" in cp else "?"
        print(f"   {i+1}. {os.path.basename(cp)} ({size_mb:.1f}MB, {steps} steps) - {mod_time}")
    
    return all_checkpoints[0]

def test_v7_intuition_trading():
    """üéØ Teste V7Intuition com configura√ß√µes exatas do daytrader"""
    
    print(f"üí∞ TESTE V7INTUITION - MODELOS DAYTRADER")
    print("=" * 60)
    print(f"üíµ Portfolio Inicial: ${INITIAL_PORTFOLIO}")
    print(f"üìä Base Lot: {BASE_LOT_SIZE}")
    print(f"üìä Max Lot: {MAX_LOT_SIZE}")
    print(f"üß† Modo: INFER√äNCIA (n√£o-determin√≠stico)")
    print("=" * 60)
    
    try:
        # Imports
        from sb3_contrib import RecurrentPPO
        from daytrader import TradingEnv  # üî• USANDO DAYTRADER original
        
        print("‚úÖ Imports carregados")
        
        # Usar checkpoint espec√≠fico configurado
        checkpoint_path = CHECKPOINT_PATH
        if not os.path.exists(checkpoint_path):
            print(f"‚ùå Checkpoint n√£o encontrado: {checkpoint_path}")
            # Fallback para fun√ß√£o de busca
            checkpoint_path = find_8m_checkpoint()
            if not checkpoint_path:
                print("‚ùå Nenhum checkpoint DAYTRADER encontrado!")
                return False
        
        print(f"üìÇ Usando checkpoint: {os.path.basename(checkpoint_path)}")
        
        # Dataset real para teste
        print("üìä Carregando dataset para teste...")
        dataset_path = "D:/Projeto/data/GC_YAHOO_ENHANCED_V3_BALANCED_20250804_192226.csv"
        
        if not os.path.exists(dataset_path):
            print(f"‚ùå Dataset n√£o encontrado: {dataset_path}")
            return False
            
        df = pd.read_csv(dataset_path)
        
        # Processar dataset
        if 'time' in df.columns:
            df['timestamp'] = pd.to_datetime(df['time'])
            df.set_index('timestamp', inplace=True)
            df.drop('time', axis=1, inplace=True)
        
        # Renomear colunas
        df = df.rename(columns={
            'open': 'open_5m',
            'high': 'high_5m',
            'low': 'low_5m', 
            'close': 'close_5m',
            'tick_volume': 'volume_5m'
        })
        
        # Dataset completo para m√∫ltiplos epis√≥dios
        total_len = len(df)
        print(f"‚úÖ Dataset V3 carregado: {total_len:,} barras totais")
        print(f"üìÖ Per√≠odo completo: {df.index.min()} at√© {df.index.max()}")
        print(f"üéØ Configurado para {NUM_EPISODES} epis√≥dios de {TEST_STEPS} steps cada")
        
        # Criar ambiente de trading com configura√ß√µes EXATAS (8D ACTION SPACE OTIMIZADO)
        # üî• TradingEnv do daytrader.py usa 8D otimizado: [0-2, 0-1, -3-3, -3-3, -3-3, -3-3, -3-3, -3-3]
        trading_params = {
            'base_lot_size': BASE_LOT_SIZE,
            'max_lot_size': MAX_LOT_SIZE,
            'initial_balance': INITIAL_PORTFOLIO,
            'target_trades_per_day': 18,  # Como no daytrader
            'stop_loss_range': (2.0, 8.0),
            'take_profit_range': (3.0, 15.0)
        }
        
        # Criar ambiente ser√° feito para cada epis√≥dio
        print("‚úÖ Par√¢metros de trading configurados")
        
        # Carregar modelo
        print("ü§ñ Carregando modelo DAYTRADER 2M (8D OTIMIZADO)...")
        
        # Carregar modelo com compatibilidade for√ßada - FODA-SE AS DIFEREN√áAS
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        try:
            # Tentar carregamento normal primeiro
            model = RecurrentPPO.load(checkpoint_path, device=device)
            print("‚úÖ Carregamento normal bem-sucedido")
        except Exception as e1:
            print(f"‚ö†Ô∏è Carregamento normal falhou: {e1}")
            try:
                # Tentar com policy_kwargs V7Intuition
                from trading_framework.policies.two_head_v7_intuition import get_v7_intuition_kwargs
                intuition_kwargs = get_v7_intuition_kwargs()
                model = RecurrentPPO.load(checkpoint_path, policy_kwargs=intuition_kwargs, device=device)
                print("‚úÖ Carregamento com policy_kwargs bem-sucedido")
            except Exception as e2:
                print(f"‚ö†Ô∏è Carregamento com policy_kwargs falhou: {e2}")
                try:
                    # √öLTIMA TENTATIVA: Usar torch.load e carregar manualmente
                    import zipfile
                    import tempfile
                    
                    with tempfile.TemporaryDirectory() as temp_dir:
                        # Extrair ZIP
                        with zipfile.ZipFile(checkpoint_path, 'r') as zip_ref:
                            zip_ref.extractall(temp_dir)
                        
                        # Carregar policy.pth diretamente
                        import glob
                        policy_files = glob.glob(f"{temp_dir}/**/policy.pth", recursive=True)
                        if not policy_files:
                            raise FileNotFoundError("policy.pth n√£o encontrado no ZIP")
                        
                        policy_state = torch.load(policy_files[0], map_location=device)
                        
                        # Criar modelo novo da arquitetura V7Intuition
                        from trading_framework.policies.two_head_v7_intuition import get_v7_intuition_kwargs
                        intuition_kwargs = get_v7_intuition_kwargs()
                        
                        # Criar ambiente tempor√°rio para carregamento (8D ACTION SPACE √© padr√£o)
                        temp_env = TradingEnv(
                            df.head(100), 
                            window_size=20, 
                            is_training=False,
                            initial_balance=INITIAL_PORTFOLIO,
                            trading_params=trading_params
                        )
                        
                        # Usar m√©todo de carregamento do stable-baselines3
                        model = RecurrentPPO("MlpLstmPolicy", temp_env, policy_kwargs=intuition_kwargs, device=device)
                        
                        # Carregar pesos compat√≠veis ignorando incompat√≠veis
                        current_state = model.policy.state_dict()
                        compatible_state = {}
                        
                        for key, value in policy_state.items():
                            if key in current_state and current_state[key].shape == value.shape:
                                compatible_state[key] = value
                                print(f"‚úÖ Carregado: {key}")
                            else:
                                print(f"‚ö†Ô∏è Ignorado: {key}")
                        
                        model.policy.load_state_dict(compatible_state, strict=False)
                        print(f"‚úÖ Carregamento FOR√áA BRUTA bem-sucedido - {len(compatible_state)} par√¢metros carregados")
                
                except Exception as e3:
                    print(f"‚ùå Todos os m√©todos falharam: {e3}")
                    raise e3
        model.policy.set_training_mode(False)  # üî• MODO INFER√äNCIA
        
        print(f"‚úÖ Modelo carregado em {model.device}")
        
        # EXECUTAR M√öLTIPLOS EPIS√ìDIOS
        print(f"üöÄ Iniciando {NUM_EPISODES} epis√≥dios de trading ({TEST_STEPS} steps cada)...")
        
        # Resultados consolidados
        all_episodes = []
        total_returns = []
        
        for episode_num in range(NUM_EPISODES):
            print(f"\nüéÆ EPIS√ìDIO {episode_num + 1}/{NUM_EPISODES}")
            print("=" * 50)
            
            # üî• USAR DADOS MAIS RECENTES - trabalhar de tr√°s para frente
            # Come√ßar do final do dataset e ir para tr√°s
            buffer_size = TEST_STEPS + 100
            start_from_end = (episode_num + 1) * buffer_size
            start_idx = total_len - start_from_end
            
            # Garantir que n√£o vai antes do in√≠cio
            if start_idx < 0:
                start_idx = 0
                print(f"‚ö†Ô∏è Ajustando in√≠cio para {start_idx} (in√≠cio do dataset)")
            else:
                print(f"üî• Usando dados recentes: posi√ß√£o {start_idx} (√∫ltimos {start_from_end} registros)")
            
            end_idx = start_idx + TEST_STEPS + 100  # +100 buffer para janela
            episode_df = df.iloc[start_idx:end_idx].copy()
            
            print(f"üìä Dataset epis√≥dio: {len(episode_df):,} barras")
            print(f"üìÖ Per√≠odo: {episode_df.index.min()} at√© {episode_df.index.max()}")
            
            # Criar ambiente espec√≠fico para este epis√≥dio (8D ACTION SPACE √© padr√£o)
            env = TradingEnv(
                episode_df,
                window_size=20,
                is_training=False,  # üî• MODO AVALIA√á√ÉO
                initial_balance=INITIAL_PORTFOLIO,
                trading_params=trading_params
            )
            
            # üîç DEBUG: Verificar action space
            print(f"üîç Action Space: {env.action_space}")
            print(f"üîç Action Shape: {env.action_space.shape}")
            if hasattr(env.action_space, 'low'):
                print(f"üîç Action Low: {env.action_space.low}")
                print(f"üîç Action High: {env.action_space.high}")
            
            # Executar epis√≥dio
            obs = env.reset()
            lstm_states = None
            done = False
            step = 0
            
            # Vari√°veis de tracking do epis√≥dio
            portfolio_history = [INITIAL_PORTFOLIO]
            trades_log = []
            actions_log = []
            
            # üö® SISTEMA DE COOLDOWN - Mesmo do daytrader.py
            cooldown_counter = 0
            COOLDOWN_STEPS = 15  # Mesmo valor do daytrader.py
            
            # üö® CORRE√á√ÉO: Track trades diretamente do environment
            initial_trades_count = len(getattr(env, 'trades', []))
            
            while not done and step < TEST_STEPS:
                # PREDI√á√ÉO EM MODO INFER√äNCIA (n√£o-determin√≠stico)
                action, lstm_states = model.predict(obs, state=lstm_states, deterministic=False)
                
                # üö® DEBUG: Log da decis√£o antes do step (8D ACTION SPACE)
                if len(action) >= 2:
                    # üîß USAR MESMO MAPEAMENTO DO DAYTRADER.PY
                    raw_decision = float(action[0])
                    ACTION_THRESHOLD_LONG = 0.33   # Mesmo valor do daytrader.py
                    ACTION_THRESHOLD_SHORT = 0.67  # Mesmo valor do daytrader.py
                    
                    if raw_decision < ACTION_THRESHOLD_LONG:
                        entry_decision = 0  # HOLD
                    elif raw_decision < ACTION_THRESHOLD_SHORT:
                        entry_decision = 1  # LONG
                    else:
                        entry_decision = 2  # SHORT
                        
                    entry_confidence = float(action[1])  # FUS√ÉO quality+risk
                    
                    # üéØ APLICAR MESMO FILTRO DO DAYTRADER.PY
                    MIN_CONFIDENCE_THRESHOLD = 0.8  # üö® ANTI-OVERTRADING: Mesmo valor do daytrader.py
                    
                    # üö® APLICAR COOLDOWN - Mesmo sistema do daytrader.py
                    if cooldown_counter > 0:
                        entry_decision = 0  # FOR√áA HOLD durante cooldown
                        cooldown_counter -= 1
                        if step % 1000 == 0:  # Log a cada 1000 steps como daytrader.py
                            print(f"    [COOLDOWN] For√ßando HOLD - {cooldown_counter} steps restantes")
                    
                    # üö® FILTRO DE CONFIAN√áA - EXATAMENTE COMO DAYTRADER.PY
                    if entry_decision > 0 and entry_confidence < MIN_CONFIDENCE_THRESHOLD:
                        entry_decision = 0  # REJEITAR entrada
                        if step % 1000 == 0:  # Log a cada 1000 steps como daytrader.py
                            print(f"    [CONFIDENCE FILTER] Entry rejected: confidence={entry_confidence:.2f} < {MIN_CONFIDENCE_THRESHOLD}")
                    elif entry_decision > 0 and entry_confidence >= MIN_CONFIDENCE_THRESHOLD:
                        if step % 1000 == 0:  # Log de entradas aprovadas
                            print(f"    [ENTRY APPROVED] Decision={entry_decision}, Confidence={entry_confidence:.2f} >= {MIN_CONFIDENCE_THRESHOLD}")
                
                # Executar a√ß√£o no ambiente (que deve aplicar a mesma l√≥gica internamente)
                obs, reward, done, info = env.step(action)
                
                # Log da a√ß√£o (8D action space OTIMIZADO)
                actions_log.append({
                    'step': step,
                    'entry_decision': entry_decision,
                    'entry_confidence': float(action[1]),  # Fus√£o quality+risk
                    'sl_position_3': float(action[2]),
                    'tp_position_3': float(action[3]),
                    'sl_position_1': float(action[4]),
                    'tp_position_1': float(action[5]),
                    'sl_position_2': float(action[6]),
                    'tp_position_2': float(action[7]),
                    'portfolio_value': env.portfolio_value,
                    'current_price': getattr(env, 'current_price', 0)
                })
                
                # üö® DEBUG: Log mais detalhado do environment response
                if step % 500 == 0:
                    current_positions = len(getattr(env, 'positions', []))
                    print(f"    [ENV STATE] Positions: {current_positions}, Portfolio: ${env.portfolio_value:.2f}")
                    if hasattr(env, 'last_action_debug'):
                        print(f"    [LAST ACTION] {env.last_action_debug}")
                
                # üö® CORRE√á√ÉO: Log trades diretamente do environment
                current_trades_count = len(getattr(env, 'trades', []))
                if current_trades_count > len(trades_log) + initial_trades_count:
                    # Novos trades foram completados
                    new_trades = env.trades[-(current_trades_count - len(trades_log) - initial_trades_count):]
                    for trade in new_trades:
                        trade_info = {
                            'step': step,
                            'type': trade.get('type', 'unknown'),
                            'entry_price': trade.get('entry_price', 0),
                            'exit_price': trade.get('exit_price', 0),
                            'pnl': trade.get('pnl_usd', 0),
                            'lot_size': trade.get('volume', 0),
                            'duration': trade.get('duration', 0)
                        }
                        trades_log.append(trade_info)
                        print(f"  üíº Trade #{len(trades_log)}: {trade_info['type']} PnL=${trade_info['pnl']:.2f}")
                        
                        # üö® ATIVAR COOLDOWN ap√≥s fechamento de trade
                        cooldown_counter = COOLDOWN_STEPS
                        print(f"  üïê COOLDOWN ATIVADO: {COOLDOWN_STEPS} steps")
                
                # üö® DEBUG: Check for new positions opened
                current_positions = len(getattr(env, 'positions', []))
                if step > 0 and current_positions > getattr(env, 'prev_positions_count', 0):
                    print(f"  üü¢ NEW POSITION OPENED at step {step}! Total positions: {current_positions}")
                env.prev_positions_count = current_positions
                
                portfolio_history.append(env.portfolio_value)
                
                if (step + 1) % 1000 == 0:  # Reduzir frequ√™ncia de logs para m√∫ltiplos epis√≥dios
                    print(f"  Step {step+1}/{TEST_STEPS} - Portfolio: ${env.portfolio_value:.2f}")
                    print(f"    Positions: {len(env.positions)} | Realized: ${env.realized_balance:.2f} | Unrealized: ${env._get_unrealized_pnl():.2f}")
                
                step += 1
            
            # An√°lise do epis√≥dio
            final_portfolio = env.portfolio_value
            episode_return = ((final_portfolio - INITIAL_PORTFOLIO) / INITIAL_PORTFOLIO) * 100
            
            episode_result = {
                'episode': episode_num + 1,
                'start_idx': start_idx,
                'end_idx': end_idx,
                'period': f"{episode_df.index.min()} at√© {episode_df.index.max()}",
                'initial_portfolio': INITIAL_PORTFOLIO,
                'final_portfolio': final_portfolio,
                'return_pct': episode_return,
                'trades_count': len(trades_log),
                'actions_log': actions_log,
                'trades_log': trades_log,
                'portfolio_history': portfolio_history
            }
            
            all_episodes.append(episode_result)
            total_returns.append(episode_return)
            
            print(f"‚úÖ Epis√≥dio {episode_num + 1}: ${INITIAL_PORTFOLIO:.2f} ‚Üí ${final_portfolio:.2f} ({episode_return:+.2f}%)")
            print(f"   Trades executados: {len(trades_log)}")
            
            if len(trades_log) > 0:
                profitable_trades = [t for t in trades_log if t['pnl'] > 0]
                win_rate = (len(profitable_trades) / len(trades_log)) * 100
                print(f"   Win Rate: {win_rate:.1f}%")
            
            # Limpeza de mem√≥ria
            del env
            del episode_df
        
        # AN√ÅLISE CONSOLIDADA DOS M√öLTIPLOS EPIS√ìDIOS
        print("\n" + "=" * 80)
        print(f"üìä RESULTADOS CONSOLIDADOS - {NUM_EPISODES} EPIS√ìDIOS V7INTUITION")
        print("=" * 80)
        
        # Estat√≠sticas gerais
        avg_return = np.mean(total_returns)
        median_return = np.median(total_returns)
        std_return = np.std(total_returns)
        min_return = min(total_returns)
        max_return = max(total_returns)
        positive_episodes = len([r for r in total_returns if r > 0])
        
        print(f"üíµ Portfolio Inicial por epis√≥dio: ${INITIAL_PORTFOLIO:.2f}")
        print(f"üìà Retorno M√©dio: {avg_return:+.2f}%")
        print(f"üìä Retorno Mediano: {median_return:+.2f}%")
        print(f"üìà Melhor Epis√≥dio: {max_return:+.2f}%")
        print(f"üìâ Pior Epis√≥dio: {min_return:+.2f}%")
        print(f"üìä Desvio Padr√£o: {std_return:.2f}%")
        print(f"üéØ Epis√≥dios Lucrativos: {positive_episodes}/{NUM_EPISODES} ({(positive_episodes/NUM_EPISODES)*100:.1f}%)")
        
        # Detalhes por epis√≥dio
        print(f"\nüìã DETALHES POR EPIS√ìDIO:")
        for i, episode in enumerate(all_episodes):
            ep_return = episode['return_pct']
            grade_emoji = "üü¢" if ep_return > 5 else "üü°" if ep_return > 0 else "üî¥" 
            print(f"   {grade_emoji} Epis√≥dio {i+1}: ${episode['initial_portfolio']:.2f} ‚Üí ${episode['final_portfolio']:.2f} ({ep_return:+.2f}%) - {episode['trades_count']} trades")
        
        # An√°lise consolidada de trades
        all_trades = []
        for episode in all_episodes:
            all_trades.extend(episode['trades_log'])
        
        # Usar o √∫ltimo epis√≥dio para an√°lise de a√ß√µes (representativo)
        last_episode = all_episodes[-1] if all_episodes else {'actions_log': []}
        actions_log = last_episode['actions_log']
        
        # An√°lise consolidada de trades
        if all_trades:
            total_trades = len(all_trades)
            profitable_trades = [t for t in all_trades if t['pnl'] > 0]
            losing_trades = [t for t in all_trades if t['pnl'] < 0]
            
            win_rate = (len(profitable_trades) / total_trades) * 100 if total_trades > 0 else 0
            avg_profit = np.mean([t['pnl'] for t in profitable_trades]) if profitable_trades else 0
            avg_loss = np.mean([t['pnl'] for t in losing_trades]) if losing_trades else 0
            total_pnl = sum(t['pnl'] for t in all_trades)
            
            print(f"\nüìä AN√ÅLISE CONSOLIDADA DE TRADES:")
            print(f"üìä Total de Trades (todos epis√≥dios): {total_trades}")
            print(f"üéØ Win Rate Global: {win_rate:.1f}%")
            print(f"üíö Trades Lucrativos: {len(profitable_trades)}")
            print(f"‚ùå Trades Perdedores: {len(losing_trades)}")
            print(f"üí∞ Lucro M√©dio por Trade: ${avg_profit:.2f}")
            print(f"üìâ Perda M√©dia por Trade: ${avg_loss:.2f}")
            print(f"üíµ PnL Total (todos epis√≥dios): ${total_pnl:.2f}")
            
            # Profit Factor
            if avg_loss != 0 and losing_trades:
                gross_profit = sum(t['pnl'] for t in profitable_trades)
                gross_loss = abs(sum(t['pnl'] for t in losing_trades))
                profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')
                print(f"‚öñÔ∏è Profit Factor Global: {profit_factor:.2f}")
            
            # Frequ√™ncia de trading
            total_steps = NUM_EPISODES * TEST_STEPS
            trading_frequency = (total_trades / total_steps) * 100
            print(f"üìà Frequ√™ncia de Trading: {trading_frequency:.2f}% dos steps")
            print(f"üìà Trades por Epis√≥dio: {total_trades/NUM_EPISODES:.1f}")
            
        else:
            print(f"\n‚ö†Ô∏è NENHUM TRADE EXECUTADO EM {NUM_EPISODES} EPIS√ìDIOS")
            print("üîç Modelo extremamente conservador em todos os per√≠odos")
        
        # An√°lise de a√ß√µes - V7Intuition (8D OTIMIZADO - todas a√ß√µes s√£o √∫teis)
        if actions_log:
            entry_decisions = [a['entry_decision'] for a in actions_log]
            entry_confidences = [a['entry_confidence'] for a in actions_log]
            sl_pos3s = [a['sl_position_3'] for a in actions_log]
            tp_pos3s = [a['tp_position_3'] for a in actions_log]
            sl_pos1s = [a['sl_position_1'] for a in actions_log]
            tp_pos1s = [a['tp_position_1'] for a in actions_log]
            sl_pos2s = [a['sl_position_2'] for a in actions_log]
            tp_pos2s = [a['tp_position_2'] for a in actions_log]
            
            hold_pct = (sum(1 for d in entry_decisions if d == 0) / len(entry_decisions)) * 100
            long_pct = (sum(1 for d in entry_decisions if d == 1) / len(entry_decisions)) * 100
            short_pct = (sum(1 for d in entry_decisions if d == 2) / len(entry_decisions)) * 100
            
            avg_confidence = np.mean(entry_confidences)
            avg_sl_pos3 = np.mean(sl_pos3s)
            avg_tp_pos3 = np.mean(tp_pos3s)
            avg_sl_pos1 = np.mean(sl_pos1s)
            avg_tp_pos1 = np.mean(tp_pos1s)
            avg_sl_pos2 = np.mean(sl_pos2s)
            avg_tp_pos2 = np.mean(tp_pos2s)
            
            print(f"\nüéÆ AN√ÅLISE DAS A√á√ïES V7INTUITION (8D OTIMIZADO):")
            print(f"   üìä ENTRY DECISIONS:")
            print(f"   ‚ö™ HOLD: {hold_pct:.1f}%")
            print(f"   üü¢ LONG: {long_pct:.1f}%") 
            print(f"   üî¥ SHORT: {short_pct:.1f}%")
            print(f"   ‚≠ê Entry Confidence M√©dia: {avg_confidence:.3f} (fus√£o quality+risk)")
            print(f"   üìä SISTEMA SL/TP POR POSI√á√ÉO (action space order):")
            print(f"   üéØ [2,3] Posi√ß√£o 3: SL {avg_sl_pos3:+.2f} | TP {avg_tp_pos3:+.2f}")
            print(f"   üéØ [4,5] Posi√ß√£o 1: SL {avg_sl_pos1:+.2f} | TP {avg_tp_pos1:+.2f}")
            print(f"   üéØ [6,7] Posi√ß√£o 2: SL {avg_sl_pos2:+.2f} | TP {avg_tp_pos2:+.2f}")
        
        # Drawdown analysis (do √∫ltimo epis√≥dio como exemplo)
        last_portfolio_history = all_episodes[-1]['portfolio_history'] if all_episodes else []
        if len(last_portfolio_history) > 1:
            portfolio_array = np.array(last_portfolio_history)
            running_max = np.maximum.accumulate(portfolio_array)
            drawdown = (portfolio_array - running_max) / running_max * 100
            max_drawdown = np.min(drawdown)
            
            print(f"\nüìâ Max Drawdown (√∫ltimo epis√≥dio): {max_drawdown:.2f}%")
        
        # Avalia√ß√£o final consolidada
        print(f"\nüéñÔ∏è AVALIA√á√ÉO FINAL CONSOLIDADA ({NUM_EPISODES} EPIS√ìDIOS):")
        if avg_return > 5:
            grade = "üü¢ EXCELENTE"
        elif avg_return > 2:
            grade = "üü° BOM"
        elif avg_return > -2:
            grade = "üü† REGULAR"
        else:
            grade = "üî¥ RUIM"
        
        # Classifica√ß√£o por consist√™ncia
        consistency = (positive_episodes / NUM_EPISODES) * 100
        if consistency >= 80:
            consistency_grade = "üî• MUITO CONSISTENTE"
        elif consistency >= 60:
            consistency_grade = "‚úÖ CONSISTENTE"
        elif consistency >= 40:
            consistency_grade = "‚ö†Ô∏è MODERADAMENTE CONSISTENTE"
        else:
            consistency_grade = "‚ùå INCONSISTENTE"
        
        print(f"   {grade}")
        print(f"   Retorno M√©dio: {avg_return:+.2f}% (œÉ={std_return:.2f}%)")
        print(f"   Consist√™ncia: {consistency_grade} ({positive_episodes}/{NUM_EPISODES})")
        print(f"   Melhor/Pior: {max_return:+.2f}% / {min_return:+.2f}%")
        if all_trades:
            print(f"   Total Trades: {len(all_trades)} (Win Rate: {win_rate:.1f}%)")
            print(f"   Trades/Epis√≥dio: {len(all_trades)/NUM_EPISODES:.1f}")
        
        # Sharpe Ratio aproximado (assumindo retornos di√°rios)
        if std_return > 0:
            sharpe_ratio = avg_return / std_return
            print(f"   Sharpe Ratio: {sharpe_ratio:.2f}")
        
        # Recomenda√ß√£o final
        print(f"\nüí° RECOMENDA√á√ÉO:")
        if avg_return > 10 and consistency >= 60:
            print("   üöÄ MODELO PRONTO PARA PRODU√á√ÉO!")
        elif avg_return > 5 and consistency >= 40:
            print("   ‚úÖ Modelo promissor, considere mais testes")
        elif avg_return > 0:
            print("   ‚ö†Ô∏è Modelo precisa de otimiza√ß√£o")
        else:
            print("   ‚ùå Modelo precisa de revis√£o completa")
        
        # SALVAR RELAT√ìRIO AUTOM√ÅTICO
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # Extrair steps do nome do checkpoint se poss√≠vel
        steps_from_name = "unknown"
        if checkpoint_path and "_steps_" in checkpoint_path:
            try:
                steps_match = checkpoint_path.split("_steps_")[0].split("_")[-1]
                steps_from_name = f"{int(steps_match)//1000}k"
            except:
                steps_from_name = "unknown"
        
        report_filename = f"D:/Projeto/avaliacoes/avaliacao_v7_{steps_from_name}_{timestamp}.txt"
        
        print(f"\nüíæ Salvando relat√≥rio: {report_filename}")
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(f"AVALIA√á√ÉO V7 INTUITION CHECKPOINT {steps_from_name.upper()} - {timestamp}\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"Checkpoint: {CHECKPOINT_PATH}\n")
            f.write(f"Episodes: {NUM_EPISODES}\n")
            f.write(f"Steps per episode: {TEST_STEPS}\n")
            f.write(f"Portfolio inicial: ${INITIAL_PORTFOLIO}\n\n")
            
            f.write("RESULTADOS CONSOLIDADOS:\n")
            f.write(f"Retorno M√©dio: {avg_return:+.2f}%\n")
            f.write(f"Retorno Mediano: {median_return:+.2f}%\n")
            f.write(f"Melhor Epis√≥dio: {max_return:+.2f}%\n")
            f.write(f"Pior Epis√≥dio: {min_return:+.2f}%\n")
            f.write(f"Desvio Padr√£o: {std_return:.2f}%\n")
            f.write(f"Epis√≥dios Lucrativos: {positive_episodes}/{NUM_EPISODES} ({(positive_episodes/NUM_EPISODES)*100:.1f}%)\n\n")
            
            f.write("DETALHES POR EPIS√ìDIO:\n")
            for i, episode in enumerate(all_episodes):
                ep_return = episode['return_pct']
                grade_emoji = "üü¢" if ep_return > 5 else "üü°" if ep_return > 0 else "üî¥" 
                f.write(f"{grade_emoji} Epis√≥dio {i+1}: ${episode['initial_portfolio']:.2f} ‚Üí ${episode['final_portfolio']:.2f} ({ep_return:+.2f}%) - {episode['trades_count']} trades\n")
            
            if all_trades:
                f.write(f"\nAN√ÅLISE DE TRADES:\n")
                f.write(f"Total de Trades: {len(all_trades)}\n")
                f.write(f"Win Rate Global: {win_rate:.1f}%\n")
                f.write(f"Trades Lucrativos: {len(profitable_trades)}\n")
                f.write(f"Trades Perdedores: {len(losing_trades)}\n")
                f.write(f"Lucro M√©dio: ${avg_profit:.2f}\n")
                f.write(f"Perda M√©dia: ${avg_loss:.2f}\n")
                f.write(f"PnL Total: ${total_pnl:.2f}\n")
                
                if avg_loss != 0 and losing_trades:
                    f.write(f"Profit Factor: {profit_factor:.2f}\n")
                f.write(f"Frequ√™ncia Trading: {trading_frequency:.2f}%\n")
            
            f.write(f"\nAVALIA√á√ÉO FINAL: {grade}\n")
            f.write(f"Consist√™ncia: {consistency_grade} ({positive_episodes}/{NUM_EPISODES})\n")
        
        print(f"‚úÖ Relat√≥rio salvo!")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ERRO CR√çTICO: {e}")
        print(f"Detalhes: {traceback.format_exc()}")
        return False

if __name__ == "__main__":
    print(f"üöÄ INICIANDO TESTE V7INTUITION - {datetime.now().strftime('%H:%M:%S')}")
    
    success = test_v7_intuition_trading()
    
    if success:
        print(f"\n‚úÖ TESTE V7INTUITION CONCLU√çDO - {datetime.now().strftime('%H:%M:%S')}")
    else:
        print(f"\n‚ùå TESTE V7INTUITION FALHOU - {datetime.now().strftime('%H:%M:%S')}")