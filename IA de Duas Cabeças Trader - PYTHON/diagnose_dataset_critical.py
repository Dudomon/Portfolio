#!/usr/bin/env python3
"""
DIAGN√ìSTICO CR√çTICO DO DATASET DESAFIADOR
An√°lise minuciosa do problema de converg√™ncia
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

def critical_dataset_diagnosis():
    """Diagn√≥stico cr√≠tico para descobrir por que o modelo n√£o converge"""
    
    dataset_path = 'data/GOLD_SAFE_CHALLENGING_2M_20250801_203251.csv'
    
    print("üîç DIAGN√ìSTICO CR√çTICO DO DATASET DESAFIADOR")
    print("="*80)
    
    try:
        df = pd.read_csv(dataset_path)
        print(f"‚úÖ Dataset carregado: {len(df):,} barras")
    except FileNotFoundError:
        print(f"‚ùå Dataset n√£o encontrado: {dataset_path}")
        return
    
    # === AN√ÅLISE DETALHADA DE RETURNS ===
    df['returns'] = df['close'].pct_change()
    returns_clean = df['returns'].dropna()
    
    print(f"\nüìä AN√ÅLISE DETALHADA DE RETURNS:")
    print(f"   Mean return: {returns_clean.mean():.8f}")
    print(f"   Std return: {returns_clean.std():.6f}")
    print(f"   Min return: {returns_clean.min():.6f}")
    print(f"   Max return: {returns_clean.max():.6f}")
    print(f"   Skewness: {returns_clean.skew():.4f}")
    print(f"   Kurtosis: {returns_clean.kurtosis():.4f}")
    
    # === PROBLEMA CR√çTICO 1: ZERO RETURNS ===
    zero_returns = np.sum(np.abs(returns_clean) < 1e-8)
    tiny_returns = np.sum(np.abs(returns_clean) < 1e-6)
    small_returns = np.sum(np.abs(returns_clean) < 1e-4)
    
    print(f"\n‚ö†Ô∏è  AN√ÅLISE DE RETURNS EXTREMAMENTE PEQUENOS:")
    print(f"   Zero returns (< 1e-8): {zero_returns:,} ({zero_returns/len(returns_clean)*100:.2f}%)")
    print(f"   Tiny returns (< 1e-6): {tiny_returns:,} ({tiny_returns/len(returns_clean)*100:.2f}%)")
    print(f"   Small returns (< 1e-4): {small_returns:,} ({small_returns/len(returns_clean)*100:.2f}%)")
    
    if zero_returns > len(returns_clean) * 0.01:  # Mais de 1%
        print("   üö® PROBLEMA CR√çTICO: Muitos returns zero! Modelo n√£o aprende!")
    
    # === PROBLEMA CR√çTICO 2: OHLC SPREAD AN√ÅLISE ===
    df['ohlc_spread'] = (df['high'] - df['low']) / df['close']
    df['oc_spread'] = np.abs(df['close'] - df['open']) / df['open']
    
    zero_ohlc_spread = np.sum(df['ohlc_spread'] < 1e-6)
    zero_oc_spread = np.sum(df['oc_spread'] < 1e-6)
    
    print(f"\nüìè AN√ÅLISE DE SPREADS OHLC:")
    print(f"   OHLC spread m√©dio: {df['ohlc_spread'].mean():.6f}")
    print(f"   OC spread m√©dio: {df['oc_spread'].mean():.6f}")
    print(f"   Zero OHLC spreads: {zero_ohlc_spread:,} ({zero_ohlc_spread/len(df)*100:.2f}%)")
    print(f"   Zero OC spreads: {zero_oc_spread:,} ({zero_oc_spread/len(df)*100:.2f}%)")
    
    if zero_ohlc_spread > len(df) * 0.001:  # Mais de 0.1%
        print("   üö® PROBLEMA CR√çTICO: Barras com spread zero! Sem movimento!")
    
    # === PROBLEMA CR√çTICO 3: REGIME TRANSITIONS ===
    if 'regime' in df.columns:
        regime_changes = np.sum(df['regime'] != df['regime'].shift(1))
        avg_regime_duration = len(df) / regime_changes if regime_changes > 0 else len(df)
        
        print(f"\nüîÑ AN√ÅLISE DE REGIMES:")
        print(f"   Mudan√ßas de regime: {regime_changes:,}")
        print(f"   Dura√ß√£o m√©dia de regime: {avg_regime_duration:.1f} barras")
        
        # An√°lise de performance por regime
        regime_stats = df.groupby('regime')['returns'].agg(['mean', 'std', 'count'])
        print(f"   Performance por regime:")
        for regime in regime_stats.index:
            mean_ret = regime_stats.loc[regime, 'mean']
            std_ret = regime_stats.loc[regime, 'std']
            count = regime_stats.loc[regime, 'count']
            print(f"     {regime}: mean={mean_ret:.8f}, std={std_ret:.6f}, count={count:,}")
        
        # PROBLEMA: Regimes com performance muito similar
        regime_means = regime_stats['mean'].values
        if np.std(regime_means) < 1e-6:
            print("   üö® PROBLEMA CR√çTICO: Regimes t√™m performance id√™ntica! Sem sinal!")
    
    # === PROBLEMA CR√çTICO 4: AUTOCORRELA√á√ÉO TEMPORAL ===
    autocorrs = [returns_clean.autocorr(lag=i) for i in range(1, 11)]
    max_autocorr = max(np.abs(autocorrs))
    
    print(f"\nüìà AN√ÅLISE DE AUTOCORRELA√á√ÉO:")
    print(f"   Autocorr lag-1: {autocorrs[0]:.6f}")
    print(f"   Max autocorr (lags 1-10): {max_autocorr:.6f}")
    
    if max_autocorr < 0.01:
        print("   üö® PROBLEMA CR√çTICO: Zero autocorrela√ß√£o! Puramente aleat√≥rio!")
    
    # === PROBLEMA CR√çTICO 5: VOLUME CORRELATION ===
    if 'volume' in df.columns:
        volume_price_corr = np.corrcoef(df['volume'], np.abs(df['returns'].fillna(0)))[0,1]
        volume_volatility_corr = np.corrcoef(df['volume'], df['ohlc_spread'])[0,1]
        
        print(f"\nüìä AN√ÅLISE DE VOLUME:")
        print(f"   Volume-Return correlation: {volume_price_corr:.6f}")
        print(f"   Volume-Volatility correlation: {volume_volatility_corr:.6f}")
        
        if abs(volume_price_corr) < 0.05 and abs(volume_volatility_corr) < 0.05:
            print("   üö® PROBLEMA CR√çTICO: Volume n√£o correlaciona com pre√ßo/volatilidade!")
    
    # === DIAGN√ìSTICO DE NORMALIZA√á√ÉO ===
    print(f"\nüîß AN√ÅLISE DE NORMALIZA√á√ÉO:")
    
    # Simular normaliza√ß√£o que o modelo faria
    price_values = df[['open', 'high', 'low', 'close']].values
    price_normalized = (price_values - price_values.mean()) / price_values.std()
    
    extreme_values = np.sum(np.abs(price_normalized) > 3)
    zero_values = np.sum(np.abs(price_normalized) < 1e-6)
    
    print(f"   Valores extremos (|z| > 3): {extreme_values:,}")
    print(f"   Valores pr√≥ximos de zero: {zero_values:,}")
    
    # === TESTE DE PREDIBILIDADE ===
    print(f"\nüéØ TESTE DE PREDIBILIDADE:")
    
    # Correla√ß√£o com pr√≥ximo return
    future_returns = df['returns'].shift(-1)
    current_features = df[['returns', 'ohlc_spread', 'volume']].fillna(0)
    
    correlations = {}
    for col in current_features.columns:
        if col in df.columns:
            corr = np.corrcoef(current_features[col], future_returns.fillna(0))[0,1]
            correlations[col] = corr
            print(f"   {col} -> future_return: {corr:.6f}")
    
    max_pred_corr = max(np.abs(list(correlations.values())))
    if max_pred_corr < 0.01:
        print("   üö® PROBLEMA CR√çTICO: Dataset n√£o tem predibilidade!")
    
    # === SUMMARY DO DIAGN√ìSTICO ===
    print(f"\n{'='*80}")
    print("üîç RESUMO DO DIAGN√ìSTICO:")
    print("="*80)
    
    problems = []
    
    if zero_returns > len(returns_clean) * 0.01:
        problems.append("MUITOS RETURNS ZERO")
    
    if zero_ohlc_spread > len(df) * 0.001:
        problems.append("BARRAS SEM MOVIMENTO")
    
    if 'regime' in df.columns and np.std(regime_stats['mean'].values) < 1e-6:
        problems.append("REGIMES ID√äNTICOS")
    
    if max_autocorr < 0.01:
        problems.append("ZERO AUTOCORRELA√á√ÉO")
    
    if max_pred_corr < 0.01:
        problems.append("ZERO PREDIBILIDADE")
    
    if len(problems) == 0:
        print("‚úÖ Dataset tecnicamente correto")
        print("üí° Problema pode ser:")
        print("   - Hiperpar√¢metros muito conservadores")
        print("   - Learning rate muito baixo")
        print("   - Clipping muito agressivo")
        print("   - Normaliza√ß√£o muito forte")
    else:
        print("üö® PROBLEMAS CR√çTICOS ENCONTRADOS:")
        for i, problem in enumerate(problems, 1):
            print(f"   {i}. {problem}")
        
        print(f"\nüí° SOLU√á√ïES RECOMENDADAS:")
        if "MUITOS RETURNS ZERO" in problems:
            print("   - Aumentar volatilidade m√≠nima no dataset")
            print("   - Evitar arredondamentos excessivos")
        
        if "BARRAS SEM MOVIMENTO" in problems:
            print("   - Garantir spread m√≠nimo em todas as barras")
            print("   - Revisar l√≥gica de gera√ß√£o OHLC")
        
        if "REGIMES ID√äNTICOS" in problems:
            print("   - Aumentar diferen√ßa entre regimes")
            print("   - Revisar par√¢metros de drift por regime")
        
        if "ZERO AUTOCORRELA√á√ÉO" in problems:
            print("   - Adicionar componente de momentum")
            print("   - Introduzir persistence nos returns")
        
        if "ZERO PREDIBILIDADE" in problems:
            print("   - Adicionar features preditivas")
            print("   - Introduzir padr√µes identific√°veis")
    
    # === CRIAR DATASET CORRIGIDO ===
    if len(problems) > 0:
        print(f"\nüîß GERANDO DATASET CORRIGIDO...")
        create_corrected_dataset(df, problems)

def create_corrected_dataset(original_df, problems):
    """Criar vers√£o corrigida do dataset baseado nos problemas encontrados"""
    
    print("üîß CRIANDO DATASET CORRIGIDO...")
    
    df = original_df.copy()
    
    # Corre√ß√£o 1: Garantir movimento m√≠nimo
    if "BARRAS SEM MOVIMENTO" in problems:
        min_spread = 0.0001  # 0.01% m√≠nimo
        for idx in range(len(df)):
            current_spread = (df.iloc[idx]['high'] - df.iloc[idx]['low']) / df.iloc[idx]['close']
            if current_spread < min_spread:
                mid_price = (df.iloc[idx]['high'] + df.iloc[idx]['low']) / 2
                df.iloc[idx, df.columns.get_loc('high')] = mid_price * (1 + min_spread/2)
                df.iloc[idx, df.columns.get_loc('low')] = mid_price * (1 - min_spread/2)
        print("   ‚úÖ Spread m√≠nimo aplicado")
    
    # Corre√ß√£o 2: Ajustar regimes para ter diferen√ßas claras
    if "REGIMES ID√äNTICOS" in problems and 'regime' in df.columns:
        # Aplicar drifts mais distintivos por regime
        regime_adjustments = {
            'bull': 0.0002,    # +0.02% drift
            'bear': -0.0002,   # -0.02% drift  
            'sideways': 0.0    # zero drift
        }
        
        for regime, drift in regime_adjustments.items():
            mask = df['regime'] == regime
            if mask.any():
                # Aplicar drift cumulativo
                regime_indices = df[mask].index
                cumulative_drift = np.cumsum([drift] * len(regime_indices))
                df.loc[regime_indices, 'close'] *= (1 + cumulative_drift)
                # Ajustar OHLC proporcionalmente
                df.loc[regime_indices, 'open'] *= (1 + cumulative_drift)
                df.loc[regime_indices, 'high'] *= (1 + cumulative_drift)
                df.loc[regime_indices, 'low'] *= (1 + cumulative_drift)
        
        print("   ‚úÖ Regimes diferenciados aplicados")
    
    # Corre√ß√£o 3: Adicionar autocorrela√ß√£o
    if "ZERO AUTOCORRELA√á√ÉO" in problems:
        # Aplicar suaviza√ß√£o para criar autocorrela√ß√£o
        df['returns_raw'] = df['close'].pct_change()
        returns_smoothed = df['returns_raw'].rolling(window=3, center=True).mean().fillna(df['returns_raw'])
        
        # Reconstruir pre√ßos com returns suavizados
        df['close_corrected'] = df['close'].iloc[0] * (1 + returns_smoothed).cumprod()
        df['close'] = df['close_corrected']
        
        print("   ‚úÖ Autocorrela√ß√£o adicionada")
    
    # Salvar dataset corrigido
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    corrected_filename = f"data/GOLD_CORRECTED_2M_{timestamp}.csv"
    
    # Remover colunas auxiliares
    cols_to_remove = ['returns_raw', 'close_corrected']
    df = df.drop(columns=[col for col in cols_to_remove if col in df.columns])
    
    df.to_csv(corrected_filename, index=False)
    print(f"   ‚úÖ Dataset corrigido salvo: {corrected_filename}")
    
    return corrected_filename

if __name__ == '__main__':
    critical_dataset_diagnosis()