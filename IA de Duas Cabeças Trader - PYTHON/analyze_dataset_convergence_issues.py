#!/usr/bin/env python3
"""
An√°lise completa do dataset GOLD_TRADING_READY_2M para identificar problemas
que possam impedir converg√™ncia de RL.
"""

import pandas as pd
import numpy as np
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

def load_and_examine_dataset(filepath):
    """Carrega e examina estrutura b√°sica do dataset"""
    print("="*80)
    print("1. ESTRUTURA B√ÅSICA DO DATASET")
    print("="*80)
    
    # Carregar dataset
    df = pd.read_csv(filepath)
    
    print(f"Tamanho do dataset: {df.shape[0]:,} linhas x {df.shape[1]} colunas")
    print(f"Per√≠odo de tempo: {df['timestamp'].iloc[0]} at√© {df['timestamp'].iloc[-1]}")
    
    # Converter timestamp
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df.set_index('timestamp', inplace=True)
    
    print(f"\nColunas dispon√≠veis:")
    for i, col in enumerate(df.columns):
        print(f"  {i+1}. {col} ({df[col].dtype})")
    
    # Estat√≠sticas b√°sicas de tamanho
    print(f"\nMem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    return df

def calculate_descriptive_stats(df):
    """Calcula estat√≠sticas descritivas dos returns e features principais"""
    print("\n" + "="*80)
    print("2. ESTAT√çSTICAS DESCRITIVAS")
    print("="*80)
    
    # Calcular returns
    df['returns'] = df['close'].pct_change()
    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
    
    # Estat√≠sticas b√°sicas dos pre√ßos
    price_cols = ['open', 'high', 'low', 'close', 'volume']
    print("\nEstat√≠sticas dos Pre√ßos:")
    print(df[price_cols].describe())
    
    # Estat√≠sticas dos returns
    print("\nEstat√≠sticas dos Returns:")
    returns_stats = df[['returns', 'log_returns']].describe()
    print(returns_stats)
    
    # Estat√≠sticas avan√ßadas dos returns
    returns = df['returns'].dropna()
    log_returns = df['log_returns'].dropna()
    
    print(f"\nEstat√≠sticas Avan√ßadas dos Returns:")
    print(f"  Assimetria (Skewness): {stats.skew(returns):.6f}")
    print(f"  Curtose (Kurtosis): {stats.kurtosis(returns):.6f}")
    print(f"  Teste Jarque-Bera: {stats.jarque_bera(returns)}")
    
    # Volatilidade
    print(f"\nVolatilidade:")
    print(f"  Returns std: {returns.std():.6f}")
    print(f"  Log returns std: {log_returns.std():.6f}")
    print(f"  Volatilidade anualizada: {returns.std() * np.sqrt(252*288):.6f}")  # 288 per√≠odos de 5min por dia
    
    return df

def analyze_returns_distribution(df):
    """Analisa distribui√ß√£o dos returns, outliers e anomalias"""
    print("\n" + "="*80)
    print("3. AN√ÅLISE DA DISTRIBUI√á√ÉO DOS RETURNS")
    print("="*80)
    
    returns = df['returns'].dropna()
    
    # Outliers usando IQR
    Q1 = returns.quantile(0.25)
    Q3 = returns.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = returns[(returns < lower_bound) | (returns > upper_bound)]
    
    print(f"Outliers identificados:")
    print(f"  Total de outliers: {len(outliers):,} ({len(outliers)/len(returns)*100:.2f}%)")
    print(f"  Range normal: [{lower_bound:.6f}, {upper_bound:.6f}]")
    print(f"  Outliers extremos: min={outliers.min():.6f}, max={outliers.max():.6f}")
    
    # Percentis extremos
    percentiles = [0.1, 1, 5, 95, 99, 99.9]
    print(f"\nPercentis dos returns:")
    for p in percentiles:
        print(f"  {p:4.1f}%: {returns.quantile(p/100):.6f}")
    
    # Teste de normalidade
    shapiro_stat, shapiro_p = stats.shapiro(returns[:5000])  # Shapiro limitado a 5000 amostras
    ks_stat, ks_p = stats.kstest(returns, 'norm', args=(returns.mean(), returns.std()))
    
    print(f"\nTestes de Normalidade:")
    print(f"  Shapiro-Wilk: stat={shapiro_stat:.6f}, p-value={shapiro_p:.2e}")
    print(f"  Kolmogorov-Smirnov: stat={ks_stat:.6f}, p-value={ks_p:.2e}")
    
    # An√°lise de zeros
    zero_returns = (returns == 0).sum()
    print(f"\nReturns zerados: {zero_returns:,} ({zero_returns/len(returns)*100:.2f}%)")
    
    return returns

def analyze_autocorrelation(df):
    """Analisa autocorrela√ß√£o dos returns"""
    print("\n" + "="*80)
    print("4. AN√ÅLISE DE AUTOCORRELA√á√ÉO")
    print("="*80)
    
    returns = df['returns'].dropna()
    
    # Autocorrela√ß√£o em diferentes lags
    lags = [1, 5, 10, 20, 50, 100, 288]  # 288 = 1 dia em per√≠odos de 5min
    print("Autocorrela√ß√£o dos returns:")
    for lag in lags:
        if lag < len(returns):
            autocorr = returns.autocorr(lag=lag)
            print(f"  Lag {lag:3d}: {autocorr:.6f}")
    
    # An√°lise manual de autocorrela√ß√£o significativa
    print(f"\nAn√°lise de autocorrela√ß√£o significativa:")
    significant_lags = []
    for lag in lags:
        if lag < len(returns):
            autocorr = returns.autocorr(lag=lag)
            if abs(autocorr) > 0.05:  # Threshold para signific√¢ncia
                significant_lags.append((lag, autocorr))
    
    if significant_lags:
        print("  Lags com autocorrela√ß√£o significativa (>0.05):")
        for lag, autocorr in significant_lags:
            print(f"    Lag {lag}: {autocorr:.6f}")
    else:
        print("  Nenhuma autocorrela√ß√£o significativa detectada")
    
    # Autocorrela√ß√£o dos returns ao quadrado (heterocedasticidade)
    returns_sq = returns ** 2
    print(f"\nAutocorrela√ß√£o dos returns ao quadrado (volatilidade clustering):")
    for lag in lags[:5]:  # Apenas primeiros lags
        if lag < len(returns_sq):
            autocorr_sq = returns_sq.autocorr(lag=lag)
            print(f"  Lag {lag:3d}: {autocorr_sq:.6f}")

def test_stationarity(df):
    """Testa estacionariedade dos dados usando m√©todos simples"""
    print("\n" + "="*80)
    print("5. AN√ÅLISE DE ESTACIONARIEDADE")
    print("="*80)
    
    price_series = df['close'].dropna()
    returns_series = df['returns'].dropna()
    
    # An√°lise visual de estacionariedade - pre√ßos
    print("An√°lise PRE√áOS:")
    rolling_mean = price_series.rolling(window=1000).mean()
    rolling_std = price_series.rolling(window=1000).std()
    
    mean_variation = (rolling_mean.max() - rolling_mean.min()) / price_series.mean() * 100
    std_variation = (rolling_std.max() - rolling_std.min()) / rolling_std.mean() * 100
    
    print(f"  Varia√ß√£o da m√©dia m√≥vel: {mean_variation:.2f}%")
    print(f"  Varia√ß√£o do desvio padr√£o m√≥vel: {std_variation:.2f}%")
    print(f"  Resultado: {'N√£o-estacion√°rio (esperado)' if mean_variation > 10 else 'Possivelmente estacion√°rio'}")
    
    # An√°lise visual de estacionariedade - returns
    print(f"\nAn√°lise RETURNS:")
    returns_rolling_mean = returns_series.rolling(window=1000).mean()
    returns_rolling_std = returns_series.rolling(window=1000).std()
    
    returns_mean_variation = abs(returns_rolling_mean.max() - returns_rolling_mean.min()) / abs(returns_series.mean()) * 100 if returns_series.mean() != 0 else 0
    returns_std_variation = (returns_rolling_std.max() - returns_rolling_std.min()) / returns_rolling_std.mean() * 100
    
    print(f"  Varia√ß√£o da m√©dia m√≥vel: {returns_mean_variation:.2f}%")
    print(f"  Varia√ß√£o do desvio padr√£o m√≥vel: {returns_std_variation:.2f}%")
    print(f"  M√©dia dos returns: {returns_series.mean():.6f}")
    
    if abs(returns_series.mean()) < 1e-4 and returns_std_variation < 50:
        print(f"  Resultado: Provavelmente estacion√°rio")
    else:
        print(f"  Resultado: Possivelmente n√£o-estacion√°rio")

def analyze_technical_indicators(df):
    """Analisa qualidade dos indicadores t√©cnicos (se existirem)"""
    print("\n" + "="*80)
    print("6. AN√ÅLISE DE INDICADORES T√âCNICOS")
    print("="*80)
    
    # Verificar se existem colunas de indicadores t√©cnicos
    possible_indicators = ['sma', 'ema', 'rsi', 'macd', 'bb_upper', 'bb_lower', 'atr', 'momentum']
    existing_indicators = [col for col in df.columns if any(ind in col.lower() for ind in possible_indicators)]
    
    print(f"Colunas dispon√≠veis: {list(df.columns)}")
    print(f"Indicadores t√©cnicos encontrados: {existing_indicators}")
    
    if not existing_indicators:
        print("PROBLEMA: Nenhum indicador t√©cnico encontrado no dataset!")
        print("Isso pode ser um problema cr√≠tico para RL - agente precisa de features t√©cnicas")
        
        # Criar alguns indicadores b√°sicos para an√°lise
        print("\nCriando indicadores b√°sicos para an√°lise:")
        
        # SMA
        df['sma_20'] = df['close'].rolling(window=20).mean()
        df['sma_50'] = df['close'].rolling(window=50).mean()
        
        # RSI simplificado
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # Volatilidade
        df['volatility'] = df['returns'].rolling(window=20).std()
        
        existing_indicators = ['sma_20', 'sma_50', 'rsi', 'volatility']
        print(f"Indicadores criados: {existing_indicators}")
    
    # Analisar qualidade dos indicadores
    for indicator in existing_indicators:
        if indicator in df.columns:
            ind_data = df[indicator].dropna()
            if len(ind_data) > 0:
                print(f"\n{indicator.upper()}:")
                print(f"  Missing values: {df[indicator].isna().sum():,} ({df[indicator].isna().sum()/len(df)*100:.2f}%)")
                print(f"  Range: [{ind_data.min():.4f}, {ind_data.max():.4f}]")
                print(f"  Mean ¬± Std: {ind_data.mean():.4f} ¬± {ind_data.std():.4f}")
                
                # Verificar valores extremos ou constantes
                if ind_data.std() == 0:
                    print(f"  ‚ö†Ô∏è  PROBLEMA: {indicator} √© constante!")
                elif ind_data.std() < 1e-6:
                    print(f"  ‚ö†Ô∏è  PROBLEMA: {indicator} tem varia√ß√£o muito baixa!")

def analyze_temporal_patterns(df):
    """Identifica padr√µes temporais"""
    print("\n" + "="*80)
    print("7. AN√ÅLISE DE PADR√ïES TEMPORAIS")
    print("="*80)
    
    # Adicionar componentes temporais
    df_temp = df.copy()
    df_temp.reset_index(inplace=True)
    df_temp['hour'] = df_temp['timestamp'].dt.hour
    df_temp['day_of_week'] = df_temp['timestamp'].dt.dayofweek
    df_temp['day_of_month'] = df_temp['timestamp'].dt.day
    
    # An√°lise por hora
    hourly_stats = df_temp.groupby('hour')['returns'].agg(['count', 'mean', 'std']).round(6)
    print("Estat√≠sticas por Hora do Dia:")
    print(hourly_stats)
    
    # Verificar se h√° padr√µes √≥bvios
    hourly_mean_range = hourly_stats['mean'].max() - hourly_stats['mean'].min()
    print(f"\nRange de returns m√©dios por hora: {hourly_mean_range:.6f}")
    
    # An√°lise por dia da semana
    daily_stats = df_temp.groupby('day_of_week')['returns'].agg(['count', 'mean', 'std']).round(6)
    print(f"\nEstat√≠sticas por Dia da Semana:")
    print(daily_stats)
    
    # Verificar tend√™ncias
    df_temp['period'] = df_temp.index // 10000  # Dividir em per√≠odos
    period_stats = df_temp.groupby('period')['returns'].agg(['mean', 'std']).round(6)
    print(f"\nEstat√≠sticas por Per√≠odo (cada ~10k observa√ß√µes):")
    print(period_stats.head(10))
    
    # Verificar se h√° drift nos returns
    df_temp['index_num'] = range(len(df_temp))
    trend_corr = df_temp['returns'].corr(df_temp['index_num'])
    print(f"\nCorrela√ß√£o returns vs tempo (drift): {trend_corr:.6f}")
    if abs(trend_corr) > 0.1:
        print("‚ö†Ô∏è  PROBLEMA: Poss√≠vel trend/drift nos returns!")

def check_data_quality(df):
    """Verifica missing values e dados corrompidos"""
    print("\n" + "="*80)
    print("8. VERIFICA√á√ÉO DE QUALIDADE DOS DADOS")
    print("="*80)
    
    # Missing values
    print("Missing Values por coluna:")
    missing_stats = df.isnull().sum()
    for col, missing_count in missing_stats.items():
        pct = missing_count / len(df) * 100
        print(f"  {col}: {missing_count:,} ({pct:.2f}%)")
    
    # Valores infinitos
    print(f"\nValores Infinitos:")
    for col in df.select_dtypes(include=[np.number]).columns:
        inf_count = np.isinf(df[col]).sum()
        if inf_count > 0:
            print(f"  {col}: {inf_count:,} valores infinitos")
    
    # OHLC consistency
    print(f"\nConsist√™ncia OHLC:")
    ohlc_issues = 0
    
    # High >= max(Open, Close) and Low <= min(Open, Close)
    high_issues = (df['high'] < df[['open', 'close']].max(axis=1)).sum()
    low_issues = (df['low'] > df[['open', 'close']].min(axis=1)).sum()
    
    print(f"  High < max(Open,Close): {high_issues:,}")
    print(f"  Low > min(Open,Close): {low_issues:,}")
    
    if high_issues > 0 or low_issues > 0:
        print("‚ö†Ô∏è  PROBLEMA: Inconsist√™ncias em dados OHLC!")
        ohlc_issues = high_issues + low_issues
    
    # Verificar gaps extremos
    price_cols = ['open', 'high', 'low', 'close']
    for col in price_cols:
        price_changes = df[col].pct_change().abs()
        extreme_changes = (price_changes > 0.1).sum()  # Mudan√ßas > 10%
        if extreme_changes > 0:
            print(f"  {col}: {extreme_changes:,} mudan√ßas extremas (>10%)")
    
    # Verificar volume zero
    zero_volume = (df['volume'] == 0).sum()
    print(f"  Volume zero: {zero_volume:,} ({zero_volume/len(df)*100:.2f}%)")
    
    # Verificar duplicatas de timestamp
    if 'timestamp' in df.index.names or 'timestamp' in df.columns:
        if 'timestamp' in df.columns:
            duplicates = df['timestamp'].duplicated().sum()
        else:
            duplicates = df.index.duplicated().sum()
        print(f"  Timestamps duplicados: {duplicates:,}")
    
    return ohlc_issues

def identify_rl_convergence_issues(df, returns, ohlc_issues):
    """Identifica problemas espec√≠ficos que podem impedir converg√™ncia de RL"""
    print("\n" + "="*80)
    print("9. PROBLEMAS ESPEC√çFICOS PARA CONVERG√äNCIA DE RL")
    print("="*80)
    
    issues = []
    
    # 1. Falta de features t√©cnicas
    tech_indicators = [col for col in df.columns if any(ind in col.lower() 
                      for ind in ['sma', 'ema', 'rsi', 'macd', 'bb', 'atr', 'momentum'])]
    if len(tech_indicators) < 3:
        issues.append("CR√çTICO: Poucos indicadores t√©cnicos - agente RL precisa de features ricas")
    
    # 2. Distribui√ß√£o de returns problem√°tica
    returns_clean = returns.dropna()
    if abs(stats.skew(returns_clean)) > 2:
        issues.append(f"PROBLEMA: Returns muito assim√©tricos (skew={stats.skew(returns_clean):.3f})")
    
    if stats.kurtosis(returns_clean) > 10:
        issues.append(f"PROBLEMA: Returns com curtose extrema (kurt={stats.kurtosis(returns_clean):.3f})")
    
    # 3. Volatilidade extrema
    vol_rolling = returns_clean.rolling(window=100).std()
    vol_changes = vol_rolling.pct_change().abs()
    extreme_vol_changes = (vol_changes > 2).sum()  # Mudan√ßas de volatilidade > 200%
    if extreme_vol_changes > len(returns_clean) * 0.01:  # > 1% das observa√ß√µes
        issues.append(f"PROBLEMA: Volatilidade muito inst√°vel ({extreme_vol_changes:,} mudan√ßas extremas)")
    
    # 4. Returns zerados excessivos
    zero_returns_pct = (returns_clean == 0).sum() / len(returns_clean) * 100
    if zero_returns_pct > 5:
        issues.append(f"PROBLEMA: Muitos returns zerados ({zero_returns_pct:.2f}%)")
    
    # 5. Autocorrela√ß√£o forte (n√£o random walk)
    autocorr_1 = returns_clean.autocorr(lag=1)
    if abs(autocorr_1) > 0.1:
        issues.append(f"ALERTA: Autocorrela√ß√£o forte lag-1 ({autocorr_1:.4f}) - n√£o √© random walk")
    
    # 6. Dados OHLC inconsistentes
    if ohlc_issues > 0:
        issues.append(f"CR√çTICO: {ohlc_issues:,} inconsist√™ncias em dados OHLC")
    
    # 7. Regime √∫nico
    if 'regime' in df.columns:
        regime_counts = df['regime'].value_counts()
        if len(regime_counts) == 1:
            issues.append(f"PROBLEMA: Apenas um regime ({regime_counts.index[0]}) - falta diversidade")
        elif regime_counts.min() / regime_counts.max() < 0.1:
            issues.append(f"PROBLEMA: Distribui√ß√£o de regimes muito desbalanceada")
    
    # 8. Falta de variabilidade nos pre√ßos
    price_range = (df['high'].max() - df['low'].min()) / df['close'].mean() * 100
    if price_range < 5:  # Menos de 5% de range total
        issues.append(f"PROBLEMA: Range de pre√ßos muito pequeno ({price_range:.2f}%)")
    
    # 9. Volume patterns
    if (df['volume'] == df['volume'].iloc[0]).sum() / len(df) > 0.5:
        issues.append("PROBLEMA: Volume muito constante - pode ser sint√©tico de forma inadequada")
    
    # 10. Timeframe issues
    time_diffs = df.index.to_series().diff()[1:]
    expected_diff = pd.Timedelta(minutes=5)
    irregular_intervals = (time_diffs != expected_diff).sum()
    if irregular_intervals > len(df) * 0.01:  # > 1%
        issues.append(f"PROBLEMA: {irregular_intervals:,} intervalos irregulares no timeframe")
    
    # Resumo
    print(f"TOTAL DE PROBLEMAS IDENTIFICADOS: {len(issues)}")
    for i, issue in enumerate(issues, 1):
        print(f"{i:2d}. {issue}")
    
    if len(issues) == 0:
        print("‚úÖ Nenhum problema cr√≠tico identificado para RL")
    elif len(issues) <= 3:
        print("‚ö†Ô∏è  Alguns problemas identificados - podem afetar converg√™ncia")
    else:
        print("üö® MUITOS PROBLEMAS - converg√™ncia de RL provavelmente comprometida")
    
    return issues

def main():
    """Fun√ß√£o principal de an√°lise"""
    filepath = r"D:\Projeto\data\GOLD_TRADING_READY_2M_20250803_222334.csv"
    
    print("AN√ÅLISE COMPLETA DO DATASET PARA IDENTIFICA√á√ÉO DE PROBLEMAS DE CONVERG√äNCIA DE RL")
    print("="*90)
    
    try:
        # 1. Carregar e examinar estrutura
        df = load_and_examine_dataset(filepath)
        
        # 2. Estat√≠sticas descritivas
        df = calculate_descriptive_stats(df)
        
        # 3. An√°lise de distribui√ß√£o
        returns = analyze_returns_distribution(df)
        
        # 4. Autocorrela√ß√£o
        analyze_autocorrelation(df)
        
        # 5. Estacionariedade
        test_stationarity(df)
        
        # 6. Indicadores t√©cnicos
        analyze_technical_indicators(df)
        
        # 7. Padr√µes temporais
        analyze_temporal_patterns(df)
        
        # 8. Qualidade dos dados
        ohlc_issues = check_data_quality(df)
        
        # 9. Problemas espec√≠ficos de RL
        issues = identify_rl_convergence_issues(df, returns, ohlc_issues)
        
        print("\n" + "="*90)
        print("AN√ÅLISE CONCLU√çDA")
        print("="*90)
        print(f"Dataset analisado: {len(df):,} observa√ß√µes")
        print(f"Problemas identificados: {len(issues)}")
        
        return df, issues
        
    except Exception as e:
        print(f"ERRO na an√°lise: {e}")
        import traceback
        traceback.print_exc()
        return None, []

if __name__ == "__main__":
    df, issues = main()