#!/usr/bin/env python3
"""
üîß FIX SATURA√á√ÉO CR√çTICA - Reinicializar pesos saturados

Detecta e reinicializa pesos que est√£o 100% saturados
"""

import torch
import torch.nn as nn
import numpy as np

def fix_saturated_weights(model, threshold=0.8, verbose=True):
    """
    üîß Corrigir pesos saturados no modelo
    
    Args:
        model: Modelo PyTorch
        threshold: Threshold para considerar saturado (0.8 = 80%)
        verbose: Imprimir detalhes
    """
    
    fixed_components = []
    
    if verbose:
        print("üîß [SATURA√á√ÉO FIX] Iniciando corre√ß√£o de pesos saturados...")
    
    for name, param in model.named_parameters():
        if param.data is not None:
            # Verificar satura√ß√£o
            data_tensor = param.data.detach()
            
            # Contar valores pr√≥ximos dos extremos
            near_zero = (torch.abs(data_tensor) < 0.01).float().mean().item()
            near_one = (torch.abs(data_tensor) > 0.99).float().mean().item()
            
            total_saturation = near_zero + near_one
            
            if total_saturation > threshold:
                if verbose:
                    print(f"   üö® FIXING: {name} - {total_saturation*100:.1f}% saturado")
                
                # CORRE√á√ÉO ESPEC√çFICA POR TIPO
                if 'log_std' in name.lower():
                    # log_std cr√≠tico: deve permitir explora√ß√£o
                    # log_std = 0 ‚Üí std = 1, log_std = -1 ‚Üí std = 0.37
                    param.data.normal_(mean=-0.5, std=0.2)  # std m√©dia ~= 0.6
                    
                elif 'bias' in name.lower():
                    # Bias pequeno
                    param.data.normal_(mean=0.0, std=0.01)
                    
                elif 'weight' in name.lower():
                    # Reinicializa√ß√£o Xavier/He
                    if len(param.shape) >= 2:
                        # Linear/Conv layers
                        fan_in = param.shape[1] if len(param.shape) >= 2 else param.shape[0]
                        fan_out = param.shape[0]
                        
                        # He initialization para ReLU-like, Xavier para outros
                        if 'relu' in name.lower() or 'gelu' in name.lower():
                            std = np.sqrt(2.0 / fan_in)  # He
                        else:
                            std = np.sqrt(2.0 / (fan_in + fan_out))  # Xavier
                            
                        param.data.normal_(mean=0.0, std=std)
                    else:
                        # 1D params
                        param.data.normal_(mean=0.0, std=0.02)
                        
                elif 'pos_encoding' in name.lower() or 'embedding' in name.lower():
                    # Positional encoding - pequena varia√ß√£o
                    param.data.normal_(mean=0.0, std=0.02)
                    
                else:
                    # Default: small random values
                    param.data.normal_(mean=0.0, std=0.02)
                
                fixed_components.append(name)
    
    if verbose:
        if fixed_components:
            print(f"‚úÖ [SATURA√á√ÉO FIX] {len(fixed_components)} componentes corrigidos:")
            for comp in fixed_components[:10]:  # Top 10
                print(f"      {comp}")
        else:
            print("‚úÖ [SATURA√á√ÉO FIX] Nenhuma satura√ß√£o cr√≠tica detectada")
    
    return len(fixed_components)

def apply_fix_to_policy(model, verbose=True):
    """
    üîß Aplicar fix espec√≠fico para policy PPO
    """
    
    if verbose:
        print("üîß [POLICY FIX] Aplicando corre√ß√£o espec√≠fica para PPO policy...")
    
    fixed_count = 0
    
    # Fix policy espec√≠fica
    if hasattr(model, 'policy'):
        fixed_count += fix_saturated_weights(model.policy, threshold=0.7, verbose=verbose)
    
    # Fix adicional para componentes cr√≠ticos
    for name, module in model.policy.named_modules():
        if isinstance(module, (nn.LSTM, nn.GRU)):
            if verbose:
                print(f"   üîß Reinicializando LSTM: {name}")
            
            # Reinicializar LSTM weights
            for param_name, param in module.named_parameters():
                if 'weight' in param_name:
                    nn.init.orthogonal_(param.data, gain=1.0)
                elif 'bias' in param_name:
                    # LSTM bias: forget gate deve ser 1.0
                    if 'bias_hh' in param_name or 'bias_ih' in param_name:
                        param.data.fill_(0.0)
                        # Set forget gate bias to 1
                        hidden_size = param.size(0) // 4
                        param.data[hidden_size:2*hidden_size].fill_(1.0)
                        
            fixed_count += 1
    
    if verbose:
        print(f"‚úÖ [POLICY FIX] Corre√ß√£o conclu√≠da. {fixed_count} componentes afetados.")
    
    return fixed_count

if __name__ == "__main__":
    print("üîß Fix para satura√ß√£o de pesos - Pronto para uso")
    print("   Usage: fix_saturated_weights(model)")
    print("   Usage: apply_fix_to_policy(model)")