# ğŸ”§ CORREÃ‡ÃƒO KL DIVERGENCE INSTÃVEL

## ğŸ“Š Problema Identificado:
- **KL divergence oscilando**: 10 â†’ 0.1 â†’ picos altos
- **Causa**: ParÃ¢metros ultra-conservadores causando instabilidade paradoxal

## ğŸ”§ Ajustes Aplicados:

### 1. **target_kl**: 0.01 â†’ **0.03**
- **RazÃ£o**: 0.01 Ã© excessivamente restritivo
- **Efeito**: Permite mudanÃ§as graduais na polÃ­tica

### 2. **clip_range**: 0.10 â†’ **0.15** 
- **RazÃ£o**: 0.10 Ã© muito conservador para dados complexos
- **Efeito**: Maior flexibilidade para updates de polÃ­tica

### 3. **n_epochs**: 4 â†’ **3**
- **RazÃ£o**: Reduzir overfitting que causa KL spikes
- **Efeito**: Menos iteraÃ§Ãµes = menos chance de overfit

### 4. **Learning Rates**: Actor 1.5e-05, Critic 3.0e-05 â†’ **2.0e-05 (ambos)**
- **RazÃ£o**: LRs diferentes causam conflitos actor-critic
- **Efeito**: Aprendizado sincronizado e estÃ¡vel

## ğŸ“ˆ Resultado Esperado:
- KL divergence estÃ¡vel entre 0.01-0.05
- Menos oscilaÃ§Ãµes bruscas
- Treinamento mais suave e consistente

## ğŸ¯ Monitoramento:
Observar nas prÃ³ximas 50k-100k steps:
- KL divergence deve estabilizar < 0.05
- Policy loss deve ser mais consistente
- Entropia deve manter-se estÃ¡vel

## âš ï¸ Se Problema Persistir:
1. Aumentar `batch_size`: 32 â†’ 64
2. Reduzir `ent_coef`: 0.05 â†’ 0.03
3. Verificar se reward scaling estÃ¡ adequado