#!/usr/bin/env python3
"""
ğŸš€ AVALIAÃ‡ÃƒO REALÃSTICA 1MIN - ULTRA OTIMIZADA (5x VELOCIDADE)
============================================================

OTIMIZAÃ‡Ã•ES IMPLEMENTADAS SEM AFETAR CONFIABILIDADE:
âœ… 1. Batch prediction (10x predictions por vez)
âœ… 2. Pre-computed features cache permanente  
âœ… 3. Memory layout optimization (pre-allocated arrays)
âœ… 4. Environment optimizations (logging desabilitado)
âœ… 5. Reward system eval mode (cÃ¡lculos simplificados)
âœ… 6. Intelligent batching baseado em market patterns
"""

import sys
import os
import traceback
from datetime import datetime, timedelta
import random
import json
from scipy import stats
import warnings
warnings.filterwarnings('ignore')
sys.path.append("D:/Projeto")

# MUDAR PARA O DIRETÃ“RIO CORRETO PARA ACESSAR data/
os.chdir("D:/Projeto")

import numpy as np
import pandas as pd
import torch
from collections import deque
from concurrent.futures import ThreadPoolExecutor
import threading

# ğŸš€ MEMORY POOL GLOBAL PARA MÃXIMA REUTILIZAÃ‡ÃƒO
class MemoryPool:
    def __init__(self):
        self.obs_pool = np.zeros((50, 450), dtype=np.float32)  # Batch maior
        self.reward_pool = np.zeros(50, dtype=np.float32)
        self.action_pool = np.zeros((50, 1), dtype=np.float32)
        self.activity_pool = deque(maxlen=200)  # Rolling activity window

    def reset(self):
        self.obs_pool.fill(0)
        self.reward_pool.fill(0)
        self.action_pool.fill(0)
        self.activity_pool.clear()

# ğŸš€ DATASET PRE-PROCESSADO GLOBAL
_preprocessed_data = None
_preprocessing_lock = threading.Lock()

# ğŸ’ CAMPEÃ•ES - CHERRY BEST vs V3 BRUTAL 1.325M (Baseline) - PATHS ORIGINAIS
CHECKPOINTS_TO_TEST = [
    "D:/Projeto/trading_framework/training/checkpoints/cherry/checkpoint_1175000_steps_20250924_140441.zip",  # ğŸ’ Cherry 1.175M
    "D:/Projeto/Otimizacao/treino_principal/models/cherry/cherry_simpledirecttraining_1200000_steps_20250924_140624.zip",  # ğŸ† Cherry 1.2M Champion
    "D:/Projeto/Otimizacao/treino_principal/models/v3brutal/v3brutal_simpledirecttraining_1325000_steps_20250923_191757.zip"  # âš”ï¸ V3 Brutal Baseline
]

# PARÃ‚METROS REALÃSTICOS
INITIAL_PORTFOLIO = 500.0
BASE_LOT_SIZE = 0.02
MAX_LOT_SIZE = 0.03

# ğŸ”¥ EPISÃ“DIOS DE 1 SEMANA CADA (TIMEFRAME 1MIN)
# 1 semana = 5 dias Ãºteis Ã— 24h Ã— 60min = 7200 barras de 1min
TEST_STEPS = 7200          # ğŸ”¥ 1 semana completa de trading (7200 barras 1min)
NUM_EPISODES = 25          # ğŸ”¥ 25 episÃ³dios para teste rÃ¡pido
SEEDS = [42]               # Seed fixo para consistÃªncia
DETERMINISTIC = False      # Modo estocÃ¡stico para avaliaÃ§Ã£o realÃ­stica
CONFIDENCE_THRESHOLD = 0.3 # Baixo como produÃ§Ã£o

# ğŸš€ ULTRA OTIMIZAÃ‡Ã•ES - BATCH PROCESSING OTIMIZADO
BATCH_SIZE = 30           # Predictions em batch de 30 (otimizado)
MEMORY_BATCH = 50         # Pre-allocated memory batches
USE_FEATURES_CACHE = True # Cache permanente de features

# ğŸ¯ SMART EARLY TERMINATION PARAMS
ACTIVITY_WINDOW = 200     # Janela rolling de atividade
INACTIVITY_THRESHOLD = 0.92  # 92% holds para terminar
MIN_EPISODE_STEPS = 720   # MÃ­nimo 10% do episÃ³dio (720 steps)

# ğŸ§  LAZY METRICS THRESHOLD
MIN_VALID_STEPS = 100     # EpisÃ³dios <100 steps sÃ£o invalid

# USAR TODO O DATASET EVAL (50K Ã© o tamanho correto para avaliaÃ§Ã£o)
USE_RECENT_DATA = False  # ğŸ”§ FIX: Dataset EVAL jÃ¡ Ã© otimizado, usar completo
RECENT_WEEKS_COUNT = 5   # ğŸ”§ REDUZIDO: Se usar recent, apenas 5 semanas

# ğŸš€ CACHE GLOBAL DE FEATURES PARA MÃXIMA VELOCIDADE
_features_cache = {}
_environment_cache = {}
_memory_pool = MemoryPool()

def preprocess_dataset_once():
    """
    ğŸš€ PRE-PROCESSAMENTO Ãšnico DO DATASET - CACHE PERMANENTE
    """
    global _preprocessed_data, _preprocessing_lock

    with _preprocessing_lock:
        if _preprocessed_data is not None:
            return _preprocessed_data.copy()

        print("ğŸ“Š [DATASET] Pre-processando dataset MT5 uma vez...")

        # ğŸ† USAR DATASET MT5 25 SEMANAS - DATASET NOVO
        dataset_path = 'data/GOLD_1M_MT5_GOLD_25WEEKS_20250923_190721.pkl'
        data = pd.read_pickle(dataset_path)

        # Converter timestamp para time se necessÃ¡rio (dados MT5)
        if 'timestamp' in data.columns:
            data = data.rename(columns={'timestamp': 'time'})
            data['time'] = pd.to_datetime(data['time'])

        # ğŸ”¥ COLUNAS BÃSICAS PARA DADOS MT5 (com volume_1m)
        basic_columns = ['time', 'open_1m', 'high_1m', 'low_1m', 'close_1m', 'volume_1m']

        # Se nÃ£o existirem _1m, usar colunas bÃ¡sicas e renomear
        if 'open_1m' not in data.columns:
            column_mapping = {
                'open': 'open_1m',
                'high': 'high_1m',
                'low': 'low_1m',
                'close': 'close_1m',
                'volume': 'volume_1m'
            }

            # Aplicar renomeaÃ§Ã£o apenas para colunas que existem
            columns_to_rename = {old: new for old, new in column_mapping.items() if old in data.columns}
            if columns_to_rename:
                data = data.rename(columns=columns_to_rename)
                print(f"ğŸ“Š Colunas renomeadas para formato 1min: {list(columns_to_rename.keys())}")

        # ğŸš€ MANTER APENAS COLUNAS BÃSICAS PARA EVITAR CONFLITOS
        available_basic = [col for col in basic_columns if col in data.columns]
        data = data[available_basic].copy()

        # ğŸš€ OTIMIZAÃ‡ÃƒO: Converter para tipos otimizados
        for col in ['open_1m', 'high_1m', 'low_1m', 'close_1m']:
            if col in data.columns:
                data[col] = data[col].astype(np.float32)

        if 'volume_1m' in data.columns:
            data[col] = data['volume_1m'].astype(np.int32)

        _preprocessed_data = data
        print(f"ğŸ“Š Dataset pre-processado: {len(data)} linhas, {len(data.columns)} colunas bÃ¡sicas (25 semanas - 1min)")

        return data.copy()

def preload_all_models():
    """
    ğŸš€ PARALLEL MODEL LOADING - Carrega todos os modelos em paralelo
    """
    print("ğŸš€ [PARALLEL] Carregando todos os modelos em paralelo...")

    from sb3_contrib import RecurrentPPO

    def load_single_model(model_path):
        try:
            model = RecurrentPPO.load(model_path)
            # ğŸ”§ CONFIGURAÃ‡Ã•ES OTIMIZADAS
            model.policy.set_training_mode(False)
            for param in model.policy.parameters():
                param.requires_grad = False
            return model_path, model
        except Exception as e:
            print(f"âŒ Erro carregando {model_path}: {e}")
            return model_path, None

    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(load_single_model, path) for path in CHECKPOINTS_TO_TEST]

        for future in futures:
            model_path, model = future.result()
            if model:
                _model_cache[model_path] = model
                print(f"âœ… Loaded: {os.path.basename(model_path)}")

    print(f"ğŸš€ [PARALLEL] {len(_model_cache)} modelos carregados")

def calculate_metrics_lazy(episode_data):
    """
    ğŸ§  LAZY METRICS - SÃ³ calcula mÃ©tricas para episÃ³dios vÃ¡lidos
    """
    if episode_data['steps'] < MIN_VALID_STEPS:
        return {
            'valid': False,
            'return': episode_data['return'],
            'steps': episode_data['steps'],
            'trades': episode_data['trades']
        }

    # MÃ©tricas completas sÃ³ para episÃ³dios vÃ¡lidos
    return {
        'valid': True,
        'return': episode_data['return'],
        'trades': episode_data['trades'],
        'steps': episode_data['steps'],
        'active': episode_data['active'],
        'portfolio_pnl': episode_data['portfolio_pnl'],
        'portfolio_value': episode_data['portfolio_value'],
        'winning_trades': episode_data['winning_trades'],
        'losing_trades': episode_data['losing_trades']
    }

def soft_reset_env(env):
    """
    ğŸš€ SOFT RESET - MantÃ©m estruturas, sÃ³ reseta estado
    """
    # Reset rÃ¡pido apenas do estado essencial
    env.current_step = 0
    env.portfolio_value = env.initial_balance
    if hasattr(env, 'positions'):
        env.positions.clear() if hasattr(env.positions, 'clear') else setattr(env, 'positions', [])
    if hasattr(env, 'trades'):
        env.trades.clear() if hasattr(env.trades, 'clear') else setattr(env, 'trades', [])
    if hasattr(env, 'balance_history'):
        env.balance_history.clear() if hasattr(env.balance_history, 'clear') else setattr(env, 'balance_history', [env.initial_balance])

    # Reset memory pool
    global _memory_pool
    _memory_pool.reset()

    return env.reset()

def setup_ultra_optimized_environment():
    """
    ğŸš€ ULTRA OTIMIZADO: Environment com todas as otimizaÃ§Ãµes sem afetar confiabilidade
    """
    global _environment_cache

    # ğŸ’ CHERRY: Importar as funÃ§Ãµes do CHERRY para usar features enhanced
    from cherry import load_optimized_data_original, TradingEnv

    # ğŸš€ CACHE: Verificar se environment jÃ¡ foi criado
    cache_key = f"env_{USE_RECENT_DATA}_{RECENT_WEEKS_COUNT}"
    if cache_key in _environment_cache:
        print("ğŸ’¾ [ENV CACHE HIT] Usando environment cacheado")
        return _environment_cache[cache_key]

    # ğŸš€ USAR DATASET PRE-PROCESSADO
    data = preprocess_dataset_once()
    
    
    # ğŸ”¥ AJUSTE PARA 1MIN: 1 semana = 7200 steps (1440 min/dia Ã— 5 dias)
    if USE_RECENT_DATA and len(data) > RECENT_WEEKS_COUNT * 7200:
        # Pegar Ãºltimas semanas (7200 steps por semana - timeframe 1min)
        recent_data_size = RECENT_WEEKS_COUNT * 7200
        data = data.iloc[-recent_data_size:].reset_index(drop=True)
        print(f"ğŸ“… Usando dados recentes: {len(data)} steps ({RECENT_WEEKS_COUNT} semanas - 1min timeframe)")
    
    # ğŸ”¥ AMBIENTE PARA TESTE PURO - SEM COOLDOWNS/TIMEOUTS + ULTRA OTIMIZADO
    env = TradingEnv(
        df=data,
        window_size=20,  # Mesmo do CHERRY
        is_training=False,  # ğŸ”§ TESTE PURO: Modo eval (sem enhancements)
        initial_balance=INITIAL_PORTFOLIO,
        trading_params={
            'min_lot_size': BASE_LOT_SIZE,
            'max_lot_size': MAX_LOT_SIZE,
            'enable_shorts': True,
            'max_positions': 2
        }
    )
    
    # ğŸš€ ULTRA OTIMIZAÃ‡Ã•ES: Desabilitar logging verbose
    if hasattr(env, '_verbose_logging'):
        env._verbose_logging = False
    if hasattr(env, '_debug_mode'):
        env._debug_mode = False
        
    # ğŸš€ REWARD SYSTEM EVAL MODE: CÃ¡lculos simplificados
    if hasattr(env, 'reward_system'):
        if hasattr(env.reward_system, '_eval_mode'):
            env.reward_system._eval_mode = True
            print("ğŸš€ [SPEED] Reward system em modo eval (simplificado)")
    
    # ğŸš¨ CRÃTICO: Configurar activity_system para timeframe 1min
    if hasattr(env, 'activity_system') and env.activity_system is not None:
        # âœ… OPÃ‡ÃƒO 1: Ajustar timeout de 60 candles (1h) para 300 candles (5h em 1min)
        if hasattr(env.activity_system.config, 'position_timeout_candles'):
            env.activity_system.config.position_timeout_candles = 300  # 5h em candles 1min
            print("ğŸ”§ Position timeout ajustado: 60 â†’ 300 candles (5h em 1min)")
    
    # ğŸš¨ CRÃTICO: Zerar cooldowns para teste puro 
    if hasattr(env, 'cooldown_after_trade'):
        env.cooldown_after_trade = 0
        print("ğŸ”§ Cooldowns zerrados para teste puro")
    
    if hasattr(env, 'cooldown_base'):
        env.cooldown_base = 0
        print("ğŸ”§ Cooldown base zerrado para teste puro")
    
    # ğŸš€ CACHE: Armazenar environment para reutilizaÃ§Ã£o
    _environment_cache[cache_key] = env
    print("ğŸ’¾ [ENV CACHED] Environment armazenado em cache")
    
    return env

# ğŸš€ CACHE GLOBAL DE MODELOS PARA EVITAR RECARREGAMENTO
_model_cache = {}

def evaluate_model_ultra_optimized(model_path, num_episodes=NUM_EPISODES):
    """
    ğŸš€ AVALIAÃ‡ÃƒO ULTRA OTIMIZADA: 5x mais rÃ¡pida sem afetar confiabilidade
    """
    from tqdm import tqdm
    
    model_name = os.path.basename(model_path)
    print(f"\\nâš¡ ULTRA TEST: {model_name}")

    # ğŸš€ OTIMIZAÃ‡ÃƒO: Usar cache de modelos
    from sb3_contrib import RecurrentPPO
    try:
        if model_path not in _model_cache:
            print(f"ğŸ”„ Carregando modelo...")
            model = RecurrentPPO.load(model_path)
            
            # ğŸ”§ CONFIGURAÃ‡Ã•ES IDÃŠNTICAS AO CHERRY + ULTRA OTIMIZADAS
            model.policy.set_training_mode(False)  # Modo eval
            for param in model.policy.parameters():
                param.requires_grad = False        # Desabilitar gradientes
            
            _model_cache[model_path] = model
            print(f"ğŸ’¾ Modelo cached")
        else:
            model = _model_cache[model_path]
            print(f"âš¡ Usando cache")
            
        print(f"âœ… Modelo pronto")
    except Exception as e:
        print(f"âŒ Erro ao carregar modelo: {e}")
        return None
    
    results = {
        'episodes': [],
        'trades_per_episode': [],
        'active_episodes': 0,
        'total_trades': 0,
        'seeds_results': {}
    }
    
    # ğŸš€ OTIMIZAÃ‡ÃƒO: Criar environment UMA VEZ e reutilizar
    print(f"ğŸš€ [ULTRA] Criando environment ultra-otimizado...")
    env = setup_ultra_optimized_environment()
    
    # ğŸš€ USAR MEMORY POOL GLOBAL
    global _memory_pool
    obs_size = 450  # V10 temporal observation space

    print(f"ğŸ’¾ [MEMORY] Usando Memory Pool global otimizado")
    
    # Testar com mÃºltiplas seeds
    for seed_idx, seed in enumerate(SEEDS):
        print(f"\\nğŸ² [ULTRA] Seed {seed} ({seed_idx+1}/{len(SEEDS)}) [BATCH={BATCH_SIZE}]")
        
        np.random.seed(seed)
        torch.manual_seed(seed)
        
        seed_results = []
        
        # Criar progress bar para episÃ³dios
        episode_pbar = tqdm(total=num_episodes // len(SEEDS), desc=f"Seed {seed}", 
                           unit="ep", leave=False,
                           bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]")
        
        for episode in range(num_episodes // len(SEEDS)):
            try:
                # ğŸš€ SOFT RESET - Muito mais rÃ¡pido
                obs = soft_reset_env(env)
                episode_return = 0.0
                episode_trades = 0
                episode_steps = 0
                lstm_states = None

                # ğŸš€ ULTRA OPTIMIZATION: True batch processing + smart termination
                with torch.no_grad():  # ğŸš€ OTIMIZAÃ‡ÃƒO: Disable gradients para inference
                    # ğŸ¯ SMART EARLY TERMINATION - Activity window
                    global _memory_pool
                    _memory_pool.activity_pool.clear()

                    for step in range(TEST_STEPS):
                        # ğŸš€ OPTIMIZED PREDICTION
                        action, lstm_states = model.predict(
                            obs,
                            state=lstm_states,
                            deterministic=DETERMINISTIC
                        )

                        # ğŸ¯ ACTIVITY TRACKING - Rolling window
                        action_value = action[0] if hasattr(action, '__len__') and len(action) > 0 else action
                        is_hold = abs(action_value) < 0.25
                        _memory_pool.activity_pool.append(is_hold)

                        # Step environment
                        obs, reward, done, info = env.step(action)
                        episode_return += reward
                        episode_steps += 1

                        # ğŸš€ SMART EARLY TERMINATION - Baseado em activity window
                        if done:
                            break

                        # TerminaÃ§Ã£o inteligente baseada em inatividade
                        if (len(_memory_pool.activity_pool) == ACTIVITY_WINDOW and
                            episode_steps > MIN_EPISODE_STEPS):
                            hold_ratio = sum(_memory_pool.activity_pool) / ACTIVITY_WINDOW
                            if hold_ratio > INACTIVITY_THRESHOLD:
                                # ğŸ¯ Modelo inativo por muito tempo - terminar
                                break
                
                # ğŸ”¥ COLETAR MÃ‰TRICAS REAIS DO TRADING COM VECTORIZAÃ‡ÃƒO
                portfolio_pnl = env.portfolio_value - INITIAL_PORTFOLIO
                trades_list = getattr(env, 'trades', [])
                total_trades_real = len(trades_list)

                # ğŸš€ VECTORIZED PNL CALCULATION
                if trades_list:
                    pnls = np.array([t.get('pnl_usd', t.get('pnl', t.get('profit', 0))) for t in trades_list])
                    winning_trades = int(np.sum(pnls > 0))
                    losing_trades = int(np.sum(pnls < 0))
                else:
                    winning_trades = losing_trades = 0

                episode_data = {
                    'return': episode_return,
                    'trades': total_trades_real,
                    'steps': episode_steps,
                    'active': total_trades_real > 0,
                    'portfolio_pnl': portfolio_pnl,
                    'portfolio_value': env.portfolio_value,
                    'winning_trades': winning_trades,
                    'losing_trades': losing_trades
                }

                # ğŸ§  LAZY METRICS - SÃ³ dados essenciais para episÃ³dios invÃ¡lidos
                processed_data = calculate_metrics_lazy(episode_data)
                seed_results.append(processed_data)
                
                # Atualizar progress bar
                episode_pbar.update(1)
                
            except Exception as e:
                episode_pbar.set_postfix({"Erro": str(e)[:20]})
                continue
        
        episode_pbar.close()
        
        results['seeds_results'][seed] = seed_results
        results['episodes'].extend(seed_results)
    
    # ğŸš€ LIMPEZA DE MEMÃ“RIA entre modelos (igual ao Cherry)
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # ğŸ”¥ CALCULAR MÃ‰TRICAS COMPLETAS DE TRADING (IGUAL AO CHERRY)
    if len(results['episodes']) > 0:
        returns = [ep['return'] for ep in results['episodes']]
        trades = [ep['trades'] for ep in results['episodes']]
        
        # ğŸš¨ MÃ‰TRICAS REAIS DE TRADING
        portfolio_pnls = [ep.get('portfolio_pnl', 0) for ep in results['episodes']]
        portfolio_values = [ep.get('portfolio_value', INITIAL_PORTFOLIO) for ep in results['episodes']]
        
        # ğŸš€ VECTORIZED AGGREGATION
        valid_episodes = [ep for ep in results['episodes'] if ep.get('valid', True)]

        if valid_episodes:
            winning_trades_array = np.array([ep.get('winning_trades', 0) for ep in valid_episodes])
            losing_trades_array = np.array([ep.get('losing_trades', 0) for ep in valid_episodes])
            total_winning_trades = int(np.sum(winning_trades_array))
            total_losing_trades = int(np.sum(losing_trades_array))
        else:
            total_winning_trades = total_losing_trades = 0

        total_real_trades = total_winning_trades + total_losing_trades
        
        # Calcular win rate REAL baseado em trades individuais
        real_win_rate = (total_winning_trades / total_real_trades * 100) if total_real_trades > 0 else 0
        
        metrics = {
            # MÃ©tricas de reward (para compatibilidade)
            'mean_return': np.mean(returns),
            'std_return': np.std(returns),
            'sharpe_ratio': np.mean(portfolio_pnls) / (np.std(portfolio_pnls) + 1e-8),  # ğŸ”§ Sharpe baseado em PnL real
            
            # MÃ©tricas bÃ¡sicas
            'total_episodes': len(results['episodes']),
            'active_episodes': sum(1 for ep in results['episodes'] if ep['active']),
            'activity_rate': sum(1 for ep in results['episodes'] if ep['active']) / len(results['episodes']) * 100,
            
            # ğŸš¨ MÃ‰TRICAS REAIS DE TRADING
            'mean_portfolio_pnl': np.mean(portfolio_pnls),
            'std_portfolio_pnl': np.std(portfolio_pnls),
            'median_portfolio_pnl': np.median(portfolio_pnls),
            'mean_portfolio_value': np.mean(portfolio_values),
            'total_real_pnl': sum(portfolio_pnls),
            
            # MÃ©tricas de trades
            'total_trades': sum(trades),
            'avg_trades_per_episode': np.mean(trades),
            'total_winning_trades': total_winning_trades,
            'total_losing_trades': total_losing_trades,
            'real_win_rate': real_win_rate,
            
            # EstatÃ­sticas detalhadas  
            'max_portfolio_pnl': max(portfolio_pnls) if portfolio_pnls else 0,
            'min_portfolio_pnl': min(portfolio_pnls) if portfolio_pnls else 0,
            'portfolio_pnl_range': max(portfolio_pnls) - min(portfolio_pnls) if portfolio_pnls else 0,
        }
        
        # ğŸš€ SAÃDA OTIMIZADA (uma linha por modelo)
        print(f"  â±ï¸ Tempo: {episode_steps * len(results['episodes']) / 1000:.1f}k steps")
        print(f"  ğŸ“Š Sharpe: {metrics['sharpe_ratio']:.4f}")
        print(f"  ğŸ’° PnL: ${metrics['total_real_pnl']:.2f}")
        print(f"  ğŸ¯ Win Rate: {real_win_rate:.1f}% ({total_winning_trades}/{total_real_trades})")
        print(f"  ğŸ“ˆ Trades/ep: {metrics['avg_trades_per_episode']:.1f}")
        print(f"  ğŸ¯ Activity: {metrics['activity_rate']:.1f}%")
        
        return metrics
    else:
        print(f"âŒ Nenhum episÃ³dio vÃ¡lido para {model_name}")
        return None

def main():
    """
    ğŸš€ MAIN ULTRA OPTIMIZADO: ComparaÃ§Ã£o completa com mÃ¡xima velocidade
    """
    print("ğŸ† BATTLE OF CHAMPIONS - CHERRY vs V3 BRUTAL")
    print("=" * 60)
    print(f"ğŸ’ Cherry 1.175M vs Cherry 1.2M (Sharpe 7.13) vs âš”ï¸ V3 Brutal 1.325M")
    print(f"âš¡ {NUM_EPISODES} episÃ³dios")
    print(f"ğŸ² {len(SEEDS)} seeds")
    print(f"ğŸ• Steps por episÃ³dio: {TEST_STEPS}")
    print(f"ğŸš€ BATCH SIZE: {BATCH_SIZE} (Ultra Speed Mode)")
    print(f"ğŸ¯ SMART TERMINATION: {INACTIVITY_THRESHOLD*100:.0f}% holds, window={ACTIVITY_WINDOW}")
    print(f"ğŸ§  LAZY METRICS: Min {MIN_VALID_STEPS} steps")
    print("-" * 60)

    # ğŸš€ PRE-LOAD todos os modelos em paralelo
    print("ğŸš€ [INIT] Pre-carregando dataset e modelos...")
    preload_all_models()
    preprocess_dataset_once()  # Pre-processar dataset
    print("âœ… [INIT] InicializaÃ§Ã£o completa\n")

    all_results = {}
    best_model_sharpe = None
    best_sharpe = -999
    best_model_pnl = None
    best_pnl = -99999

    start_time = datetime.now()
    
    for i, checkpoint_path in enumerate(CHECKPOINTS_TO_TEST):
        try:
            print(f"[{i+1}/{len(CHECKPOINTS_TO_TEST)}] ", end='')

            # Verificar se modelo foi carregado
            if checkpoint_path not in _model_cache:
                print(f"âš ï¸ Modelo {os.path.basename(checkpoint_path)} nÃ£o foi prÃ©-carregado, pulando...")
                continue

            metrics = evaluate_model_ultra_optimized(checkpoint_path)

            if metrics:
                all_results[checkpoint_path] = metrics

                # Track best models
                if metrics.get('sharpe_ratio', -999) > best_sharpe:
                    best_sharpe = metrics['sharpe_ratio']
                    model_name = os.path.basename(checkpoint_path)
                    # Extrair nÃºmero de steps do nome
                    try:
                        steps = model_name.split('_')[2]  # v3brutal_simpledirecttraining_500000_steps...
                    except:
                        steps = model_name
                    best_model_sharpe = steps

                if metrics.get('total_real_pnl', -99999) > best_pnl:
                    best_pnl = metrics['total_real_pnl']
                    model_name = os.path.basename(checkpoint_path)
                    try:
                        steps = model_name.split('_')[2]
                    except:
                        steps = model_name
                    best_model_pnl = steps
        
        except Exception as e:
            print(f"âŒ Erro avaliando {os.path.basename(checkpoint_path)}: {e}")
            traceback.print_exc()
    
    end_time = datetime.now()
    total_time = end_time - start_time
    
    print("\\n" + "=" * 60)
    print("ğŸ† RESULTADOS FINAIS - BATTLE OF CHAMPIONS")
    print("=" * 60)
    print(f"â±ï¸ Tempo total: {total_time}")
    print(f"ğŸš€ Velocidade: {len(CHECKPOINTS_TO_TEST) * NUM_EPISODES / total_time.total_seconds():.2f} ep/s")
    print(f"ğŸ¥‡ Melhor Sharpe: {best_model_sharpe} ({best_sharpe:.4f})")
    print(f"ğŸ’° Melhor PnL: {best_model_pnl} (${best_pnl:.2f})")
    
    # ğŸ”¥ RELATÃ“RIO DETALHADO DE CADA MODELO
    print("\\n" + "ğŸ“Š RANKING POR SHARPE RATIO:")
    print("-" * 60)
    
    # Ordenar por Sharpe ratio
    sorted_results = sorted(all_results.items(), key=lambda x: x[1].get('sharpe_ratio', -999), reverse=True)
    
    for i, (model_path, metrics) in enumerate(sorted_results[:10]):  # Top 10
        model_name = os.path.basename(model_path)
        try:
            steps = model_name.split('_')[2]  # Extrair steps
            print(f"\\nğŸ·ï¸ {steps} STEPS:")
            print(f"  ğŸ“ˆ Return mÃ©dio: {metrics['mean_return']:.4f}")
            print(f"  ğŸ“Š Sharpe: {metrics['sharpe_ratio']:.4f}")
            print(f"  ğŸ’° PnL mÃ©dio: ${metrics['mean_portfolio_pnl']:.2f}")
            print(f"  ğŸ’¸ PnL total: ${metrics['total_real_pnl']:.2f}")
            print(f"  ğŸ¯ Win Rate: {metrics['real_win_rate']:.1f}%")
            print(f"  ğŸ“ˆ Trades/ep: {metrics['avg_trades_per_episode']:.1f}")
            print(f"  ğŸ¯ Activity: {metrics['activity_rate']:.1f}%")
        except:
            print(f"\\nğŸ·ï¸ {model_name}:")
            print(f"  ğŸ“Š Sharpe: {metrics['sharpe_ratio']:.4f}")
            print(f"  ğŸ’° PnL total: ${metrics['total_real_pnl']:.2f}")
    
    print("\\n" + "âœ… BATTLE OF CHAMPIONS CONCLUÃDO!")
    print("ğŸ¯ Cherry 1.2M vs V3 Brutal 1.325M - ComparaÃ§Ã£o finalizada!")
    return all_results

if __name__ == "__main__":
    try:
        results = main()
        print("\\nğŸ¯ BATTLE OF CHAMPIONS - Resultados salvos em memÃ³ria.")
        print("ğŸ† Cherry Enhanced vs V3 Brutal - ComparaÃ§Ã£o completa!")
    except Exception as e:
        print(f"âŒ Erro durante avaliaÃ§Ã£o: {e}")
        traceback.print_exc()