#!/usr/bin/env python3
"""
ğŸ§ª TESTE DE BALANCE - V3 BRUTAL REWARD
Simula cenÃ¡rios reais para verificar se os rewards estÃ£o balanceados
"""

import numpy as np
import sys
import os
sys.path.append(os.getcwd())

from trading_framework.rewards.reward_daytrade_v3_brutal import create_brutal_daytrade_reward_system

class MockTradingEnv:
    """Mock environment para simular cenÃ¡rios de trading"""
    
    def __init__(self, realized_pnl=0, unrealized_pnl=0, current_balance=1000, portfolio_value=1000):
        self.total_realized_pnl = realized_pnl
        self.total_unrealized_pnl = unrealized_pnl
        self.current_balance = current_balance
        self.portfolio_value = portfolio_value
        self.initial_balance = 1000
        
        # HistÃ³rico simulado
        self.trades_history = []
        self.portfolio_history = [portfolio_value]
        
    def get_current_price(self):
        return 4500.0  # PreÃ§o mock do ES

def test_reward_balance():
    """Testa diferentes cenÃ¡rios de PnL para verificar balance"""
    
    print("ğŸ§ª TESTE DE BALANCE - V3 BRUTAL REWARD CORRIGIDO")
    print("=" * 60)
    
    reward_system = create_brutal_daytrade_reward_system(initial_balance=1000)
    
    # CenÃ¡rios de teste
    test_scenarios = [
        # (nome, realized_pnl, unrealized_pnl, portfolio_value)
        ("Breakeven", 0, 0, 1000),
        ("Pequeno lucro +1%", 10, 0, 1010),
        ("Lucro mÃ©dio +3%", 30, 0, 1030),  # Como no seu log
        ("Lucro bom +5%", 50, 0, 1050),
        ("Pequena perda -1%", -10, 0, 990),
        ("Perda mÃ©dia -3%", -30, 0, 970),
        ("Perda alta -5%", -50, 0, 950),
        ("PosiÃ§Ã£o aberta +2%", 0, 20, 1020),
        ("PosiÃ§Ã£o aberta -2%", 0, -20, 980),
        ("Mix: Real +2%, Aberto +1%", 20, 10, 1030),
    ]
    
    print("\nğŸ“Š RESULTADOS DOS CENÃRIOS:")
    print("-" * 60)
    
    results = []
    
    for scenario_name, realized, unrealized, portfolio in test_scenarios:
        env = MockTradingEnv(realized, unrealized, portfolio, portfolio)
        
        # AÃ§Ã£o neutra
        action = np.array([0.0, 0.0, 0.0, 0.0])
        
        reward, info, done = reward_system.calculate_reward_and_info(env, action, {})
        
        pnl_total = realized + (unrealized * 0.5)
        pnl_percent = (pnl_total / 1000) * 100
        
        results.append({
            'scenario': scenario_name,
            'pnl_percent': pnl_percent,
            'reward': reward,
            'pnl_reward': info.get('pnl_reward', 0),
            'shaping_reward': info.get('shaping_reward', 0),
            'done': done
        })
        
        status = "ğŸš¨ DONE" if done else "âœ… CONT"
        
        print(f"{status} {scenario_name:20s} | PnL: {pnl_percent:+6.2f}% | Reward: {reward:+8.4f} | PnL: {info.get('pnl_reward', 0):+6.3f}")
    
    print("\nğŸ¯ ANÃLISE DE BALANCE:")
    print("-" * 40)
    
    # Verifica se rewards sÃ£o proporcionais
    positive_rewards = [r for r in results if r['pnl_percent'] > 0]
    negative_rewards = [r for r in results if r['pnl_percent'] < 0]
    
    if positive_rewards:
        avg_pos_reward = np.mean([r['reward'] for r in positive_rewards])
        print(f"ğŸ“ˆ Reward mÃ©dio lucros: {avg_pos_reward:+.4f}")
    
    if negative_rewards:
        avg_neg_reward = np.mean([r['reward'] for r in negative_rewards])
        print(f"ğŸ“‰ Reward mÃ©dio perdas: {avg_neg_reward:+.4f}")
    
    # Verifica se reward de +3% Ã© detectÃ¡vel
    plus_3_scenario = next((r for r in results if "mÃ©dia +3%" in r['scenario']), None)
    if plus_3_scenario:
        reward_3pct = plus_3_scenario['reward']
        print(f"\nğŸ¯ CENÃRIO DO SEU LOG (+3% = +$30):")
        print(f"   Reward: {reward_3pct:+.4f}")
        if abs(reward_3pct) < 0.001:
            print("   âŒ PROBLEMA: Reward muito baixo (< 0.001)")
        else:
            print("   âœ… OK: Reward detectÃ¡vel pelo PPO")
    
    # Teste de linearidade
    print(f"\nğŸ“ TESTE DE LINEARIDADE:")
    linear_scenarios = [(r['pnl_percent'], r['reward']) for r in results if -5 <= r['pnl_percent'] <= 5]
    if len(linear_scenarios) > 2:
        pnls = [s[0] for s in linear_scenarios]
        rewards = [s[1] for s in linear_scenarios]
        correlation = np.corrcoef(pnls, rewards)[0,1] if len(set(rewards)) > 1 else 0
        print(f"   CorrelaÃ§Ã£o PnL-Reward: {correlation:.3f}")
        if correlation > 0.8:
            print("   âœ… BOA linearidade")
        else:
            print("   âš ï¸  Linearidade baixa")

def test_extreme_scenarios():
    """Testa cenÃ¡rios extremos"""
    
    print(f"\nğŸ”¥ TESTE DE CENÃRIOS EXTREMOS:")
    print("-" * 40)
    
    reward_system = create_brutal_daytrade_reward_system(initial_balance=1000)
    
    extreme_scenarios = [
        ("Lucro extremo +20%", 200, 0, 1200),
        ("Perda extrema -20%", -200, 0, 800),
        ("Drawdown catastrÃ³fico -60%", -600, 0, 400),
        ("PosiÃ§Ã£o gigante +10%", 0, 100, 1100),
    ]
    
    for scenario_name, realized, unrealized, portfolio in extreme_scenarios:
        env = MockTradingEnv(realized, unrealized, portfolio, portfolio)
        action = np.array([0.0, 0.0, 0.0, 0.0])
        
        reward, info, done = reward_system.calculate_reward_and_info(env, action, {})
        
        pnl_total = realized + (unrealized * 0.5)
        pnl_percent = (pnl_total / 1000) * 100
        
        status = "ğŸš¨ DONE" if done else "âœ… CONT"
        clipped = "ğŸ“" if abs(reward) >= 0.99 else "  "
        
        print(f"{status} {clipped} {scenario_name:25s} | PnL: {pnl_percent:+6.1f}% | Reward: {reward:+8.4f}")

if __name__ == "__main__":
    test_reward_balance()
    test_extreme_scenarios()
    
    print(f"\nğŸ CONCLUSÃƒO:")
    print("Se o reward de +3% estÃ¡ > 0.01, o sistema deve funcionar corretamente")
    print("Se ainda estiver ~0, hÃ¡ outro problema na arquitetura")