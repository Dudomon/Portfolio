#!/usr/bin/env python3
"""
üé£ AVALIA√á√ÉO PESCADOR - SISTEMA SCALP ESPECIALIZADO
=====================================================

CONFIGURA√á√ÉO ESPEC√çFICA PARA PESCADOR:
‚úÖ 1. Ambiente: PescadorEnv (reward system otimizado para scalp)
‚úÖ 2. Diret√≥rio: PESCADOR models folder
‚úÖ 3. Sample Size: 100 epis√≥dios para confiabilidade
‚úÖ 4. M√©tricas: Focadas em trades r√°pidos e efici√™ncia
‚úÖ 5. Timeout: Sistema de 4h para posi√ß√µes (scalp style)
‚úÖ 6. Gate System: Filtros adaptativos desabilitados para teste

DIFEREN√áAS DO SILUS:
- Reward system: PescadorRewardSystem (activity bonuses)
- Epis√≥dios: 3000 steps vs 2000 (baixa volatilidade)
- Cooldowns: Desabilitados para teste
- Learning Rate: 2x do padr√£o (6e-05 actor, 4e-05 critic)
- Activity System: Timeout 4h para scalping

CONFIGURA√á√ÉO:
- Portfolio: $500 (produ√ß√£o) 
- Lot range: 0.02-0.03  
- Epis√≥dios: 50 (teste otimizado)
- Steps/epis√≥dio: 1800 (5 dias √∫teis)
- Mode: DETERMINISTIC (reproduz√≠vel)
- Architecture: Compat√≠vel com pescador training
"""

import sys
import os
import traceback
from datetime import datetime, timedelta
import random
import json
from scipy import stats
import warnings
warnings.filterwarnings('ignore')
sys.path.append("D:/Projeto")

import numpy as np
import pandas as pd
import torch

# CONFIGURA√á√ÉO COMPLETA PARA TESTE CONFI√ÅVEL - baseado no original
# MODELOS‚ÄëALVO POR STEPS (PESCADOR) 
PESCADOR_DIR = "D:/Projeto/Otimizacao/treino_principal/models/PESCADOR"
# Avaliar checkpoints PESCADOR dispon√≠veis
TARGET_STEPS = [
    3_000_000,  # 3M steps
    3_500_000,  # 3.5M steps
    4_000_000,  # 4M steps
    4_500_000,  # 4.5M steps
    5_000_000,  # 5M steps
]

# Baseline Legion V1 (absoluto)
BASELINE_MODEL = "D:/Projeto/Modelo PPO Trader/Modelo daytrade/Legion V1.zip"

def build_checkpoints_from_steps(dir_path: str, steps_list):
    """Monta a lista de checkpoints desejados a partir dos steps‚Äëalvo."""
    import glob
    selected = []
    for steps in steps_list:
        pattern = os.path.join(dir_path, f"*_{steps}_steps_*.zip")
        matches = sorted(glob.glob(pattern))
        if matches:
            # pegar o mais recente pelo mtime
            matches.sort(key=os.path.getmtime, reverse=True)
            sel = matches[0]
            if sel not in selected:
                selected.append(sel)
        else:
            print(f"‚ö†Ô∏è Checkpoint n√£o encontrado para {steps:,} steps ({pattern})")
    return selected

def extract_steps_from_name(path: str) -> int | None:
    """Extrai os steps do nome do arquivo (formato *_<steps>_steps_*.zip)."""
    try:
        import re, os as _os
        m = re.search(r"(\d{6,7})_steps", _os.path.basename(path))
        return int(m.group(1)) if m else None
    except Exception:
        return None
INITIAL_PORTFOLIO = 500.0  # $500 conforme solicitado
BASE_LOT_SIZE = 0.02
MAX_LOT_SIZE = 0.03

# PAR√ÇMETROS DE AVALIA√á√ÉO MELHORADOS (vs original)
TEST_STEPS = 1800          # 1800 steps = 5 dias √∫teis (1 semana completa) 
NUM_EPISODES = 50          # 50 epis√≥dios para teste mais r√°pido
MIN_EPISODE_GAP = 10000    # Gap m√≠nimo entre epis√≥dios (evitar overlap)
CONFIDENCE_LEVEL = 0.95    # 95% confidence intervals


def find_multiple_checkpoints(max_checkpoints=10):
    """üîç Encontrar modelos finais na pasta 'Modelos para testar'"""
    
    print("üé£ Buscando checkpoints PESCADOR...")
    
    # Construir a lista a partir dos steps‚Äëalvo
    selected = build_checkpoints_from_steps(PESCADOR_DIR, TARGET_STEPS)

    # Incluir baseline Legion V1, se existir
    try:
        if os.path.exists(BASELINE_MODEL):
            selected.append(BASELINE_MODEL)
            print(f"   + Baseline inclu√≠do: {os.path.basename(BASELINE_MODEL)}")
        else:
            print(f"‚ö†Ô∏è Baseline n√£o encontrado: {BASELINE_MODEL}")
    except Exception as _e:
        print(f"‚ö†Ô∏è Erro ao incluir baseline: {_e}")
    
    if not selected:
        print("‚ùå Nenhum modelo encontrado!")
        return []
    
    print(f"‚úÖ {len(selected)} modelos selecionados para teste:")
    for i, cp in enumerate(selected):
        size_mb = os.path.getsize(cp) / (1024*1024)
        mod_time = datetime.fromtimestamp(os.path.getmtime(cp)).strftime('%Y-%m-%d %H:%M')
        
        # Identificar modelo
        if os.path.basename(cp).lower().startswith("legion v1") or "Legion V1" in cp:
            model_info = "Baseline Legion V1"
        else:
            # Inferir pelos steps no nome
            st = extract_steps_from_name(cp)
            model_info = f"PESCADOR {st/1_000_000:.2f}M" if st else "Unknown model"
        
        print(f"   {i+1}. {model_info} | {size_mb:.1f}MB | {mod_time}")
    
    return selected

def create_evaluation_dataset():
    """üéØ Criar dataset espec√≠fico para avalia√ß√£o out-of-sample - COM CACHE"""
    print("üìä Preparando dataset de avalia√ß√£o...")
    
    # CACHE para acelerar carregamento
    cache_path = "D:/Projeto/data/CACHE_eval_dataset_processed.pkl"
    
    if os.path.exists(cache_path):
        print("üöÄ Carregando dataset PR√â-PROCESSADO do cache...")
        import pickle
        try:
            with open(cache_path, 'rb') as f:
                train_df, eval_df = pickle.load(f)
            print(f"‚úÖ Cache carregado: {len(train_df):,} treino + {len(eval_df):,} avalia√ß√£o")
            print(f"üìÖ Per√≠odo avalia√ß√£o: {eval_df.index.min()} at√© {eval_df.index.max()}")
            return train_df, eval_df
        except:
            print("‚ö†Ô∏è Erro no cache, reprocessando...")
    
    # Processar dataset original (s√≥ se n√£o tem cache)
    print("üîÑ Processando dataset original (primeira vez)...")
    dataset_path = "D:/Projeto/data/GC=F_YAHOO_20250821_161220.csv"
    
    if not os.path.exists(dataset_path):
        print(f"‚ùå Dataset n√£o encontrado: {dataset_path}")
        return None, None
    
    df = pd.read_csv(dataset_path)
    
    # Processar dataset
    if 'time' in df.columns:
        df['timestamp'] = pd.to_datetime(df['time'])
        df.set_index('timestamp', inplace=True)
        df.drop('time', axis=1, inplace=True)
    
    # Renomear colunas para formato padr√£o
    df = df.rename(columns={
        'open': 'open_5m',
        'high': 'high_5m',
        'low': 'low_5m', 
        'close': 'close_5m',
        'tick_volume': 'volume_5m'
    })
    
    total_len = len(df)
    print(f"‚úÖ Dataset carregado: {total_len:,} barras")
    print(f"üìÖ Per√≠odo: {df.index.min()} at√© {df.index.max()}")
    
    # RESERVAR √öLTIMOS 20% PARA AVALIA√á√ÉO (OUT-OF-SAMPLE)
    split_point = int(total_len * 0.8)
    train_df = df.iloc[:split_point]
    eval_df = df.iloc[split_point:]
    
    print(f"üîÑ Split realizado:")
    print(f"   üìö Treinamento: {len(train_df):,} barras ({train_df.index.min()} - {train_df.index.max()})")  
    print(f"   üéØ Avalia√ß√£o: {len(eval_df):,} barras ({eval_df.index.min()} - {eval_df.index.max()})")
    
    # SALVAR CACHE para pr√≥ximas execu√ß√µes
    print("üíæ Salvando cache para pr√≥ximas execu√ß√µes...")
    import pickle
    try:
        with open(cache_path, 'wb') as f:
            pickle.dump((train_df, eval_df), f, protocol=4)
        print("‚úÖ Cache salvo com sucesso!")
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao salvar cache: {e}")
    
    return train_df, eval_df

def calculate_comprehensive_metrics(episode_results):
    """üìä Calcular m√©tricas abrangentes de performance e risco - FIXED VERSION"""
    
    if not episode_results:
        return {}
    
    # Extrair retornos de todos os epis√≥dios
    returns = [ep['return_pct'] for ep in episode_results]
    portfolio_values = [ep['final_portfolio'] for ep in episode_results]
    all_trades = []
    all_portfolio_histories = []  # üîß NOVO: Para drawdown real
    
    for ep in episode_results:
        all_trades.extend(ep.get('trades_log', []))
        if 'portfolio_history' in ep:
            all_portfolio_histories.append(ep['portfolio_history'])  # üîß COLETAR HIST√ìRICO
    
    # M√âTRICAS B√ÅSICAS
    metrics = {
        # Retorno
        'mean_return': np.mean(returns),
        'median_return': np.median(returns),
        'std_return': np.std(returns),
        'min_return': np.min(returns),
        'max_return': np.max(returns),
        
        # Consist√™ncia
        'positive_episodes': len([r for r in returns if r > 0]),
        'win_rate_episodes': len([r for r in returns if r > 0]) / len(returns) * 100,
        
        # Portfolio
        'mean_final_portfolio': np.mean(portfolio_values),
        'portfolio_growth': (np.mean(portfolio_values) - INITIAL_PORTFOLIO) / INITIAL_PORTFOLIO * 100,
    }
    
    # M√âTRICAS DE RISCO AVAN√áADAS
    if len(returns) > 1:
        # Sharpe Ratio (assumindo risk-free rate = 0)
        metrics['sharpe_ratio'] = np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0
        
        # Sortino Ratio (downside deviation)
        negative_returns = [r for r in returns if r < 0]
        if negative_returns:
            downside_deviation = np.std(negative_returns)
            metrics['sortino_ratio'] = np.mean(returns) / downside_deviation if downside_deviation > 0 else 0
        else:
            metrics['sortino_ratio'] = float('inf') if np.mean(returns) > 0 else 0
        
        # üîß DRAWDOWN REAL - M√âTODO CORRIGIDO baseado em portfolio_history
        max_drawdowns_per_episode = []
        
        for portfolio_history in all_portfolio_histories:
            if len(portfolio_history) > 1:
                portfolio_array = np.array(portfolio_history)
                
                # Calcular running peak do portfolio real
                running_peak = np.maximum.accumulate(portfolio_array)
                
                # Calcular drawdown real em percentual
                portfolio_drawdowns = (portfolio_array - running_peak) / running_peak * 100
                
                # Max drawdown deste epis√≥dio
                episode_max_dd = np.min(portfolio_drawdowns)
                max_drawdowns_per_episode.append(episode_max_dd)
        
        if max_drawdowns_per_episode:
            # Max drawdown global = pior drawdown entre todos os epis√≥dios
            metrics['max_drawdown'] = min(max_drawdowns_per_episode)  # Mais negativo
            metrics['avg_drawdown_per_episode'] = np.mean(max_drawdowns_per_episode)
            metrics['drawdown_episodes_count'] = len(max_drawdowns_per_episode)
        else:
            # Fallback para m√©todo antigo se n√£o h√° portfolio_history
            cumulative_returns = np.cumprod(1 + np.array(returns) / 100)
            running_max = np.maximum.accumulate(cumulative_returns)
            drawdowns = (cumulative_returns - running_max) / running_max
            metrics['max_drawdown'] = np.min(drawdowns) * 100
            metrics['avg_drawdown_per_episode'] = metrics['max_drawdown']
            metrics['drawdown_episodes_count'] = 0
        
        # Value at Risk (VaR) - 5% worst cases
        metrics['var_5pct'] = np.percentile(returns, 5)
        
        # Calmar Ratio usando drawdown real
        real_max_dd = abs(metrics['max_drawdown'])
        if real_max_dd > 0.1:
            metrics['calmar_ratio'] = abs(metrics['mean_return']) / real_max_dd
        else:
            metrics['calmar_ratio'] = 0
    
    # M√âTRICAS DE TRADING
    if all_trades:
        profitable_trades = [t for t in all_trades if t.get('pnl_usd', 0) > 0]
        losing_trades = [t for t in all_trades if t.get('pnl_usd', 0) < 0]
        
        metrics.update({
            'total_trades': len(all_trades),
            'win_rate_trades': len(profitable_trades) / len(all_trades) * 100,
            'avg_profit_per_trade': np.mean([t.get('pnl_usd', 0) for t in profitable_trades]) if profitable_trades else 0,
            'avg_loss_per_trade': np.mean([t.get('pnl_usd', 0) for t in losing_trades]) if losing_trades else 0,
            'total_pnl': sum(t.get('pnl_usd', 0) for t in all_trades),
            'trades_per_episode': len(all_trades) / len(episode_results),
        })
        
        # Profit Factor
        gross_profit = sum(t.get('pnl_usd', 0) for t in profitable_trades)
        gross_loss = abs(sum(t.get('pnl_usd', 0) for t in losing_trades))
        metrics['profit_factor'] = gross_profit / gross_loss if gross_loss > 0 else float('inf') if gross_profit > 0 else 0
    
    # INTERVALOS DE CONFIAN√áA (95%)
    if len(returns) > 2:
        confidence_interval = stats.t.interval(
            CONFIDENCE_LEVEL, 
            len(returns)-1, 
            loc=np.mean(returns), 
            scale=stats.sem(returns)
        )
        metrics['ci_95_lower'] = confidence_interval[0]
        metrics['ci_95_upper'] = confidence_interval[1]
    
    return metrics

def simulate_realistic_trading_costs(trades_log):
    """üí∞ Simular custos real√≠sticos de trading"""
    if not trades_log:
        return 0.0
    
    total_cost = 0.0
    for trade in trades_log:
        lot_size = trade.get('lot_size', BASE_LOT_SIZE)
        
        # Custos simplificados de trading
        spread_cost = 0.3 * lot_size
        slippage_cost = 0.2 * lot_size * random.uniform(0.5, 1.5)
        commission = 0.5 * lot_size
        
        total_cost += (spread_cost + slippage_cost + commission)
    
    return total_cost

def test_v8_elegance_trading():
    """üöÄ Teste COMPLETO - baseado no avaliar_v11.py original com melhorias"""
    
    print(f"üé£ AVALIA√á√ÉO PESCADOR OTIMIZADA - 50 EPIS√ìDIOS")
    print("=" * 80)
    print(f"üíµ Portfolio Inicial: ${INITIAL_PORTFOLIO}")
    print(f"üìä Base Lot: {BASE_LOT_SIZE}")
    print(f"üìä Max Lot: {MAX_LOT_SIZE}")
    print(f"üß† Modo: DETERMINISTIC (reproduz√≠vel)")
    print(f"üìä Epis√≥dios: {NUM_EPISODES} (teste r√°pido)")
    print(f"üìè Steps: {TEST_STEPS} (5 dias √∫teis)")
    print("=" * 80)
    
    try:
        # Imports
        from sb3_contrib import RecurrentPPO
        from pescador import PescadorEnv as TradingEnv  # üé£ USANDO AMBIENTE PESCADOR ESPEC√çFICO
        
        print("‚úÖ Imports carregados")
        
        # 1. PREPARAR DATASET OUT-OF-SAMPLE (melhoria vs original)
        train_df, eval_df = create_evaluation_dataset()
        if eval_df is None:
            return False
        
        # 2. TESTAR CHECKPOINTS PESCADOR DISPON√çVEIS
        checkpoints = find_multiple_checkpoints(max_checkpoints=12)
        
        # Verificar se todos existem
        missing = [cp for cp in checkpoints if not os.path.exists(cp)]
        if missing:
            print(f"‚ùå Checkpoints n√£o encontrados: {[os.path.basename(m) for m in missing]}")
            return False
        
        print(f"üé£ TESTANDO 5 CHECKPOINTS PESCADOR: 3M ‚Üí 3.5M ‚Üí 4M ‚Üí 4.5M ‚Üí 5M (EVOLU√á√ÉO FINAL)")
        
        # 3. PREPARAR AMBIENTE DE TRADING (igual ao original)
        trading_params = {
            'base_lot_size': BASE_LOT_SIZE,
            'max_lot_size': MAX_LOT_SIZE,
            'initial_balance': INITIAL_PORTFOLIO,
            'target_trades_per_day': 18,  # Como no daytrader
            'stop_loss_range': (2.0, 8.0),
            'take_profit_range': (3.0, 15.0)
        }
        
        print("‚úÖ Par√¢metros de trading configurados")
        
        # RESULTADOS CONSOLIDADOS (melhoria vs original)
        all_checkpoint_results = {}
        
        # 4. TESTAR CADA CHECKPOINT (baseado no loop original)
        for checkpoint_idx, checkpoint_path in enumerate(checkpoints):
            print(f"\nü§ñ TESTANDO CHECKPOINT {checkpoint_idx + 1}/{len(checkpoints)}")
            print(f"üìÇ {os.path.basename(checkpoint_path)}")
            print("-" * 60)
            
            # CARREGAR MODELO (igual ao original)
            print("ü§ñ Carregando modelo...")
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            try:
                # Tentar carregamento normal primeiro (igual ao original)
                model = RecurrentPPO.load(checkpoint_path, device=device)
                print("‚úÖ Carregamento normal bem-sucedido")
                load_method = "direct_load"
            except Exception as e1:
                print(f"‚ö†Ô∏è Carregamento normal falhou: {str(e1)[:100]}...")
                try:
                    # Tentar com policy_kwargs V11Sigmoid (corrigido)
                    from trading_framework.policies.two_head_v11_sigmoid import get_v11_sigmoid_kwargs
                    sigmoid_kwargs = get_v11_sigmoid_kwargs()
                    model = RecurrentPPO.load(checkpoint_path, policy_kwargs=sigmoid_kwargs, device=device)
                    print("‚úÖ Carregamento com policy_kwargs bem-sucedido")
                    load_method = "with_kwargs"
                except Exception as e2:
                    print(f"‚ö†Ô∏è Carregamento com kwargs falhou: {str(e2)[:100]}...")
                    try:
                        # √öLTIMA TENTATIVA (igual ao original)
                        import zipfile
                        import tempfile
                        
                        with tempfile.TemporaryDirectory() as temp_dir:
                            # Extrair ZIP
                            with zipfile.ZipFile(checkpoint_path, 'r') as zip_ref:
                                zip_ref.extractall(temp_dir)
                            
                            # Carregar policy.pth diretamente
                            import glob
                            policy_files = glob.glob(f"{temp_dir}/**/policy.pth", recursive=True)
                            if not policy_files:
                                raise FileNotFoundError("policy.pth n√£o encontrado no ZIP")
                            
                            policy_state = torch.load(policy_files[0], map_location=device)
                            
                            # Criar modelo novo da arquitetura V7Intuition (igual ao original)
                            from trading_framework.policies.two_head_v7_intuition import get_v7_intuition_kwargs
                            intuition_kwargs = get_v7_intuition_kwargs()
                            
                            # Criar ambiente tempor√°rio para carregamento
                            temp_env = TradingEnv(
                                eval_df.head(100), 
                                window_size=20, 
                                is_training=False,
                                initial_balance=INITIAL_PORTFOLIO,
                                trading_params=trading_params
                            )
                            
                            # Usar m√©todo de carregamento do stable-baselines3
                            model = RecurrentPPO("MlpLstmPolicy", temp_env, policy_kwargs=intuition_kwargs, device=device)
                            
                            # Carregar pesos compat√≠veis ignorando incompat√≠veis (igual ao original)
                            current_state = model.policy.state_dict()
                            compatible_state = {}
                            
                            for key, value in policy_state.items():
                                if key in current_state and current_state[key].shape == value.shape:
                                    compatible_state[key] = value
                                else:
                                    pass  # Ignorar incompat√≠veis
                            
                            model.policy.load_state_dict(compatible_state, strict=False)
                            print(f"‚úÖ Carregamento FOR√áA BRUTA bem-sucedido - {len(compatible_state)} par√¢metros")
                            load_method = "manual_load"
                    
                    except Exception as e3:
                        print(f"‚ùå Todos os m√©todos falharam: {str(e3)[:100]}...")
                        continue  # Pular este checkpoint
            
            # Configurar modelo para modo determin√≠stico (MELHORIA vs original)
            model.policy.set_training_mode(False)
            print(f"‚úÖ Modelo carregado em {model.device}")
            
            # EXECUTAR M√öLTIPLOS EPIS√ìDIOS (ampliado vs original)
            print(f"üöÄ Iniciando {NUM_EPISODES} epis√≥dios de trading...")
            
            # Gerar posi√ß√µes aleat√≥rias para amostragem diversificada (MELHORIA)
            eval_len = len(eval_df)
            max_start_pos = eval_len - TEST_STEPS - 100
            
            if max_start_pos <= 0:
                print("‚ö†Ô∏è Dataset de avalia√ß√£o muito pequeno")
                continue
                
            # Gerar posi√ß√µes com gap m√≠nimo (MELHORIA vs original)
            episode_positions = []
            attempts = 0
            while len(episode_positions) < NUM_EPISODES and attempts < NUM_EPISODES * 3:
                candidate_pos = random.randint(0, max_start_pos)
                
                # Verificar dist√¢ncia m√≠nima
                too_close = False
                for existing_pos in episode_positions:
                    if abs(candidate_pos - existing_pos) < MIN_EPISODE_GAP:
                        too_close = True
                        break
                
                if not too_close:
                    episode_positions.append(candidate_pos)
                    
                attempts += 1
            
            # Se n√£o conseguiu posi√ß√µes suficientes, usar espa√ßamento uniforme
            if len(episode_positions) < NUM_EPISODES:
                episode_positions = []
                step = max_start_pos // NUM_EPISODES
                for i in range(NUM_EPISODES):
                    episode_positions.append(i * step)
            
            print(f"üéØ {len(episode_positions)} epis√≥dios configurados")
            
            # Resultados consolidados
            all_episodes = []
            total_returns = []
            
            # CRIAR AMBIENTE OTIMIZADO - PR√â-PROCESSAR FEATURES
            print(f"üèóÔ∏è Criando TradingEnv otimizado com {len(eval_df)} barras...")
            
            # PR√â-CALCULAR FEATURES UMA VEZ S√ì
            cache_features_path = "D:/Projeto/data/CACHE_trading_features.pkl"
            
            if os.path.exists(cache_features_path):
                print("‚ö° Carregando features pr√©-calculadas...")
                import pickle
                try:
                    with open(cache_features_path, 'rb') as f:
                        processed_df = pickle.load(f)
                    print("‚úÖ Features carregadas do cache!")
                except:
                    print("‚ö†Ô∏è Cache inv√°lido, recalculando...")
                    processed_df = eval_df.copy()
            else:
                print("üîÑ Primeira execu√ß√£o - ser√° mais lenta...")
                processed_df = eval_df.copy()
            
            # DESABILITAR LOGS VERBOSOS durante teste
            import logging
            trading_logger = logging.getLogger('trading_env')
            old_level = trading_logger.level
            trading_logger.setLevel(logging.ERROR)  # S√≥ erros cr√≠ticos
            
            trading_env = TradingEnv(
                processed_df,  # Dataset com features pr√©-calculadas
                window_size=20,
                is_training=False,
                initial_balance=INITIAL_PORTFOLIO,
                trading_params=trading_params
            )
            
            print("üé£ Usando ambiente PESCADOR para avalia√ß√£o")
            
            # üîß AJUSTAR FILTRO DE CONFIAN√áA PARA 0.6 (conforme solicitado)
            if hasattr(trading_env, '__class__'):
                # Modificar a constante na classe dinamicamente
                import types
                original_step = trading_env.step
                
                def modified_step(self, action):
                    # Temporariamente alterar o threshold de confian√ßa
                    import sys
                    current_module = sys.modules[self.__class__.__module__]
                    if hasattr(current_module, 'MIN_CONFIDENCE_THRESHOLD'):
                        original_threshold = current_module.MIN_CONFIDENCE_THRESHOLD
                        current_module.MIN_CONFIDENCE_THRESHOLD = 0.6
                    
                    # Executar step normal
                    result = original_step(action)
                    
                    # Restaurar threshold original
                    if hasattr(current_module, 'MIN_CONFIDENCE_THRESHOLD'):
                        current_module.MIN_CONFIDENCE_THRESHOLD = original_threshold
                    
                    return result
                
                # Bind the modified method
                trading_env.step = types.MethodType(modified_step, trading_env)
                print("üîß Filtro de confian√ßa ajustado para 0.6 (era 0.8)")
            else:
                print("‚ö†Ô∏è N√£o foi poss√≠vel ajustar filtro de confian√ßa")
            
            # SILENCIAR sa√≠da verbosa do ambiente
            trading_env.verbose = False
            if hasattr(trading_env, 'debug_mode'):
                trading_env.debug_mode = False
            
            # SALVAR CACHE DE FEATURES ap√≥s primeiro processamento
            if not os.path.exists(cache_features_path):
                print("üíæ Salvando features processadas para pr√≥ximas execu√ß√µes...")
                import pickle
                try:
                    # Salvar dataset processado do ambiente
                    processed_data = getattr(trading_env, 'df', processed_df)
                    with open(cache_features_path, 'wb') as f:
                        pickle.dump(processed_data, f, protocol=4)
                    print("‚úÖ Cache de features salvo!")
                except Exception as e:
                    print(f"‚ö†Ô∏è Erro ao salvar features: {e}")
            
            print(f"‚úÖ TradingEnv OTIMIZADO criado!")
            
            # EXECUTAR EPIS√ìDIOS (baseado no loop original)
            for episode_num, start_pos in enumerate(episode_positions):
                if (episode_num + 1) % 10 == 0:
                    print(f"   üìä Progresso: {episode_num + 1}/{NUM_EPISODES} epis√≥dios")
                
                # Verificar se posi√ß√£o √© v√°lida
                if start_pos + TEST_STEPS >= len(eval_df):
                    continue  # Pular se n√£o tem dados suficientes
                
                # ‚ö†Ô∏è RESET CR√çTICO COMPLETO - CORRIGIR BUG MATEM√ÅTICO
                # For√ßar reset total para evitar ac√∫mulo entre epis√≥dios
                trading_env.current_step = start_pos + 20
                
                # 1. RESET PORTFOLIO (PRINCIPAL + TODOS BACKUPS)
                trading_env.portfolio_value = INITIAL_PORTFOLIO
                trading_env.initial_balance = INITIAL_PORTFOLIO
                trading_env.realized_balance = INITIAL_PORTFOLIO  # üéØ CR√çTICO: Esta √© a chave!
                
                # 1b. RESET PICOS E DRAWDOWNS (tamb√©m cr√≠tico!)
                if hasattr(trading_env, 'peak_portfolio'):
                    trading_env.peak_portfolio = INITIAL_PORTFOLIO
                if hasattr(trading_env, 'peak_portfolio_value'):
                    trading_env.peak_portfolio_value = INITIAL_PORTFOLIO
                if hasattr(trading_env, 'current_drawdown'):
                    trading_env.current_drawdown = 0.0
                if hasattr(trading_env, 'peak_drawdown'):
                    trading_env.peak_drawdown = 0.0
                
                # 2. RESET TODOS OS ESTADOS DE VALOR (com prote√ß√£o para properties)
                if hasattr(trading_env, 'cash'):
                    try:
                        trading_env.cash = INITIAL_PORTFOLIO
                    except (AttributeError, TypeError):
                        pass  # Property read-only, ignorar
                
                if hasattr(trading_env, 'balance'):
                    try:
                        trading_env.balance = INITIAL_PORTFOLIO
                    except (AttributeError, TypeError):
                        pass  # Property read-only, ignorar
                
                # 3. RESET HIST√ìRICO DE VALORES
                if hasattr(trading_env, 'portfolio_history'):
                    trading_env.portfolio_history = []
                if hasattr(trading_env, 'balance_history'):
                    trading_env.balance_history = []
                if hasattr(trading_env, 'net_worth_history'):
                    trading_env.net_worth_history = []
                
                # 4. RESET COMPLETO DO ESTADO DE TRADING
                if hasattr(trading_env, 'trades'):
                    trading_env.trades = []
                if hasattr(trading_env, 'position_type'):
                    trading_env.position_type = 0
                if hasattr(trading_env, 'positions'):
                    trading_env.positions = []
                if hasattr(trading_env, 'open_positions'):
                    trading_env.open_positions = []
                if hasattr(trading_env, 'current_position'):
                    trading_env.current_position = None
                
                # 5. RESET M√âTRICAS DE PERFORMANCE
                if hasattr(trading_env, 'total_reward'):
                    trading_env.total_reward = 0.0
                if hasattr(trading_env, 'cumulative_reward'):
                    trading_env.cumulative_reward = 0.0
                if hasattr(trading_env, 'episode_reward'):
                    trading_env.episode_reward = 0.0
                if hasattr(trading_env, 'returns'):
                    trading_env.returns = []
                
                # 6. RESET CONTADORES
                if hasattr(trading_env, 'total_trades'):
                    trading_env.total_trades = 0
                if hasattr(trading_env, 'profitable_trades'):
                    trading_env.profitable_trades = 0
                if hasattr(trading_env, 'losing_trades'):
                    trading_env.losing_trades = 0
                
                # Obter observa√ß√£o inicial SEM reset completo
                obs = trading_env._get_observation()
                lstm_states = None
                done = False
                step = 0
                
                portfolio_history = [INITIAL_PORTFOLIO]
                
                while not done and step < TEST_STEPS:
                    # MODO ORIGINAL - N√ÉO DETERMIN√çSTICO (mantido do original)
                    action, lstm_states = model.predict(obs, state=lstm_states, deterministic=False)
                    
                    obs, reward, done, info = trading_env.step(action)
                    portfolio_history.append(trading_env.portfolio_value)
                    step += 1
                
                # üîç DEBUG: Coletar resultados com valida√ß√£o
                final_portfolio = trading_env.portfolio_value
                episode_return = ((final_portfolio - INITIAL_PORTFOLIO) / INITIAL_PORTFOLIO) * 100
                trades_log = getattr(trading_env, 'trades', [])
                
                # üîç DEBUG: Verificar se valores s√£o realistas
                portfolio_change = final_portfolio - INITIAL_PORTFOLIO
                if abs(portfolio_change) < 0.01 and len(trades_log) > 0:
                    print(f"   ‚ö†Ô∏è Episode {episode_num + 1}: Portfolio mudou apenas ${portfolio_change:.4f} com {len(trades_log)} trades")
                    print(f"      Inicial: ${INITIAL_PORTFOLIO:.2f}, Final: ${final_portfolio:.2f}")
                    if len(trades_log) > 0:
                        total_trade_pnl = sum(t.get('pnl_usd', 0) for t in trades_log)
                        print(f"      Total PnL trades: ${total_trade_pnl:.4f}")
                
                # Simular custos real√≠sticos (melhoria)
                trading_costs = simulate_realistic_trading_costs(trades_log)
                net_portfolio = final_portfolio - trading_costs
                net_return = ((net_portfolio - INITIAL_PORTFOLIO) / INITIAL_PORTFOLIO) * 100
                
                episode_result = {
                    'episode': episode_num + 1,
                    'start_pos': start_pos,
                    'period_start': eval_df.index[start_pos],
                    'period_end': eval_df.index[min(start_pos + TEST_STEPS, len(eval_df) - 1)],
                    'initial_portfolio': INITIAL_PORTFOLIO,
                    'final_portfolio': final_portfolio,
                    'net_portfolio': net_portfolio,
                    'return_pct': episode_return,
                    'net_return_pct': net_return,
                    'trades_count': len(trades_log),
                    'trades_log': trades_log,
                    'portfolio_history': portfolio_history,
                    'trading_costs': trading_costs
                }
                
                all_episodes.append(episode_result)
                total_returns.append(episode_return)
                
                # Cleanup desnecess√°rio - reutilizando ambiente
                # del trading_env
            
            # CALCULAR M√âTRICAS ABRANGENTES (melhoria vs original)
            metrics = calculate_comprehensive_metrics(all_episodes)
            
            # Adicionar informa√ß√µes do checkpoint
            checkpoint_result = {
                'checkpoint_path': checkpoint_path,
                'checkpoint_name': os.path.basename(checkpoint_path),
                'load_method': load_method,
                'episodes_completed': len(all_episodes),
                'metrics': metrics,
                'episode_results': all_episodes
            }
            
            all_checkpoint_results[checkpoint_path] = checkpoint_result
            
            # üîß RELAT√ìRIO INDIVIDUAL EXPANDIDO COM DEBUG INFO
            print(f"\nüìä RESULTADOS - {os.path.basename(checkpoint_path)[:50]}")
            print("-" * 60)
            print(f"‚úÖ Epis√≥dios completados: {len(all_episodes)}")
            
            if metrics:
                print(f"üìà Retorno m√©dio: {metrics.get('mean_return', 0):+.2f}% (œÉ={metrics.get('std_return', 0):.2f}%)")
                print(f"üéØ Taxa de sucesso: {metrics.get('win_rate_episodes', 0):.1f}% dos epis√≥dios")
                print(f"‚öñÔ∏è Sharpe Ratio: {metrics.get('sharpe_ratio', 0):.2f}")
                print(f"üìâ Max Drawdown (REAL): {metrics.get('max_drawdown', 0):.2f}%")
                
                # üîç DEBUG INFO ADICIONAL
                if 'avg_drawdown_per_episode' in metrics:
                    print(f"üìâ Drawdown m√©dio/epis√≥dio: {metrics.get('avg_drawdown_per_episode', 0):.2f}%")
                    print(f"üìâ Epis√≥dios c/ drawdown: {metrics.get('drawdown_episodes_count', 0)}")
                
                # Portfolio range debug
                if all_episodes:
                    portfolio_range = [ep['final_portfolio'] for ep in all_episodes]
                    print(f"üí∞ Portfolio range: ${min(portfolio_range):.2f} - ${max(portfolio_range):.2f}")
                    
                    # Total portfolio change across all episodes
                    total_change = sum(ep['final_portfolio'] - INITIAL_PORTFOLIO for ep in all_episodes)
                    print(f"üí∞ Total portfolio Œî: ${total_change:.2f} ({len(all_episodes)} episodes)")
                
                if 'ci_95_lower' in metrics:
                    print(f"üìä IC 95%: [{metrics['ci_95_lower']:+.2f}%, {metrics['ci_95_upper']:+.2f}%]")
                
                if metrics.get('total_trades', 0) > 0:
                    print(f"üíπ Total trades: {metrics['total_trades']} (WR: {metrics.get('win_rate_trades', 0):.1f}%)")
                    print(f"üí∞ Profit Factor: {metrics.get('profit_factor', 0):.2f}")
            
            del model  # Limpar mem√≥ria
        
        # RELAT√ìRIO COMPARATIVO FINAL (melhoria vs original)
        print(f"\nüèÜ RELAT√ìRIO COMPARATIVO FINAL - {len(all_checkpoint_results)} CHECKPOINTS")
        print("=" * 80)
        
        if all_checkpoint_results:
            # Ranking por Sharpe Ratio
            ranked_checkpoints = sorted(
                all_checkpoint_results.items(), 
                key=lambda x: x[1]['metrics'].get('sharpe_ratio', -999), 
                reverse=True
            )
            
            print("üìä RANKING POR SHARPE RATIO:")
            for rank, (path, result) in enumerate(ranked_checkpoints, 1):
                # Identificar nome mais amig√°vel
                checkpoint_name = result['checkpoint_name']
                st = extract_steps_from_name(checkpoint_name)
                if checkpoint_name.lower().startswith("legion v1"):
                    display_name = "Baseline Legion V1"
                elif st:
                    display_name = f"PESCADOR {st/1_000_000:.2f}M"
                else:
                    display_name = checkpoint_name[:40]
                
                sharpe = result['metrics'].get('sharpe_ratio', 0)
                mean_return = result['metrics'].get('mean_return', 0)
                win_rate = result['metrics'].get('win_rate_episodes', 0)
                max_dd = result['metrics'].get('max_drawdown', 0)
                
                grade = "ü•á" if rank == 1 else "ü•à" if rank == 2 else "ü•â" if rank == 3 else f"{rank:2d}."
                
                print(f"{grade} {display_name:<22} | Sharpe: {sharpe:6.2f} | Ret: {mean_return:+6.2f}% | WR: {win_rate:5.1f}% | DD: {max_dd:6.2f}%")
            
            # RECOMENDA√á√ÉO FINAL
            if ranked_checkpoints:
                best_checkpoint = ranked_checkpoints[0]
                best_metrics = best_checkpoint[1]['metrics']
                
                print(f"\nüí° RECOMENDA√á√ÉO FINAL:")
                print(f"üèÜ Melhor checkpoint: {best_checkpoint[1]['checkpoint_name']}")
                print(f"üìä Sharpe Ratio: {best_metrics.get('sharpe_ratio', 0):.2f}")
                print(f"üìà Retorno m√©dio: {best_metrics.get('mean_return', 0):+.2f}%")
                print(f"üéØ Consist√™ncia: {best_metrics.get('win_rate_episodes', 0):.1f}% epis√≥dios lucrativos")
        
        # SALVAR RELAT√ìRIO DETALHADO (melhoria)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"D:/Projeto/avaliacoes/avaliacao_pescador_{timestamp}.json"
        
        # Salvar resultados em lista (evita colis√£o de chaves e preserva caminhos)
        save_list = []
        for path, result in all_checkpoint_results.items():
            save_result = result.copy()
            # Normalizar caminho para Windows
            try:
                save_result['checkpoint_path'] = os.path.normpath(path)
            except Exception:
                save_result['checkpoint_path'] = path
            # Adicionar steps extra√≠dos para depura√ß√£o
            save_result['steps'] = extract_steps_from_name(save_result.get('checkpoint_name', ''))
            # Apenas contador de epis√≥dios (n√£o dumpa todos)
            save_result['episode_results'] = len(result.get('episode_results', []))
            save_list.append(save_result)
        
        save_payload = {
            'results': save_list,
            '_metadata': {
                'evaluation_date': timestamp,
                'num_episodes': NUM_EPISODES,
                'test_steps': TEST_STEPS,
                'confidence_level': CONFIDENCE_LEVEL,
            'total_checkpoints_tested': len(save_list)
            }
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(save_payload, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nüíæ Resultados salvos: {results_file}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ERRO CR√çTICO: {e}")
        print(f"Detalhes: {traceback.format_exc()}")
        return False

if __name__ == "__main__":
    print(f"üé£ INICIANDO AVALIA√á√ÉO PESCADOR - {datetime.now().strftime('%H:%M:%S')}")
    
    # Set random seed para reprodutibilidade
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)
    
    # Processar argumento da linha de comando se fornecido
    if len(sys.argv) > 1:
        checkpoint_path = sys.argv[1]
        if not os.path.isabs(checkpoint_path):
            checkpoint_path = os.path.join("D:/Projeto", checkpoint_path)
        globals()['CHECKPOINT_PATH'] = checkpoint_path
        print(f"üìÇ Usando checkpoint: {os.path.basename(checkpoint_path)}")
    
    success = test_v8_elegance_trading()
    
    if success:
        print(f"\n‚úÖ AVALIA√á√ÉO PESCADOR CONCLU√çDA - {datetime.now().strftime('%H:%M:%S')}")
    else:
        print(f"\n‚ùå AVALIA√á√ÉO PESCADOR FALHOU - {datetime.now().strftime('%H:%M:%S')}")
