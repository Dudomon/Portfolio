# ğŸ”§ CORREÃ‡ÃƒO KL DIVERGENCE SPIKES V2

## ğŸ“Š Problema EspecÃ­fico:
- **KL divergence subindo MUITO** durante treinamento
- **Clip fraction estÃ¡vel** (descarta problema de clipping)
- **Causa**: RecurrentPPO + parÃ¢metros mal calibrados

## ğŸ” DiagnÃ³stico TÃ©cnico:

### 1. **RecurrentPPO vs PPO PadrÃ£o**
- `sb3_contrib.RecurrentPPO` tem dinÃ¢mica KL diferente
- States temporais causam maior variaÃ§Ã£o na policy

### 2. **ParÃ¢metros ProblemÃ¡ticos Identificados**:
- `n_epochs=3`: Muitos updates por batch â†’ KL cresce
- `ent_coef=0.05`: ExploraÃ§Ã£o excessiva conflita com policy
- `log_std_init=-0.5`: DistribuiÃ§Ãµes muito flexÃ­veis inicialmente
- `target_kl=0.03`: Muito permissivo para RecurrentPPO
- `max_grad_norm=0.2`: Permite updates muito agressivos

## ğŸ”§ Ajustes Aplicados:

### 1. **n_epochs**: 3 â†’ **2**
- **RazÃ£o**: Menos epochs = menos updates = menor acÃºmulo de KL
- **Efeito**: Policy muda gradualmente

### 2. **ent_coef**: 0.05 â†’ **0.02** 
- **RazÃ£o**: Menos exploraÃ§Ã£o = menos divergÃªncia da policy base
- **Efeito**: Policy mais estÃ¡vel

### 3. **clip_range**: 0.15 â†’ **0.12**
- **RazÃ£o**: Updates menos agressivos
- **Efeito**: MudanÃ§as de policy mais conservadoras

### 4. **target_kl**: 0.03 â†’ **0.01**
- **RazÃ£o**: Threshold mais restritivo para RecurrentPPO
- **Efeito**: Early stopping quando KL > 0.01

### 5. **max_grad_norm**: 0.2 â†’ **0.1**
- **RazÃ£o**: Gradients mais conservadores
- **Efeito**: Updates menores e mais estÃ¡veis

### 6. **log_std_init**: -0.5 â†’ **-1.0**
- **RazÃ£o**: DistribuiÃ§Ãµes mais rÃ­gidas inicialmente
- **Efeito**: Menos variabilidade inicial na policy

## ğŸ“ˆ Resultado Esperado:
- KL divergence estÃ¡vel < 0.01
- Menos spikes durante treinamento
- Policy evolution mais suave
- MantÃ©m clip fraction estÃ¡vel

## ğŸ¯ Monitoramento:
Observar nas prÃ³ximas 20k-50k steps:
- KL deve ficar consistentemente < 0.01
- Spikes devem desaparecer
- Training deve ser mais estÃ¡vel
- Performance nÃ£o deve degradar

## âš ï¸ Se Problema Persistir:
1. Reduzir `learning_rate`: 2e-5 â†’ 1e-5
2. Aumentar `batch_size`: 32 â†’ 64 (mais estabilidade)
3. Considerar `n_epochs=1` (ultra-conservador)
4. Verificar se RecurrentPPO Ã© necessÃ¡rio vs PPO padrÃ£o