#!/usr/bin/env python3
"""
üß™ TESTE SIMPLIFICADO V7 INTUITION
Teste funcional direto sem complica√ß√µes de formato
"""

import sys
import os
sys.path.append("D:/Projeto")

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional
import time
import traceback

# Imports do projeto
from trading_framework.policies.two_head_v7_intuition import TwoHeadV7Intuition
from trading_framework.extractors.transformer_extractor import TradingTransformerFeatureExtractor
from sb3_contrib import RecurrentPPO
from stable_baselines3.common.vec_env import DummyVecEnv
import gym
from gym import spaces

print("=" * 80)
print("üß™ TESTE SIMPLIFICADO V7 INTUITION")
print("=" * 80)

# Device setup
cuda_available = torch.cuda.is_available()
device = torch.device("cuda" if cuda_available else "cpu")
print(f"\n‚úì Device: {device}")
if cuda_available:
    gpu_name = torch.cuda.get_device_name(0)
    print(f"  GPU: {gpu_name}")

# ==============================================================================
# 1. CRIAR E TESTAR POL√çTICA
# ==============================================================================
print("\n1Ô∏è‚É£ CRIANDO POL√çTICA V7 INTUITION...")
print("-" * 60)

try:
    # Configura√ß√£o
    observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(2580,), dtype=np.float32)
    action_space = spaces.Box(
        low=np.array([0, 0, -3, -3, -3, -3, -3, -3]),
        high=np.array([2, 1, 3, 3, 3, 3, 3, 3]),
        dtype=np.float32
    )
    
    # Policy kwargs V7
    policy_kwargs = {
        'v7_shared_lstm_hidden': 512,
        'v7_features_dim': 256,
        'backbone_shared_dim': 256,
        'regime_embed_dim': 32,
        'gradient_mixing_strength': 0.3,
        'enable_interference_monitoring': True,
        'adaptive_sharing': True,
        'log_std_init': -1.0,
        'full_std': True,
        'use_expln': False,
        'squash_output': False,
        'features_extractor_class': TradingTransformerFeatureExtractor,
        'features_extractor_kwargs': {
            'features_dim': 128,
            'seq_len': 20
        },
        'critic_learning_rate': 4.0e-05,
        'net_arch': [
            {'pi': [512, 256], 'vf': [512, 256]}
        ]
    }
    
    # Criar pol√≠tica
    policy = TwoHeadV7Intuition(
        observation_space=observation_space,
        action_space=action_space,
        lr_schedule=lambda _: 3.5e-05,
        **policy_kwargs
    ).to(device)
    
    print(f"‚úÖ Pol√≠tica criada com sucesso")
    print(f"  Total par√¢metros: {sum(p.numel() for p in policy.parameters()):,}")
    print(f"  Device: {next(policy.parameters()).device}")
    
except Exception as e:
    print(f"‚ùå Erro ao criar pol√≠tica: {e}")
    traceback.print_exc()
    sys.exit(1)

# ==============================================================================
# 2. TESTE COM PPO REAL
# ==============================================================================
print("\n2Ô∏è‚É£ TESTANDO COM PPO...")
print("-" * 60)

try:
    # Criar ambiente mock simples
    class SimpleTradingEnv(gym.Env):
        def __init__(self):
            self.observation_space = spaces.Box(low=-np.inf, high=np.inf, 
                                               shape=(2580,), dtype=np.float32)
            self.action_space = spaces.Box(
                low=np.array([0, 0, -3, -3, -3, -3, -3, -3]),
                high=np.array([2, 1, 3, 3, 3, 3, 3, 3]),
                dtype=np.float32
            )
            self.step_count = 0
            
        def reset(self):
            self.step_count = 0
            return np.random.randn(2580).astype(np.float32)
        
        def step(self, action):
            self.step_count += 1
            obs = np.random.randn(2580).astype(np.float32)
            reward = float(np.random.randn())
            done = self.step_count >= 100  # Episode de 100 steps
            info = {}
            return obs, reward, done, info
    
    # Criar ambiente vetorizado
    env = DummyVecEnv([lambda: SimpleTradingEnv()])
    
    # Criar modelo PPO
    print("Criando modelo PPO...")
    model = RecurrentPPO(
        policy=TwoHeadV7Intuition,
        env=env,
        learning_rate=3.5e-05,
        n_steps=128,
        batch_size=32,
        n_epochs=2,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.12,
        ent_coef=0.08,
        vf_coef=1.0,
        max_grad_norm=0.1,
        verbose=0,
        device=device,
        policy_kwargs=policy_kwargs
    )
    
    print(f"‚úÖ Modelo PPO criado")
    print(f"  Device: {model.device}")
    
    # Teste r√°pido de treinamento
    print("\nTestando treinamento (256 steps)...")
    start_time = time.time()
    model.learn(total_timesteps=256, progress_bar=False)
    train_time = time.time() - start_time
    
    print(f"‚úÖ Treinamento completado em {train_time:.2f}s")
    
    # Teste de predi√ß√£o
    obs = env.reset()
    for _ in range(10):
        action, _ = model.predict(obs, deterministic=True)
        obs, rewards, dones, infos = env.step(action)
    
    print(f"‚úÖ Predi√ß√£o funcionando")
    print(f"  √öltima a√ß√£o: {action[0]}")
    
except Exception as e:
    print(f"‚ùå Erro no teste PPO: {e}")
    traceback.print_exc()

# ==============================================================================
# 3. TESTE DE FORWARD PASSES DIRETOS
# ==============================================================================
print("\n3Ô∏è‚É£ TESTANDO FORWARD PASSES...")
print("-" * 60)

try:
    # Usar o modelo PPO j√° criado
    policy = model.policy
    
    # Preparar dados de teste
    batch_size = 8
    obs = torch.randn(batch_size, 2580).to(device)
    
    # Estados LSTM no formato do PPO
    lstm_states = policy.get_initial_state(batch_size, device)
    
    # Episode starts
    episode_starts = torch.ones(batch_size, dtype=torch.bool).to(device)
    
    print(f"Shapes de entrada:")
    print(f"  Obs: {obs.shape}")
    print(f"  LSTM states: {type(lstm_states)}, len={len(lstm_states) if isinstance(lstm_states, tuple) else 'N/A'}")
    
    # Forward pass completo usando evaluate_actions (m√©todo do PPO)
    with torch.no_grad():
        # Criar a√ß√µes dummy para evaluate_actions
        dummy_actions = torch.randn(batch_size, 8).to(device)
        
        # Evaluate actions
        values, log_prob, entropy = policy.evaluate_actions(
            obs, dummy_actions, lstm_states, episode_starts
        )
        
        print(f"\n‚úÖ Forward pass via evaluate_actions:")
        print(f"  Values: {values.shape}")
        print(f"  Log prob: {log_prob.shape}")
        print(f"  Entropy: {entropy if entropy is not None else 'None'}")
    
    # Teste get_distribution
    with torch.no_grad():
        distribution = policy.get_distribution(obs, lstm_states, episode_starts)
        sampled_actions = distribution.sample()
        
        print(f"\n‚úÖ Get distribution:")
        print(f"  Distribution: {type(distribution)}")
        print(f"  Sampled actions: {sampled_actions.shape}")
    
    # Teste predict_values
    with torch.no_grad():
        predicted_values = policy.predict_values(obs, lstm_states, episode_starts)
        
        print(f"\n‚úÖ Predict values:")
        print(f"  Values: {predicted_values.shape}")
    
except Exception as e:
    print(f"‚ùå Erro no forward pass: {e}")
    traceback.print_exc()

# ==============================================================================
# 4. TESTE DE COMPONENTES
# ==============================================================================
print("\n4Ô∏è‚É£ VERIFICANDO COMPONENTES...")
print("-" * 60)

try:
    components = {
        'features_extractor': hasattr(policy, 'features_extractor'),
        'unified_backbone': hasattr(policy, 'unified_backbone'),
        'v7_actor_lstm': hasattr(policy, 'v7_actor_lstm'),
        'critic_lstm': hasattr(policy, 'critic_lstm'),
        'action_net': hasattr(policy, 'action_net'),
        'value_net': hasattr(policy, 'value_net'),
        'gradient_mixer': hasattr(policy, 'gradient_mixer'),
        'actor_optimizer': hasattr(policy, 'actor_optimizer'),
        'critic_optimizer': hasattr(policy, 'critic_optimizer'),
    }
    
    missing = []
    for component, exists in components.items():
        if exists:
            print(f"‚úÖ {component}")
        else:
            print(f"‚ùå {component} AUSENTE")
            missing.append(component)
    
    if not missing:
        print("\n‚úÖ Todos os componentes presentes!")
    else:
        print(f"\n‚ö†Ô∏è Componentes ausentes: {missing}")
    
    # Verificar feature extractor
    if hasattr(policy, 'features_extractor'):
        extractor = policy.features_extractor
        if isinstance(extractor, TradingTransformerFeatureExtractor):
            print(f"\n‚úÖ TradingTransformerFeatureExtractor configurado")
            print(f"  Features dim: {extractor.features_dim}")
            print(f"  Seq len: {extractor.seq_len}")
    
    # Verificar gradient mixer
    if hasattr(policy, 'gradient_mixer'):
        mixer = policy.gradient_mixer
        print(f"\n‚úÖ Gradient Mixer configurado")
        print(f"  Mixing strength: {mixer.mixing_strength}")
        print(f"  Monitoring: {mixer.interference_monitor is not None}")
    
except Exception as e:
    print(f"‚ùå Erro na verifica√ß√£o: {e}")
    traceback.print_exc()

# ==============================================================================
# 5. TESTE DE ESTABILIDADE
# ==============================================================================
print("\n5Ô∏è‚É£ TESTANDO ESTABILIDADE...")
print("-" * 60)

try:
    test_cases = {
        'zeros': torch.zeros(1, 2580).to(device),
        'ones': torch.ones(1, 2580).to(device),
        'large': torch.randn(1, 2580).to(device) * 100,
        'small': torch.randn(1, 2580).to(device) * 0.001,
    }
    
    lstm_states = policy.get_initial_state(1, device)
    episode_starts = torch.ones(1, dtype=torch.bool).to(device)
    
    for case_name, obs in test_cases.items():
        try:
            with torch.no_grad():
                values = policy.predict_values(obs, lstm_states, episode_starts)
                
                # Verificar NaN/Inf
                has_nan = torch.isnan(values).any()
                has_inf = torch.isinf(values).any()
                
                if has_nan or has_inf:
                    print(f"‚ùå {case_name}: NaN/Inf detectado")
                else:
                    print(f"‚úÖ {case_name}: Est√°vel (value={values[0,0]:.4f})")
                    
        except Exception as e:
            print(f"‚ùå {case_name}: Erro - {str(e)[:50]}")
    
except Exception as e:
    print(f"‚ùå Erro no teste de estabilidade: {e}")
    traceback.print_exc()

# ==============================================================================
# RESUMO FINAL
# ==============================================================================
print("\n" + "=" * 80)
print("üìä TESTE SIMPLIFICADO COMPLETO")
print("=" * 80)
print("\n‚úÖ V7 Intuition est√° funcional com PPO!")
print("‚úÖ Treinamento e predi√ß√£o funcionando")
print("‚úÖ Componentes principais presentes")
print("‚úÖ Forward passes est√°veis")
print("\nüéâ TESTE APROVADO!")