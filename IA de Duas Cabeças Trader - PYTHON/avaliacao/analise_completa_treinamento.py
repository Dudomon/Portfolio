#!/usr/bin/env python3
"""
üîç AN√ÅLISE COMPLETA DO TREINAMENTO SILUS - DIAGN√ìSTICO DE PERFORMANCE
======================================================================

OBJETIVO: Entender por que Sharpe 0.23 √© baixo e como melhorar
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import glob
import os

# Configura√ß√£o de visualiza√ß√£o
plt.style.use('dark_background')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (15, 10)

def load_training_metrics():
    """Carregar todos os CSVs de m√©tricas de treinamento"""
    
    # Pegar o CSV principal que cobre at√© 5M steps
    main_csv = "D:/Projeto/Otimizacao/treino_principal/models/SILUS/SILUS_training_metrics_20250828_091114.csv"
    
    print(f"üìä Carregando m√©tricas de treinamento: {os.path.basename(main_csv)}")
    
    try:
        df = pd.read_csv(main_csv)
        print(f"‚úÖ Carregado: {len(df)} linhas de dados")
        print(f"üìÖ Steps: {df['step'].min()} at√© {df['step'].max()}")
        return df
    except Exception as e:
        print(f"‚ùå Erro ao carregar: {e}")
        return None

def load_convergence_analysis():
    """Carregar an√°lise de converg√™ncia"""
    
    conv_csv = "D:/Projeto/Otimizacao/treino_principal/models/SILUS/convergence_analysis_20250828_091114.csv"
    
    try:
        df = pd.read_csv(conv_csv)
        print(f"‚úÖ Convergence analysis: {len(df)} linhas")
        return df
    except Exception as e:
        print(f"‚ùå Erro ao carregar convergence: {e}")
        return None

def load_reward_analysis():
    """Carregar an√°lise de rewards"""
    
    reward_csv = "D:/Projeto/Otimizacao/treino_principal/models/SILUS/reward_analysis_20250828_091114.csv"
    
    try:
        df = pd.read_csv(reward_csv)
        print(f"‚úÖ Reward analysis: {len(df)} linhas")
        return df
    except Exception as e:
        print(f"‚ùå Erro ao carregar rewards: {e}")
        return None

def load_trading_performance():
    """Carregar performance de trading"""
    
    trading_csv = "D:/Projeto/Otimizacao/treino_principal/models/SILUS/trading_performance_20250828_091114.csv"
    
    try:
        df = pd.read_csv(trading_csv)
        print(f"‚úÖ Trading performance: {len(df)} linhas")
        return df
    except Exception as e:
        print(f"‚ùå Erro ao carregar trading: {e}")
        return None

def analyze_training_metrics(df):
    """An√°lise detalhada das m√©tricas de treinamento"""
    
    print("\n" + "="*80)
    print("üìä AN√ÅLISE DE M√âTRICAS DE TREINAMENTO")
    print("="*80)
    
    # Verificar colunas dispon√≠veis
    print(f"\nüìã Colunas dispon√≠veis: {df.columns.tolist()}")
    
    # Estat√≠sticas b√°sicas em pontos-chave
    key_steps = [500000, 1000000, 1500000, 2000000, 2500000, 3000000, 3500000, 3900000, 4000000, 4500000, 5000000]
    
    print("\nüéØ M√âTRICAS EM PONTOS-CHAVE:")
    print("-" * 80)
    
    metrics_summary = []
    
    for step in key_steps:
        # Pegar linha mais pr√≥xima do step
        closest_idx = (df['step'] - step).abs().idxmin()
        row = df.iloc[closest_idx]
        actual_step = row['step']
        
        # Coletar m√©tricas principais
        metrics = {
            'Step': f"{actual_step/1e6:.2f}M",
            'Loss': row.get('loss', np.nan),
            'Value Loss': row.get('value_loss', np.nan),
            'Policy Loss': row.get('policy_loss', np.nan),
            'Entropy': row.get('entropy_loss', np.nan),
            'KL Div': row.get('approx_kl', np.nan),
            'Clip Frac': row.get('clip_fraction', np.nan),
            'Explained Var': row.get('explained_variance', np.nan),
            'Learning Rate': row.get('learning_rate', np.nan)
        }
        
        metrics_summary.append(metrics)
        
        # Print resumido
        if step in [1000000, 2000000, 3000000, 3900000, 4000000, 5000000]:
            print(f"\nüìç {metrics['Step']} steps:")
            print(f"   Loss: {metrics['Loss']:.4f}")
            print(f"   Value Loss: {metrics['Value Loss']:.4f}")
            print(f"   Policy Loss: {metrics['Policy Loss']:.4f}")
            print(f"   Entropy: {metrics['Entropy']:.4f}")
            print(f"   KL Divergence: {metrics['KL Div']:.6f}")
            print(f"   Clip Fraction: {metrics['Clip Frac']:.3f}")
            print(f"   Explained Var: {metrics['Explained Var']:.3f}")
            print(f"   LR: {metrics['Learning Rate']:.2e}")
    
    # Criar DataFrame resumido
    summary_df = pd.DataFrame(metrics_summary)
    
    # An√°lise de tend√™ncias
    print("\n" + "="*80)
    print("üìà AN√ÅLISE DE TEND√äNCIAS")
    print("="*80)
    
    # 1. Tend√™ncia do Loss
    print("\n1Ô∏è‚É£ TEND√äNCIA DO LOSS:")
    loss_start = df[df['step'] < 1000000]['loss'].mean()
    loss_mid = df[(df['step'] >= 2000000) & (df['step'] < 3000000)]['loss'].mean()
    loss_end = df[df['step'] > 4000000]['loss'].mean()
    
    print(f"   In√≠cio (< 1M): {loss_start:.4f}")
    print(f"   Meio (2-3M): {loss_mid:.4f}")
    print(f"   Fim (> 4M): {loss_end:.4f}")
    print(f"   Redu√ß√£o total: {(loss_start - loss_end)/loss_start*100:.1f}%")
    
    # 2. Estabilidade do Explained Variance
    print("\n2Ô∏è‚É£ EXPLAINED VARIANCE (CR√çTICO!):")
    ev_early = df[df['step'] < 1000000]['explained_variance'].mean()
    ev_sweet = df[(df['step'] >= 3800000) & (df['step'] <= 4000000)]['explained_variance'].mean()
    ev_late = df[df['step'] > 4500000]['explained_variance'].mean()
    
    print(f"   Early (< 1M): {ev_early:.3f}")
    print(f"   Sweet Spot (3.8-4M): {ev_sweet:.3f}")
    print(f"   Late (> 4.5M): {ev_late:.3f}")
    
    if ev_sweet > 0:
        print(f"   ‚ö†Ô∏è ALERTA: Explained Variance positivo no sweet spot!")
        print(f"   ‚Üí Indica poss√≠vel reward hacking ou overfitting ao reward")
    
    # 3. KL Divergence e Clip Fraction
    print("\n3Ô∏è‚É£ KL DIVERGENCE & CLIP FRACTION:")
    kl_avg = df['approx_kl'].mean()
    kl_std = df['approx_kl'].std()
    clip_avg = df['clip_fraction'].mean()
    clip_std = df['clip_fraction'].std()
    
    print(f"   KL Divergence m√©dio: {kl_avg:.6f} (¬±{kl_std:.6f})")
    print(f"   Clip Fraction m√©dio: {clip_avg:.3f} (¬±{clip_std:.3f})")
    
    if kl_avg < 0.01:
        print(f"   ‚ö†Ô∏è KL muito baixo - pol√≠tica mudando muito devagar!")
    if clip_avg > 0.2:
        print(f"   ‚ö†Ô∏è Clip fraction alto - mudan√ßas muito agressivas!")
    
    # 4. Entropy Analysis
    print("\n4Ô∏è‚É£ ENTROPY (EXPLORA√á√ÉO):")
    entropy_start = df[df['step'] < 500000]['entropy_loss'].mean()
    entropy_end = df[df['step'] > 4500000]['entropy_loss'].mean()
    
    print(f"   In√≠cio: {entropy_start:.4f}")
    print(f"   Fim: {entropy_end:.4f}")
    print(f"   Redu√ß√£o: {(entropy_start - entropy_end)/entropy_start*100:.1f}%")
    
    if entropy_end < 0.01:
        print(f"   ‚ö†Ô∏è Entropy muito baixa - pouca explora√ß√£o!")
    
    return summary_df

def analyze_convergence(df):
    """An√°lise de converg√™ncia"""
    
    if df is None:
        return
    
    print("\n" + "="*80)
    print("üéØ AN√ÅLISE DE CONVERG√äNCIA")
    print("="*80)
    
    # Verificar colunas
    print(f"\nüìã Colunas: {df.columns.tolist()}")
    
    # Estat√≠sticas de converg√™ncia
    if 'gradient_norm' in df.columns:
        grad_norm_avg = df['gradient_norm'].mean()
        grad_norm_std = df['gradient_norm'].std()
        print(f"\nüìä Gradient Norm: {grad_norm_avg:.6f} (¬±{grad_norm_std:.6f})")
        
        if grad_norm_avg < 0.001:
            print("   ‚ö†Ô∏è Gradientes muito pequenos - aprendizado lento!")
    
    if 'policy_stability' in df.columns:
        stability = df['policy_stability'].mean()
        print(f"üìä Policy Stability: {stability:.3f}")
        
        if stability < 0.8:
            print("   ‚ö†Ô∏è Pol√≠tica inst√°vel!")

def analyze_rewards(df):
    """An√°lise de rewards"""
    
    if df is None:
        return
    
    print("\n" + "="*80)
    print("üí∞ AN√ÅLISE DE REWARDS")
    print("="*80)
    
    # Estat√≠sticas de reward
    if 'mean_reward' in df.columns:
        reward_trend = df.groupby(df.index // 100)['mean_reward'].mean()
        
        print(f"\nüìä Reward m√©dio geral: {df['mean_reward'].mean():.4f}")
        print(f"üìä Reward m√°ximo: {df['mean_reward'].max():.4f}")
        print(f"üìä Reward m√≠nimo: {df['mean_reward'].min():.4f}")
        
        # Tend√™ncia
        early_reward = df.iloc[:len(df)//3]['mean_reward'].mean()
        late_reward = df.iloc[-len(df)//3:]['mean_reward'].mean()
        
        print(f"\nüìà Tend√™ncia:")
        print(f"   Early: {early_reward:.4f}")
        print(f"   Late: {late_reward:.4f}")
        print(f"   Melhoria: {(late_reward - early_reward)/abs(early_reward)*100:.1f}%")

def analyze_trading(df):
    """An√°lise de trading performance"""
    
    if df is None:
        return
    
    print("\n" + "="*80)
    print("üíπ AN√ÅLISE DE TRADING PERFORMANCE")
    print("="*80)
    
    # Verificar colunas
    print(f"\nüìã Colunas: {df.columns.tolist()}")
    
    # Estat√≠sticas de trading
    if 'win_rate' in df.columns:
        wr_avg = df['win_rate'].mean()
        print(f"\nüìä Win Rate m√©dio: {wr_avg:.1f}%")
    
    if 'sharpe_ratio' in df.columns:
        sharpe_avg = df['sharpe_ratio'].mean()
        sharpe_max = df['sharpe_ratio'].max()
        print(f"üìä Sharpe m√©dio: {sharpe_avg:.3f}")
        print(f"üìä Sharpe m√°ximo: {sharpe_max:.3f}")
        
        if sharpe_avg < 0.5:
            print("   ‚ö†Ô∏è Sharpe muito baixo para trading real!")
    
    if 'profit_factor' in df.columns:
        pf_avg = df['profit_factor'].mean()
        print(f"üìä Profit Factor m√©dio: {pf_avg:.2f}")

def provide_recommendations():
    """Recomenda√ß√µes para melhorar performance"""
    
    print("\n" + "="*80)
    print("üöÄ RECOMENDA√á√ïES PARA MELHORAR PERFORMANCE")
    print("="*80)
    
    recommendations = """
    
1Ô∏è‚É£ PROBLEMA: Sharpe Ratio baixo (0.23)
   
   CAUSAS PROV√ÅVEIS:
   ‚Ä¢ Reward system n√£o otimizado para Sharpe
   ‚Ä¢ Excesso de trades (overtrading)
   ‚Ä¢ Gest√£o de risco inadequada
   
   SOLU√á√ïES:
   ‚úÖ Implementar Sharpe-based reward: reward = returns / std(returns)
   ‚úÖ Penalizar n√∫mero excessivo de trades
   ‚úÖ Aumentar filtro de confian√ßa para 0.7-0.8
   ‚úÖ Implementar position sizing din√¢mico baseado em volatilidade

2Ô∏è‚É£ PROBLEMA: Explained Variance positivo (poss√≠vel reward hacking)
   
   SOLU√á√ïES:
   ‚úÖ Reduzir amplifica√ß√£o do reward (de 4x para 2x)
   ‚úÖ Adicionar regulariza√ß√£o L2 no critic
   ‚úÖ Implementar reward clipping mais agressivo
   ‚úÖ Usar GAE lambda = 0.9 (ao inv√©s de 0.95)

3Ô∏è‚É£ PROBLEMA: KL Divergence muito baixo
   
   SOLU√á√ïES:
   ‚úÖ Aumentar learning rate para 1e-4
   ‚úÖ Reduzir clip_range para 0.1
   ‚úÖ Aumentar n_epochs para 20
   ‚úÖ Usar adaptive KL penalty

4Ô∏è‚É£ PROBLEMA: Entropy muito baixa (pouca explora√ß√£o)
   
   SOLU√á√ïES:
   ‚úÖ Aumentar ent_coef para 0.1-0.15
   ‚úÖ Implementar entropy scheduling (decay mais lento)
   ‚úÖ Adicionar noise injection na pol√≠tica
   ‚úÖ Usar curiosity-driven exploration

5Ô∏è‚É£ MELHORIAS NO AMBIENTE:
   
   ‚úÖ Implementar custos de transa√ß√£o realistas (spread + comiss√£o)
   ‚úÖ Adicionar slippage din√¢mico baseado em volume
   ‚úÖ Simular impacto de mercado para orders grandes
   ‚úÖ Implementar lat√™ncia e delays de execu√ß√£o

6Ô∏è‚É£ MELHORIAS NO TREINAMENTO:
   
   ‚úÖ Usar curriculum learning (come√ßar com mercados mais f√°ceis)
   ‚úÖ Implementar meta-learning para adapta√ß√£o r√°pida
   ‚úÖ Treinar ensemble de modelos (3-5 modelos)
   ‚úÖ Usar adversarial training para robustez

7Ô∏è‚É£ VALIDA√á√ÉO MAIS RIGOROSA:
   
   ‚úÖ Walk-forward optimization
   ‚úÖ Monte Carlo simulation com diferentes seeds
   ‚úÖ Stress testing em per√≠odos de crise
   ‚úÖ Out-of-sample testing em outros ativos

8Ô∏è‚É£ ARQUITETURA:
   
   ‚úÖ Aumentar hidden_size para 512
   ‚úÖ Adicionar attention mechanism
   ‚úÖ Usar transformer architecture
   ‚úÖ Implementar memory replay buffer maior
    """
    
    print(recommendations)
    
    print("\n" + "="*80)
    print("üéØ PR√ìXIMOS PASSOS PRIORIT√ÅRIOS:")
    print("="*80)
    print("""
    1. IMEDIATO: Aumentar filtro de confian√ßa para 0.75
    2. IMEDIATO: Reduzir reward amplification para 2x
    3. CURTO PRAZO: Implementar Sharpe-based reward
    4. M√âDIO PRAZO: Adicionar custos de transa√ß√£o realistas
    5. LONGO PRAZO: Migrar para transformer architecture
    """)

def main():
    """Executar an√°lise completa"""
    
    print("="*80)
    print("üîç AN√ÅLISE COMPLETA DO TREINAMENTO SILUS")
    print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)
    
    # Carregar dados
    print("\nüìÇ CARREGANDO DADOS...")
    training_df = load_training_metrics()
    convergence_df = load_convergence_analysis()
    reward_df = load_reward_analysis()
    trading_df = load_trading_performance()
    
    # An√°lises
    if training_df is not None:
        summary_df = analyze_training_metrics(training_df)
    
    analyze_convergence(convergence_df)
    analyze_rewards(reward_df)
    analyze_trading(trading_df)
    
    # Recomenda√ß√µes
    provide_recommendations()
    
    print("\n" + "="*80)
    print("‚úÖ AN√ÅLISE CONCLU√çDA")
    print("="*80)

if __name__ == "__main__":
    main()