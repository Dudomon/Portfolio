# üö® RELAT√ìRIO: Vi√©s Vendedor Dominante nos Modelos

## Sum√°rio Executivo

Identificado **vi√©s vendedor estrutural CR√çTICO** no action space que faz os modelos preferirem SHORT mesmo com mercado em alta.

## üéØ Causa Raiz Identificada

### **CULPADO: ACTION SPACE ASSIM√âTRICO**

```python
# cherry.py linha 3580
self.action_space = spaces.Box(
    low=np.array([0, 0, -1, -1]),
    high=np.array([2, 1, 1, 1]),
    dtype=np.float32
)

# Mapeamento (linhas 77-78, 3868-3874)
ACTION_THRESHOLD_LONG = 0.33
ACTION_THRESHOLD_SHORT = 0.67

# action[0] em [0, 2]:
if raw_decision < 0.33:      # HOLD
    entry_decision = 0
elif raw_decision < 0.67:    # LONG
    entry_decision = 1
else:                        # SHORT (>= 0.67)
    entry_decision = 2
```

## üìä An√°lise Quantitativa

### Distribui√ß√£o com A√ß√µes Aleat√≥rias Uniformes [0,2]:

| A√ß√£o  | Range         | % do Espa√ßo | Samples (100k) | % Observado |
|-------|---------------|-------------|----------------|-------------|
| HOLD  | [0.00, 0.33]  | 16.5%      | 16,515         | 16.5%       |
| LONG  | [0.33, 0.67]  | 17.0%      | 17,110         | 17.1%       |
| SHORT | [0.67, 2.00]  | **66.5%**  | **66,375**     | **66.4%**   |

### üö® Vi√©s Estrutural Detectado:

- **Range SHORT**: 1.33 unidades (66.5% do espa√ßo total)
- **Range LONG**: 0.34 unidades (17.0% do espa√ßo total)
- **Range HOLD**: 0.33 unidades (16.5% do espa√ßo total)

**FATOR DE VI√âS: SHORT √© 3.91x MAIOR que LONG**

## üîç Por Que Isso √â Um Problema?

### 1. **Facilita√ß√£o Estrutural**
O modelo tem **3.91x mais facilidade** para escolher SHORT do que LONG porque:
- Qualquer sa√≠da da rede neural entre [0.67, 2.0] resulta em SHORT
- Apenas sa√≠das entre [0.33, 0.67] resultam em LONG
- Durante treinamento, gradientes naturalmente empurram para ranges maiores

### 2. **Refor√ßo Durante Treinamento**
- Modelos exploram aleatoriamente no in√≠cio
- 66% das explora√ß√µes aleat√≥rias s√£o SHORT
- Se o mercado est√° em baixa em QUALQUER momento do treino:
  - SHORTs acumulam reward positivo
  - Network aprende: "SHORT = bom"
  - Bias se cristaliza nos pesos

### 3. **Imposs√≠vel Recuperar**
Uma vez que o bias √© aprendido:
- Network precisa "desaprender" milh√µes de steps de SHORT
- Enquanto isso, continua vendo 66% SHORT em novas explora√ß√µes
- Feedback loop positivo mant√©m o vi√©s

## üß† Compara√ß√£o Robot_cherry.py vs cherry.py

### Robot_cherry.py (linha 385-389):
```python
self.action_space = spaces.Box(
    low=np.array([-10.0, 0.0, -3.0, -3.0]),
    high=np.array([10.0, 1.0, 3.0, 3.0]),
    dtype=np.float32
)
```

**PROBLEMA**: Action space DIFERENTE do ambiente de treino!
- Robot usa [-10, 10] mas n√£o h√° mapeamento documentado
- Log mostra threshold em linha 3549-3555 (IGUAL ao cherry.py)
- **INCONSIST√äNCIA CR√çTICA**: Spaces diferentes, mas mesmo mapeamento

### Mapeamento Robot (linha 3549-3555):
```python
raw_decision = float(action[0])
if raw_decision < 0.33:      # < 0.33 = HOLD
    entry_decision = 0
elif raw_decision < 0.67:    # < 0.67 = LONG
    entry_decision = 1
else:                        # >= 0.67 = SHORT
    entry_decision = 2
```

**PROBLEMA ADICIONAL**:
- Robot espera [-10, 10] mas usa thresholds [0.33, 0.67]
- Qualquer valor negativo vira HOLD
- Qualquer valor > 0.67 vira SHORT
- Range SHORT ainda maior!

## ‚úÖ Solu√ß√µes Propostas

### Solu√ß√£o 1: **Action Space Balanceado (RECOMENDADA)**

```python
# cherry.py
self.action_space = spaces.Box(
    low=np.array([-1, 0, -1, -1]),  # Centrado em zero
    high=np.array([1, 1, 1, 1]),
    dtype=np.float32
)

# Novo mapeamento SIM√âTRICO
ACTION_THRESHOLD_LONG = -0.33   # [-1, -0.33] = HOLD (33%)
ACTION_THRESHOLD_SHORT = 0.33   # [-0.33, 0.33] = LONG (33%)
                                # [0.33, 1] = SHORT (33%)

raw_decision = float(action[0])
if raw_decision < -0.33:
    entry_decision = 0  # HOLD
elif raw_decision < 0.33:
    entry_decision = 1  # LONG
else:
    entry_decision = 2  # SHORT
```

**Vantagens**:
- Ranges perfeitamente balanceados (0.67 cada)
- Centrado em zero (melhor para redes neurais)
- Sim√©trico (LONG e SHORT equidistantes de zero)

### Solu√ß√£o 2: **Discrete Action Space**

```python
self.action_space = spaces.MultiDiscrete([3, 101, 201, 201])
# [0] entry: 0=HOLD, 1=LONG, 2=SHORT (discreto, sem vi√©s)
# [1] confidence: 0-100 (mapeado para [0,1])
# [2-3] management: 0-200 (mapeado para [-1,1])
```

**Vantagens**:
- Elimina completamente vi√©s estrutural
- Mais f√°cil de interpretar
- Melhor para debugging

### Solu√ß√£o 3: **Penaliza√ß√£o de SHORT no Reward**

```python
# Adicionar em reward_daytrade_v3_brutal.py
if entry_decision == 2:  # SHORT
    # Penalizar SHORT para compensar vi√©s estrutural
    base_reward *= 0.7  # Reduzir reward de SHORT em 30%
```

**Desvantagens**:
- Hack tempor√°rio, n√£o resolve causa raiz
- Dificulta aprendizado leg√≠timo de SHORTs
- N√£o recomendado

## üéØ Plano de A√ß√£o Recomendado

### Passo 1: **Corrigir cherry.py** ‚úÖ PRIORIT√ÅRIO
1. Mudar action_space para [-1, 1] na dimens√£o [0]
2. Atualizar thresholds para [-0.33, 0.33]
3. Atualizar mapeamento em todas as fun√ß√µes:
   - `step()` linha 3868
   - `_process_v5_specialized_action()` linha 6560
   - `_calculate_entry_reward()` linha 6207

### Passo 2: **Alinhar Robot_cherry.py** ‚úÖ PRIORIT√ÅRIO
1. Corrigir action_space para [-1, 1] (linha 385)
2. Garantir mapeamento id√™ntico ao cherry.py
3. Testar que ranges s√£o balanceados

### Passo 3: **Re-treinar Modelos** ‚ö†Ô∏è NECESS√ÅRIO
- Modelos atuais foram treinados com vi√©s
- Precisam ser retreinados do zero com novo action space
- Checkpoints antigos s√£o INCOMPAT√çVEIS

### Passo 4: **Validar Distribui√ß√£o**
```python
# Adicionar em cherry.py callback
if self.num_timesteps % 10000 == 0:
    print(f"Distribui√ß√£o a√ß√µes: HOLD={hold_pct:.1f}% LONG={long_pct:.1f}% SHORT={short_pct:.1f}%")
```

## üìä Evid√™ncias Adicionais

### 1. Logs do Robot mostram:
```
[PREDI√á√ÉO] SHORT | Entry: X.XX | Confidence: X.XX
[PREDI√á√ÉO] SHORT | Entry: X.XX | Confidence: X.XX
[PREDI√á√ÉO] SHORT | Entry: X.XX | Confidence: X.XX
```

### 2. Reward System V3Brutal:
- ‚úÖ Sim√©trico entre LONG e SHORT
- ‚úÖ Sem vi√©s no c√°lculo de PnL
- ‚úÖ Pain multiplier igual para ambos

### 3. Action Space Cherry:
- ‚ùå Range SHORT 3.91x maior
- ‚ùå Explora√ß√£o naturalmente viesada
- ‚ùå Gradientes favorecem SHORT

## üéì Conclus√£o

O **vi√©s vendedor dominante** √© causado por **design assim√©trico do action space**, n√£o por problemas de reward ou features.

**SOLU√á√ÉO: Balancear action space para [-1, 1] com thresholds sim√©tricos**

**IMPACTO**: Todos os modelos precisam ser retreinados do zero

**PRIORIDADE**: üö® CR√çTICA - Afeta fundamentalmente o comportamento do modelo

---

**Data**: 2025-09-30
**Status**: Causa identificada, solu√ß√£o proposta
**A√ß√£o Necess√°ria**: Aprovar corre√ß√£o e re-treino
